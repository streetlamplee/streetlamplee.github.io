---
layout: post
title: Pytorch_DNN ì‹¤ìŠµ (2)
date: 2023-12-27 17:00 +0900
last_modified_at: 2023-12-27 17:00:00 +0900
tags: [deeplearning, Pytorch, DNN]
toc:  true
---

# DNN êµ¬í˜„(3)

### ì‹¤ìŠµ ëª©ì°¨
* 1. Custom Dataset êµ¬ì¶•í•˜ê¸°
  * 1-1. ìì—°ì–´ ë°ì´í„°ì˜ ì „ì²˜ë¦¬
  * 1-2. ë°ì´í„°ì…‹ í´ë˜ìŠ¤ êµ¬ì¶•í•˜ê¸°

* 2. Next word prediction ëª¨ë¸ êµ¬ì¶•
  * 2-1. Next word predictionì„ ìœ„í•œ DNN ëª¨ë¸ êµ¬ì¶•
  * 2-2. ëª¨ë¸ í•™ìŠµ ë° ì¶”ë¡ 


### í™˜ê²½ ì„¤ì •

- íŒ¨í‚¤ì§€ ì„¤ì¹˜ ë° ì„í¬íŠ¸


```python
!pip install scikit-learn==1.3.0 -q
!pip install torch==2.0.1 -q
!pip install torchvision==0.15.2 -q
!pip install torchtext==0.15.2 -q
```

    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m10.8/10.8 MB[0m [31m85.9 MB/s[0m eta [36m0:00:00[0m
    [?25h


```python
import numpy as np # ê¸°ë³¸ì ì¸ ì—°ì‚°ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
import matplotlib.pyplot as plt # ê·¸ë¦¼ì´ë‚˜ ê·¸ë˜í”„ë¥¼ ê·¸ë¦¬ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
from tqdm.notebook import tqdm # ìƒíƒœ ë°”ë¥¼ ë‚˜íƒ€ë‚´ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
import pandas as pd # ë°ì´í„°í”„ë ˆì„ì„ ì¡°ì‘í•˜ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬

import torch # PyTorch ë¼ì´ë¸ŒëŸ¬ë¦¬
import torch.nn as nn # ëª¨ë¸ êµ¬ì„±ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
import torch.optim as optim # optimizer ì„¤ì •ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
from torch.utils.data import Dataset, DataLoader # ë°ì´í„°ì…‹ ì„¤ì •ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬

from torchtext.data import get_tokenizer # torchì—ì„œ tokenizerë¥¼ ì–»ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
import torchtext # torchì—ì„œ textë¥¼ ë” ì˜ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬

from sklearn.metrics import accuracy_score # ì„±ëŠ¥ì§€í‘œ ì¸¡ì •
from sklearn.model_selection import train_test_split # train-validation-test set ë‚˜ëˆ„ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬

import re # text ì „ì²˜ë¦¬ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
```


```python
# seed ê³ ì •
import random
import torch.backends.cudnn as cudnn

def random_seed(seed_num):
    torch.manual_seed(seed_num)
    torch.cuda.manual_seed(seed_num)
    torch.cuda.manual_seed_all(seed_num)
    np.random.seed(seed_num)
    cudnn.benchmark = False
    cudnn.deterministic = True
    random.seed(seed_num)

random_seed(42)
```


```python
device = 'cuda:0'
```

###  ë°ì´í„° ì…‹ ê°œìš” </b>

* ë°ì´í„°ì…‹: <a href='https://www.kaggle.com/datasets/dorianlazar/medium-articles-dataset'>Medium Dataset</a>
* ë°ì´í„°ì…‹ ê°œìš”: "Towards Data Science", "UX Collective", "The Startup", "The Writing Cooperative", "Data Driven Investor", "Better Humans", "Better Marketing" ì˜ 7ê°œì˜ ì£¼ì œë¥¼ ê°€ì§€ëŠ” publication ì— ëŒ€í•´ì„œ í¬ë¡¤ë§ì„ í•œ ë°ì´í„°ì…ë‹ˆë‹¤. ì›ë³¸ ë°ì´í„°ëŠ” ì´ 6,508ê°œì˜ ë¸”ë¡œê·¸ ì´ë¯¸ì§€ì™€ ë©”íƒ€ ë°ì´í„°(.csv)ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤. ì‹¤ìŠµì—ì„œëŠ” ë©”íƒ€ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ CustomDatasetì„ êµ¬í˜„í•©ë‹ˆë‹¤.
  * [How to collect ths dataset?](https://dorianlazar.medium.com/scraping-medium-with-python-beautiful-soup-3314f898bbf5)
- ë©”íƒ€ ë°ì´í„° ìŠ¤í‚¤ë§ˆ: ë©”íƒ€ ë°ì´í„°ëŠ” ì´ **10**ê°œì˜ columnìœ¼ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.
  - id: ì•„ì´ë””
  - url: í¬ìŠ¤íŒ… ë§í¬
  - title: ì œëª©
  - subtitle: ë¶€ì œëª©
  - image: í¬ìŠ¤íŒ… ì´ë¯¸ì§€ì˜ íŒŒì¼ ì´ë¦„
  - claps: ì¶”ì²œ ìˆ˜
  - reponses: ëŒ“ê¸€ ìˆ˜
  - reading_time: ì½ëŠ”ë° ê±¸ë¦¬ëŠ” ì‹œê°„
  - publication: ì£¼ì œ ì¹´í…Œê³ ë¦¬(e.g. Towards Data Science..)
  - date: ì‘ì„± ë‚ ì§œ
- ë°ì´í„° ì…‹ ì €ì‘ê¶Œ: CC0: Public Domain

## 1. Custom Dataset êµ¬ì¶•í•˜ê¸°



- 1-1. ìì—°ì–´ ë°ì´í„°ì˜ ì „ì²˜ë¦¬
- 1-2. Custom Dataset class êµ¬ì¶•í•˜ê¸°


### 1-1 ìì—°ì–´ ë°ì´í„° ì „ì²˜ë¦¬

> textë¡œ ëœ ë°ì´í„°ë¥¼ ì–´ë–»ê²Œ ìˆ«ì í˜•ì‹ìœ¼ë¡œ ë°”ê¾¸ê³ , ëª¨ë¸ì— ë„£ëŠ” êµ¬ì¡°ë¡œ ë°”ê¾¸ëŠ”ì§€ ì§ì ‘ ì‹¤ìŠµí•´ë´…ë‹ˆë‹¤.


#### ğŸ“ ì„¤ëª…: Next word prediction
* ê¸€ì˜ ì¼ë¶€ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡ (next word prediction)í•˜ëŠ” ëª¨ë¸ì„ êµ¬ì¶•í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤.
* ì˜ˆë¥¼ ë“¤ì–´, "ë‚˜ëŠ” í•™êµë¥¼ ê°€ì„œ ë°¥ì„ ë¨¹ì—ˆë‹¤." ë¼ëŠ” ë¬¸ì¥ì´ ì£¼ì–´ì§„ë‹¤ê³  í•´ë´…ì‹œë‹¤.

|input|label|
|------|---|
|ë‚˜ëŠ”|í•™êµë¥¼|
|ë‚˜ëŠ” í•™êµë¥¼|ê°€ì„œ|
|ë‚˜ëŠ” í•™êµë¥¼ ê°€ì„œ|ë°¥ì„|
|ë‚˜ëŠ” í•™êµë¥¼ ê°€ì„œ ë°¥ì„|ë¨¹ì—ˆë‹¤.|

* ì´ì™€ ê°™ì´ ë°ì´í„°ì…‹ì„ êµ¬ì¶•í•˜ê³ , DNNì„ í†µí•´ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•´ë´…ë‹ˆë‹¤.

ğŸ“š ì°¸ê³ í• ë§Œí•œ ìë£Œ:
* [Next word prediction](https://wikidocs.net/45101)


```python
data_csv = pd.read_csv('medium_data.csv')
data_csv.head()
```





  <div id="df-74176281-52a0-4fcd-99ba-4ffd46dddc3f" class="colab-df-container">
    <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>url</th>
      <th>title</th>
      <th>subtitle</th>
      <th>image</th>
      <th>claps</th>
      <th>responses</th>
      <th>reading_time</th>
      <th>publication</th>
      <th>date</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>https://towardsdatascience.com/a-beginners-gui...</td>
      <td>A Beginnerâ€™s Guide to Word Embedding with Gens...</td>
      <td>NaN</td>
      <td>1.png</td>
      <td>850</td>
      <td>8</td>
      <td>8</td>
      <td>Towards Data Science</td>
      <td>2019-05-30</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>https://towardsdatascience.com/hands-on-graph-...</td>
      <td>Hands-on Graph Neural Networks with PyTorch &amp; ...</td>
      <td>NaN</td>
      <td>2.png</td>
      <td>1100</td>
      <td>11</td>
      <td>9</td>
      <td>Towards Data Science</td>
      <td>2019-05-30</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>https://towardsdatascience.com/how-to-use-ggpl...</td>
      <td>How to Use ggplot2 inÂ Python</td>
      <td>A Grammar of Graphics forÂ Python</td>
      <td>3.png</td>
      <td>767</td>
      <td>1</td>
      <td>5</td>
      <td>Towards Data Science</td>
      <td>2019-05-30</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>https://towardsdatascience.com/databricks-how-...</td>
      <td>Databricks: How to Save Files in CSV on Your L...</td>
      <td>When I work on Python projectsÂ dealingâ€¦</td>
      <td>4.jpeg</td>
      <td>354</td>
      <td>0</td>
      <td>4</td>
      <td>Towards Data Science</td>
      <td>2019-05-30</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>https://towardsdatascience.com/a-step-by-step-...</td>
      <td>A Step-by-Step Implementation of Gradient Desc...</td>
      <td>One example of buildingÂ neuralâ€¦</td>
      <td>5.jpeg</td>
      <td>211</td>
      <td>3</td>
      <td>4</td>
      <td>Towards Data Science</td>
      <td>2019-05-30</td>
    </tr>
  </tbody>
</table>
</div>
    <div class="colab-df-buttons">

  <div class="colab-df-container">
    <button class="colab-df-convert" onclick="convertToInteractive('df-74176281-52a0-4fcd-99ba-4ffd46dddc3f')"
            title="Convert this dataframe to an interactive table."
            style="display:none;">

  <svg xmlns="http://www.w3.org/2000/svg" height="24px" viewBox="0 -960 960 960">
    <path d="M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z"/>
  </svg>
    </button>

  <style>
    .colab-df-container {
      display:flex;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    .colab-df-buttons div {
      margin-bottom: 4px;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

    <script>
      const buttonEl =
        document.querySelector('#df-74176281-52a0-4fcd-99ba-4ffd46dddc3f button.colab-df-convert');
      buttonEl.style.display =
        google.colab.kernel.accessAllowed ? 'block' : 'none';

      async function convertToInteractive(key) {
        const element = document.querySelector('#df-74176281-52a0-4fcd-99ba-4ffd46dddc3f');
        const dataTable =
          await google.colab.kernel.invokeFunction('convertToInteractive',
                                                    [key], {});
        if (!dataTable) return;

        const docLinkHtml = 'Like what you see? Visit the ' +
          '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
          + ' to learn more about interactive tables.';
        element.innerHTML = '';
        dataTable['output_type'] = 'display_data';
        await google.colab.output.renderOutput(dataTable, element);
        const docLink = document.createElement('div');
        docLink.innerHTML = docLinkHtml;
        element.appendChild(docLink);
      }
    </script>
  </div>


<div id="df-333e7f20-b2a1-4d96-82dc-8434ffaf6b47">
  <button class="colab-df-quickchart" onclick="quickchart('df-333e7f20-b2a1-4d96-82dc-8434ffaf6b47')"
            title="Suggest charts."
            style="display:none;">

<svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
     width="24px">
    <g>
        <path d="M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z"/>
    </g>
</svg>
  </button>

<style>
  .colab-df-quickchart {
    background-color: #E8F0FE;
    border: none;
    border-radius: 50%;
    cursor: pointer;
    display: none;
    fill: #1967D2;
    height: 32px;
    padding: 0 0 0 0;
    width: 32px;
  }

  .colab-df-quickchart:hover {
    background-color: #E2EBFA;
    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
    fill: #174EA6;
  }

  [theme=dark] .colab-df-quickchart {
    background-color: #3B4455;
    fill: #D2E3FC;
  }

  [theme=dark] .colab-df-quickchart:hover {
    background-color: #434B5C;
    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
    fill: #FFFFFF;
  }
</style>

  <script>
    async function quickchart(key) {
      const charts = await google.colab.kernel.invokeFunction(
          'suggestCharts', [key], {});
    }
    (() => {
      let quickchartButtonEl =
        document.querySelector('#df-333e7f20-b2a1-4d96-82dc-8434ffaf6b47 button');
      quickchartButtonEl.style.display =
        google.colab.kernel.accessAllowed ? 'block' : 'none';
    })();
  </script>
</div>
    </div>
  </div>





```python
print(data_csv.shape)
```

    (6508, 10)
    


```python
# ê°ê°ì˜ titleë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤.
# ìš°ë¦¬ëŠ” titleì˜ ì²« ë‹¨ì–´ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì„ ìˆ˜í–‰í•  ê²ƒì…ë‹ˆë‹¤.
data = data_csv['title'].values
```

#### ğŸ“ ì„¤ëª…: í…ìŠ¤íŠ¸ ë°ì´í„° ì „ì²˜ë¦¬í•˜ê¸°
* í•´ë‹¹ ë°ì´í„°ì…‹ì€ í¬ë¡¤ë§(ì¸í„°ë„·ì— ìˆëŠ” ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ëŠ” ê¸°ë²•)ì„ í†µí•´ êµ¬ì¶•ë˜ì—ˆê¸° ë•Œë¬¸ì— no-break spaceê°€ ì¢…ì¢… ë°œìƒí•©ë‹ˆë‹¤. ì´ëŸ¬í•œ no-break spaceë¥¼ ì œê±°í•˜ëŠ” ì „ì²˜ë¦¬ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.
  * No-Break Spaceë€? ì›¹ í˜ì´ì§€ë‚˜ ë¬¸ì„œ ë“±ì—ì„œ ë‹¨ì–´ë‚˜ ë¬¸ì¥ ì‚¬ì´ì˜ ê³µë°±ì´ ìˆëŠ” ê²½ìš°, í•´ë‹¹ ê³µë°±ì´ ì¤„ ë°”ê¿ˆìœ¼ë¡œ ì¸í•´ ë¶„ë¦¬ë˜ì§€ ì•Šê³  í•œ ë‹¨ì–´ë‚˜ ë¬¸ì¥ìœ¼ë¡œ ì¸ì‹ë˜ë„ë¡ í•˜ëŠ”ë° ì‚¬ìš©ë˜ëŠ” ê³µë°±
  * ì˜ˆì‹œ (no-break-space ì‚¬ìš© X)
    ```
    Hello
    World~
    ```
    
    (no-break-space ì‚¬ìš©)
    ```
    Hello,âµworld!
    ```
* no-break spaceë¥¼ ì œê±°í•˜ê¸° ìœ„í•´ì„  unicode í˜•ì‹ìœ¼ë¡œ ì œê±°ë¥¼ í•´ì•¼í•©ë‹ˆë‹¤.
  * unicodeë€? ì „ ì„¸ê³„ì˜ ëª¨ë“  ë¬¸ìì™€ ê¸°í˜¸ë¥¼ ì¼ê´€ì„± ìˆê²Œ í‘œí˜„í•˜ê¸° ìœ„í•œ í‘œì¤€ ë¬¸ì ì¸ì½”ë”© ì²´ê³„

* `re` ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì´ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì‰½ê²Œ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ğŸ“š ì°¸ê³ í• ë§Œí•œ ìë£Œ:
* <a href='https://www.compart.com/en/unicode'>unicode ê²€ìƒ‰ ì‚¬ì´íŠ¸</a>
* [re ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì´ìš©í•œ í…ìŠ¤íŠ¸ ë°ì´í„° ì‚¬ìš©ë²•](https://velog.io/@hoegon02/%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC-12-%ED%85%8D%EC%8A%A4%ED%8A%B8-%EC%A0%84%EC%B2%98%EB%A6%AC-%EC%A0%95%EA%B7%9C-%ED%91%9C%ED%98%84%EC%8B%9D-3qmtwryf)


```python
def cleaning_text(text):
    cleaned_text = re.sub( r"[^a-zA-Z0-9.,@#!\s']+", "", text) # íŠ¹ìˆ˜ë¬¸ì ë¥¼ ëª¨ë‘ ì§€ìš°ëŠ” ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    cleaned_text = cleaned_text.replace(u'\xa0',u' ') # No-break spaceë¥¼ unicode ë¹ˆì¹¸ìœ¼ë¡œ ë³€í™˜
    cleaned_text = cleaned_text.replace('\u200a',' ') # unicode ë¹ˆì¹¸ì„ ë¹ˆì¹¸ìœ¼ë¡œ ë³€í™˜
    return cleaned_text

cleaned_data = list(map(cleaning_text, data)) # ëª¨ë“  íŠ¹ìˆ˜ë¬¸ìì™€ ê³µë°±ì„ ì§€ì›€
print('Before preprocessing')
print(data[:5])
print('After preprocessing')
print(cleaned_data[:5])
```

    Before preprocessing
    ['A Beginnerâ€™s Guide to Word Embedding with Gensim Word2Vec\xa0Model'
     'Hands-on Graph Neural Networks with PyTorch & PyTorch Geometric'
     'How to Use ggplot2 in\xa0Python'
     'Databricks: How to Save Files in CSV on Your Local\xa0Computer'
     'A Step-by-Step Implementation of Gradient Descent and Backpropagation']
    After preprocessing
    ['A Beginners Guide to Word Embedding with Gensim Word2Vec Model', 'Handson Graph Neural Networks with PyTorch  PyTorch Geometric', 'How to Use ggplot2 in Python', 'Databricks How to Save Files in CSV on Your Local Computer', 'A StepbyStep Implementation of Gradient Descent and Backpropagation']
    

#### ğŸ“ ì„¤ëª…: Tokenizer
* TokenizerëŠ” í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì‘ì€ ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•´ì£¼ëŠ” ë„êµ¬ì…ë‹ˆë‹¤.
* í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë¨¸ì‹  ëŸ¬ë‹ ëª¨ë¸ì— ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•˜ê±°ë‚˜ ìì—°ì–´ ì²˜ë¦¬ ì‘ì—…ì„ ìˆ˜í–‰í•  ë•Œ, ë¬¸ì¥ì„ ë‹¨ì–´ ë˜ëŠ” í•˜ìœ„ ë‹¨ìœ„(subword)ë¡œ ë¶„ë¦¬í•˜ëŠ” ì—­í• ì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•œ ë„êµ¬ì…ë‹ˆë‹¤.
  * í…ìŠ¤íŠ¸ë¥¼ ë‹¨ì–´ ë˜ëŠ” í•˜ìœ„ ë‹¨ìœ„ë¡œ ë¶„ë¦¬ (í† í° ë¶„ë¦¬): í…ìŠ¤íŠ¸ë¥¼ ë„ì–´ì“°ê¸° ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ê±°ë‚˜, ë³´ë‹¤ ì‘ì€ ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•©ë‹ˆë‹¤.
    * ì˜ˆë¥¼ ë“¤ì–´, "I love PyTorch"ì´ë¼ëŠ” ë¬¸ì¥ì„ ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•˜ë©´ ["I", "love", "PyTorch"]ê³¼ ê°™ì´ ë©ë‹ˆë‹¤.
    * í•˜ìœ„ ë‹¨ìœ„ í† í¬ë‚˜ì´ì €ëŠ” ì–¸ì–´ì˜ íŠ¹ì„±ì— ë”°ë¼ ë‹¨ì–´ë¥¼ ë” ì‘ì€ ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•˜ì—¬ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, "playing"ì´ë¼ëŠ” ë‹¨ì–´ë¥¼ "play"ì™€ "ing"ìœ¼ë¡œ ë¶„ë¦¬í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

  * í† í°ì„ ìˆ«ìë¡œ ë§¤í•‘: ë¨¸ì‹  ëŸ¬ë‹ ëª¨ë¸ì€ í…ìŠ¤íŠ¸ë¥¼ ìˆ«ìë¡œ ì²˜ë¦¬í•´ì•¼ í•©ë‹ˆë‹¤. ë”°ë¼ì„œ ëª¨ë¸ì´ í…ìŠ¤íŠ¸ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆê²Œ ë‹¨ì–´ë‚˜ í•˜ìœ„ ë‹¨ìœ„ë¥¼ ê³ ìœ í•œ ìˆ«ì IDë¡œ ë§¤í•‘í•˜ëŠ” ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    * ì˜ˆë¥¼ ë“¤ì–´, ["I", "love", "PyTorch"] ì´ë¼ëŠ” ë‹¨ì–´ë“¤ì´ ìˆì„ ë•Œ, ì´ë¥¼ ì´ìš©í•˜ì—¬ {"I":0, "love":1, "PyTorch":2}ì™€ ê°™ì€ ë‹¨ì–´ ì‚¬ì „ì„ ë§Œë“¤ê³ , ì´ë¥¼ í†µí•´ [0, 1, 2]ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
  * íŠ¹ìˆ˜ í† í° ì¶”ê°€: í…ìŠ¤íŠ¸ë¥¼ ëª¨ë¸ì— ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•  ë•Œ, íŠ¹ë³„í•œ ì˜ë¯¸ë¥¼ ê°€ì§„ í† í°ì„ ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    * ì˜ˆë¥¼ ë“¤ì–´ ë¬¸ì¥ì˜ ì‹œì‘(<sos> í† í°)ê³¼ ë(<eox> í† í°)ì„ ë‚˜íƒ€ë‚´ëŠ”ë° ì‚¬ìš©ë˜ê±°ë‚˜, ë¯¸ë¦¬ ì •ì˜ëœ ì‚¬ì „ì— ì—†ëŠ” ë‹¨ì–´ë¥¼ ëŒ€ì²´í•˜ëŠ”ë° ì‚¬ìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    
* ìì—°ì–´ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ì¸ `torchtext.vocab.build_vocab_from_iterator`ë¥¼ ì´ìš©í•˜ì—¬ ìœ„ ê³¼ì •ì„ ëª¨ë‘ ì‰½ê²Œ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
  
ğŸ“š ì°¸ê³ í• ë§Œí•œ ìë£Œ:
* [torchtext getTokenizer](https://pytorch.org/text/stable/data_utils.html#get-tokenizer)
* [Vocab tokenize ì„¤ëª…](https://velog.io/@nkw011/nlp-vocab)

#### ğŸ“ ì„¤ëª…: build_vocab_from_iterator
`torchtext.vocab.build_vocab_from_iterator`ëŠ” iteratorë¥¼ ì´ìš©í•˜ì—¬ Vocab í´ë˜ìŠ¤(ë‹¨ì–´ì‚¬ì „)ë¥¼ ë§Œë“œëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.
* ì£¼ìš” parameter
  * iterator: ë‹¨ì–´ ì‚¬ì „ì„ ë§Œë“¤ ë•Œ ì‚¬ìš©ë˜ëŠ” iterator
  * min_freq: ë‹¨ì–´ ì‚¬ì „ì— í¬í•¨ë˜ê¸° ìœ„í•œ ìµœì†Œ ë¹ˆë„ ìˆ˜
* output
  * torchtext.vocab.Vocab í´ë˜ìŠ¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
  * ì´ë¡œì¨ Vocab classì— ìˆëŠ” í•¨ìˆ˜ë“¤ì„ ëª¨ë‘ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ğŸ“š ì°¸ê³ í• ë§Œí•œ ìë£Œ:
* [build_vocab_from_iterator](https://pytorch.org/text/stable/vocab.html#build-vocab-from-iterator)
* [Vocab classì˜ í•¨ìˆ˜ë“¤](https://pytorch.org/text/stable/vocab.html)


```python
# í† í¬ë‚˜ì´ì €ë¥¼ í†µí•´ ë‹¨ì–´ ë‹¨ìœ„ì˜ í† í°ì„ ìƒì„±í•©ë‹ˆë‹¤.
tokenizer = get_tokenizer("basic_english")
tokens = tokenizer(cleaned_data[0])
print("Original text : ", cleaned_data[0])
print("Token: ", tokens)
```

    Original text :  A Beginners Guide to Word Embedding with Gensim Word2Vec Model
    Token:  ['a', 'beginners', 'guide', 'to', 'word', 'embedding', 'with', 'gensim', 'word2vec', 'model']
    


```python
# ë‹¨ì–´ ì‚¬ì „ì„ ìƒì„±í•œ í›„, ì‹œì‘ê³¼ ë í‘œì‹œë¥¼ í•´ì¤ë‹ˆë‹¤.
vocab = torchtext.vocab.build_vocab_from_iterator(map(tokenizer, cleaned_data)) # ë‹¨ì–´ ì‚¬ì „ì„ ìƒì„±í•©ë‹ˆë‹¤.
vocab.insert_token('<pad>', 0)
```


```python
id2token = vocab.get_itos() # id to string
id2token[:10]
```




    ['<pad>', 'to', 'the', 'a', 'of', 'and', 'how', 'in', 'your', 'for']




```python
token2id = vocab.get_stoi() # string to id
token2id = dict(sorted(token2id.items(), key=lambda item: item[1]))
for idx, (k,v) in enumerate(token2id.items()):
    print(k,v)
    if idx == 5:
        break
```

    <pad> 0
    to 1
    the 2
    a 3
    of 4
    and 5
    


```python
vocab.lookup_indices(tokenizer(cleaned_data[0])) # ë¬¸ì¥ì„ í† í°í™” í›„ idë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
```




    [3, 273, 66, 1, 467, 1582, 12, 2884, 8549, 99]



#### ğŸ“ ì„¤ëª…: ë°ì´í„° ì „ì²˜ë¦¬

  
* inputì— ë“¤ì–´ê°€ëŠ” ë‹¨ì–´ ìˆ˜ê°€ ëª¨ë‘ ë‹¤ë¥´ë¯€ë¡œ ì´ë¥¼ ë°”ë¡œ ëª¨ë¸ì— ë„£ê¸°ì—ëŠ” ì–´ë µìŠµë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´, \<pad\> (0)ì„ ë„£ì–´ì„œ ê¸¸ì´ë¥¼ ë§ì¶°ì£¼ëŠ” ê³¼ì •ì„ padding ì´ë¼ê³  í•©ë‹ˆë‹¤.
<!-- * label ê°’ì€ OneHotEncodingì„ í•´ì•¼í•©ë‹ˆë‹¤.
  * torch.nn.functional.one_hot í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ì—¬ onehot encodingì„ ì‰½ê²Œ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
  * OneHotEncodingì´ë€? : ì¹´í…Œê³ ë¦¬ í˜•íƒœì˜ ë°ì´í„°ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” ë°©ë²•ìœ¼ë¡œ, í•´ë‹¹í•˜ëŠ” ì¹´í…Œê³ ë¦¬ì— í•´ë‹¹í•˜ëŠ” ì¸ë±ìŠ¤ë§Œ 1ì´ê³  ë‚˜ë¨¸ì§€ëŠ” ëª¨ë‘ 0ì¸ ì´ì§„ ë²¡í„°ë¡œ í‘œí˜„í•˜ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.
  * ì™œ OneHotEncodingdì„ í•´ì•¼í• ê¹Œ? : multi-class(ê°œ, ê³ ì–‘ì´, í† ë¼ ë¶„ë¥˜ì™€ ê°™ì€) ë¬¸ì œë¡œ í’€ê¸° ìœ„í•¨ì…ë‹ˆë‹¤.   -->
  
ğŸ“š ì°¸ê³ í• ë§Œí•œ ìë£Œ:
* [Padding ì„¤ëª…](https://wikidocs.net/83544)


```python
seq = []
for i in cleaned_data:
    token_id = vocab.lookup_indices(tokenizer(i))
    for j in range(1, len(token_id)):
        sequence = token_id[:j+1]
        seq.append(sequence)
```


```python
seq[:5]
```




    [[3, 273],
     [3, 273, 66],
     [3, 273, 66, 1],
     [3, 273, 66, 1, 467],
     [3, 273, 66, 1, 467, 1582]]




```python
max_len = max(len(sublist) for sublist in seq) # seqì— ì €ì¥ëœ ìµœëŒ€ í† í° ê¸¸ì´ ì°¾ê¸°
print(max_len)
```

    24
    


```python
def pre_zeropadding(seq, max_len): # max_len ê¸¸ì´ì— ë§ì¶°ì„œ 0 ìœ¼ë¡œ padding ì²˜ë¦¬ (ì•ë¶€ë¶„ì— padding ì²˜ë¦¬)
    return np.array([i[:max_len] if len(i) >= max_len else [0] * (max_len - len(i)) + i for i in seq])
zero_padding_data = pre_zeropadding(seq, max_len)
zero_padding_data[0]
```




    array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
             0,   0,   0,   0,   0,   0,   0,   0,   0,   3, 273])




```python
input_x = zero_padding_data[:,:-1]
label = zero_padding_data[:,-1]
```


```python
input_x[:5] # input ê°’ í™•ì¸
```




    array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0,   0,   0,   0,   0,   0,   0,   0,   3],
           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0,   0,   0,   0,   0,   0,   0,   3, 273],
           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0,   0,   0,   0,   0,   0,   3, 273,  66],
           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0,   0,   0,   0,   0,   3, 273,  66,   1],
           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0,   0,   0,   0,   3, 273,  66,   1, 467]])




```python
label[:5] # label ê°’ í™•ì¸
```




    array([ 273,   66,    1,  467, 1582])



### 1-2 Custom Dataset êµ¬í˜„

> 1-1ì—ì„œ ì§„í–‰í•œ ì „ì²˜ë¦¬ ì§„í–‰ì„ ëª¨ë“ˆí™” ì‹œì¼œì„œ í•˜ë‚˜ì˜ classë¡œ êµ¬í˜„í•©ë‹ˆë‹¤.


#### ğŸ“ ì„¤ëª…: Custom Dataset ì •ì˜í•˜ê¸°
* 1-1ì—ì„œ ì§„í–‰í•œ ì „ì²˜ë¦¬ ê³¼ì •ì„ ëª¨ë‘ í•¨ìˆ˜í™” ì‹œì¼œì„œ í•˜ë‚˜ì˜ classë¡œ êµ¬ì¶•í•©ë‹ˆë‹¤.
* ì´ë¡œ ì¸í•´, ì†ì‰¬ìš´ ëª¨ë“ˆí™”ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤.
* ë°ì´í„°ë¥¼ ë³€í™˜í•˜ëŠ” ê³¼ì •ì€ ë˜ë„ë¡ì´ë©´ getitem ì´ ì•„ë‹Œ init ë¶€ë¶„ì— í•˜ì—¬, ì „ì²˜ë¦¬í•˜ëŠ” ì‹œê°„ì„ ì¤„ì´ë„ë¡ í•©ë‹ˆë‹¤.
  * init ë¶€ë¶„ì— í•œ ë²ˆì— í•˜ê²Œ ë˜ë©´ datasetì„ ì •ì˜í•  ë•Œë§Œ ë³€í™˜ ì‹œê°„ì´ ì†Œìš”ë˜ê³ , ê·¸ ì´í›„ë¡œëŠ” ë°ì´í„° ì „ì²˜ë¦¬ ì‹œê°„ì´ ì†Œìš”ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

ğŸ“š ì°¸ê³ í• ë§Œí•œ ìë£Œ:
* [Custom Dataset êµ¬ì¶• - Pytorch ê³µì‹ íŠœí† ë¦¬ì–¼](https://tutorials.pytorch.kr/beginner/basics/data_tutorial.html)


```python
class CustomDataset(Dataset):
    def __init__(self, data, vocab, tokenizer, max_len):
        self.data = data
        self.vocab = vocab
        self.max_len = max_len
        self.tokenizer = tokenizer
        seq = self.make_sequence(self.data, self.vocab, self.tokenizer) # next word predictionì„ í•˜ê¸° ìœ„í•œ í˜•íƒœë¡œ ë³€í™˜
        self.seq = self.pre_zeropadding(seq, self.max_len) # zero paddingìœ¼ë¡œ ì±„ì›Œì¤Œ
        self.X = torch.tensor(self.seq[:,:-1])
        self.label = torch.tensor(self.seq[:,-1])

    def make_sequence(self, data, vocab, tokenizer):
        seq = []
        for i in data:
            token_id = vocab.lookup_indices(tokenizer(i))
            for j in range(1, len(token_id)):
                sequence = token_id[:j+1]
                seq.append(sequence)
        return seq

    def pre_zeropadding(self, seq, max_len): # max_len ê¸¸ì´ì— ë§ì¶°ì„œ 0 ìœ¼ë¡œ padding ì²˜ë¦¬ (ì•ë¶€ë¶„ì— padding ì²˜ë¦¬)
        return np.array([i[:max_len] if len(i) >= max_len else [0] * (max_len - len(i)) + i for i in seq])

    def __len__(self): # datasetì˜ ì „ì²´ ê¸¸ì´ ë°˜í™˜
        return len(self.X)

    def __getitem__(self, idx): # dataset ì ‘ê·¼
        X = self.X[idx]
        label = self.label[idx]
        return X, label
```


```python
def cleaning_text(text):
    cleaned_text = re.sub( r"[^a-zA-Z0-9.,@#!\s']+", "", text) # íŠ¹ìˆ˜ë¬¸ì ë¥¼ ëª¨ë‘ ì§€ìš°ëŠ” ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    cleaned_text = cleaned_text.replace(u'\xa0',u' ') # No-break spaceë¥¼ unicode ë¹ˆì¹¸ìœ¼ë¡œ ë³€í™˜
    cleaned_text = cleaned_text.replace('\u200a',' ') # unicode ë¹ˆì¹¸ì„ ë¹ˆì¹¸ìœ¼ë¡œ ë³€í™˜
    return cleaned_text

data = list(map(cleaning_text, data))
tokenizer = get_tokenizer("basic_english")
vocab = torchtext.vocab.build_vocab_from_iterator(map(tokenizer, data))
vocab.insert_token('<pad>',0)
max_len = 20
```


```python
# train set, validation set, test setìœ¼ë¡œ data setì„ ë‚˜ëˆ•ë‹ˆë‹¤. 8 : 1 : 1 ì˜ ë¹„ìœ¨ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤.
train, test = train_test_split(data, test_size = .2, random_state = 42)
val, test = train_test_split(test, test_size = .5, random_state = 42)
```


```python
print("Train ê°œìˆ˜: ", len(train))
print("Validation ê°œìˆ˜: ", len(val))
print("Test ê°œìˆ˜: ", len(test))
```

    Train ê°œìˆ˜:  5206
    Validation ê°œìˆ˜:  651
    Test ê°œìˆ˜:  651
    


```python
train_dataset = CustomDataset(train, vocab, tokenizer, max_len)
valid_dataset = CustomDataset(val, vocab, tokenizer, max_len)
test_dataset = CustomDataset(test, vocab, tokenizer, max_len)
```


```python
batch_size = 32

train_dataloader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)
valid_dataloader = DataLoader(valid_dataset, batch_size = batch_size, shuffle = False)
test_dataloader = DataLoader(test_dataset, batch_size = batch_size, shuffle = False)
```

## 2. Next word prediction ëª¨ë¸ êµ¬í˜„


- 2-1. Next word predictionì„ ìœ„í•œ DNN ëª¨ë¸ êµ¬í˜„
- 2-2. ëª¨ë¸ í•™ìŠµ ë° ì¶”ë¡ 

### 2-1 Next word predictionì„ ìœ„í•œ DNN ëª¨ë¸ êµ¬ì¶•

> Next word predictionì„ ìœ„í•œ DNN ëª¨ë¸ì„ ì§ì ‘ êµ¬ì¶•í•´ë´…ë‹ˆë‹¤.


#### ğŸ“ ì„¤ëª…: Next word predictionì„ ìœ„í•œ DNN ëª¨ë¸ êµ¬ì¶•
* DNN êµ¬í˜„ (2)ì—ì„œ í•™ìŠµí•˜ì˜€ë˜, DNN ëª¨ë¸ì„ ê¸°ë°˜ì— `nn.Embedding`ì„ ì¶”ê°€í•˜ì—¬ next word predictionì„ í•˜ê¸° ìœ„í•œ DNN ëª¨ë¸ì„ êµ¬ì¶•í•©ë‹ˆë‹¤.
* Embeddingì´ë€?
  * í…ìŠ¤íŠ¸ë‚˜ ë²”ì£¼í˜• ë°ì´í„°ì™€ ê°™ì´ ëª¨ë¸ì´ ì²˜ë¦¬í•˜ê¸° ì–´ë ¤ìš´ í˜•íƒœì˜ ë°ì´í„°ë¥¼ ìˆ˜ì¹˜ í˜•íƒœë¡œ ë³€í™˜í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.
  * ì£¼ì–´ì§„ ë°ì´í„°ë¥¼ ì €ì°¨ì›ì˜ ë²¡í„° ê³µê°„ì— í‘œí˜„í•˜ëŠ” ë°©ë²•ìœ¼ë¡œ, ë‹¨ì–´, ë¬¸ì¥, ë²”ì£¼í˜• ë³€ìˆ˜ ë“±ì„ ê³ ì •ëœ ê¸¸ì´ì˜ ì‹¤ìˆ˜ ë²¡í„°ë¡œ ë§¤í•‘í•˜ì—¬ í‘œí˜„í•©ë‹ˆë‹¤.
* `nn.Embedding`
  * num_embedding : embeddingí•  inputê°’ì˜ ìˆ˜ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. ìì—°ì–´ì²˜ë¦¬ì—ì„  ë‹¨ì–´ ì‚¬ì „ì˜ í¬ê¸°ì™€ ë™ì¼í•©ë‹ˆë‹¤.
  * embedding_dim : embedding ë²¡í„°ì˜ ì°¨ì›ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.
  
ğŸ“š ì°¸ê³ í• ë§Œí•œ ìë£Œ:
* [torch.nn.Embedding - Pytorch ê³µì‹ íŠœí† ë¦¬ì–¼](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)
* [Embedding ì„¤ëª…](https://wikidocs.net/64779)


```python
class NextWordPredictionModel(nn.Module):
    def __init__(self, vocab_size, embedding_dims, hidden_dims, num_classes, dropout_ratio, set_super):
        if set_super:
            super().__init__()

        self.embedding = nn.Embedding(vocab_size, embedding_dims, padding_idx = 0) # padding index ì„¤ì • => gradient ê³„ì‚°ì—ì„œ ì œì™¸
        self.hidden_dims = hidden_dims
        self.layers = nn.ModuleList()
        self.num_classes = num_classes
        for i in range(len(self.hidden_dims) - 1):
            self.layers.append(nn.Linear(self.hidden_dims[i], self.hidden_dims[i+1]))

            self.layers.append(nn.BatchNorm1d(self.hidden_dims[i+1]))

            self.layers.append(nn.ReLU())

            self.layers.append(nn.Dropout(dropout_ratio))

        self.classifier = nn.Linear(self.hidden_dims[-1], self.num_classes)
        self.softmax = nn.LogSoftmax(dim = 1)

    def forward(self, x):
        '''
        INPUT:
            x: [batch_size, sequence_len] # padding ì œì™¸
        OUTPUT:
            output : [batch_size, vocab_size]
        '''
        x = self.embedding(x) # [batch_size, sequence_len, embedding_dim]
        x = torch.sum(x, dim=1) # [batch_size, embedding_dim] ê° ë¬¸ì¥ì— ëŒ€í•´ ì„ë² ë”©ëœ ë‹¨ì–´ë“¤ì„ í•©ì³ì„œ, í•´ë‹¹ ë¬¸ì¥ì— ëŒ€í•œ ì„ë² ë”© ë²¡í„°ë¡œ ë§Œë“¤ì–´ì¤ë‹ˆë‹¤.
        for layer in self.layers:
            x = layer(x)

        output = self.classifier(x) # [batch_size, num_classes]
        output = self.softmax(output) # [batch_size, num_classes]
        return output

    def count_parameters(self):
        return sum(p.numel() for p in self.parameters() if p.requires_grad)
```

### 2-2 ëª¨ë¸ í•™ìŠµ ë° ì¶”ë¡ 

> Next word prediction ëª¨ë¸ì„ ì§ì ‘ í•™ìŠµí•˜ê³ , textë¥¼ ì§ì ‘ ë„£ì–´ next word predictionì„ ì§ì ‘ ìˆ˜í–‰í•´ë´…ë‹ˆë‹¤.


#### ğŸ“ ì„¤ëª…: Next word prediction í•™ìŠµí•˜ê¸°
* DNN ëª¨ë¸ì„ í•™ìŠµí•˜ê¸° ìœ„í•´ ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ì •í•´ì¤ë‹ˆë‹¤.
* embedding layerì™€ fully connected layerì˜ ì—°ì‚°ì´ ê°€ëŠ¥í•˜ê²Œ í•˜ê¸° ìœ„í•´ hidden dimension ë¦¬ìŠ¤íŠ¸ êµ¬ì„± ì‹œ, embedding dimensionì„ ì²«ë²ˆì§¸ ê°’ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.
* ì˜ˆì¸¡í•˜ë ¤ëŠ” labelì˜ ê°œìˆ˜ëŠ” ë‹¨ì–´ ì‚¬ì „ì— ìˆëŠ” ë‹¨ì–´ì˜ ê°œìˆ˜ì™€ ë™ì¼í•©ë‹ˆë‹¤.


```python
# training ì½”ë“œ, evaluation ì½”ë“œ, training loop ì½”ë“œ
def training(model, dataloader, train_dataset, criterion, optimizer, device, epoch, num_epochs):
    model.train()  # ëª¨ë¸ì„ í•™ìŠµ ëª¨ë“œë¡œ ì„¤ì •
    train_loss = 0.0
    train_accuracy = 0

    tbar = tqdm(dataloader)
    for texts, labels in tbar:
        texts = texts.to(device)
        labels = labels.to(device)

        # ìˆœì „íŒŒ
        outputs = model(texts)

        loss = criterion(outputs, labels)

        # ì—­ì „íŒŒ ë° ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        # ì†ì‹¤ê³¼ ì •í™•ë„ ê³„ì‚°
        train_loss += loss.item()
        # torch.maxì—ì„œ dim ì¸ìì— ê°’ì„ ì¶”ê°€í•  ê²½ìš°, í•´ë‹¹ dimensionì—ì„œ ìµœëŒ“ê°’ê³¼ ìµœëŒ“ê°’ì— í•´ë‹¹í•˜ëŠ” ì¸ë±ìŠ¤ë¥¼ ë°˜í™˜
        _, predicted = torch.max(outputs, dim=1)


        train_accuracy += (predicted == labels).sum().item()

        # tqdmì˜ ì§„í–‰ë°”ì— í‘œì‹œë  ì„¤ëª… í…ìŠ¤íŠ¸ë¥¼ ì„¤ì •
        tbar.set_description(f"Epoch [{epoch+1}/{num_epochs}], Train Loss: {loss.item():.4f}")

    # ì—í­ë³„ í•™ìŠµ ê²°ê³¼ ì¶œë ¥
    train_loss = train_loss / len(dataloader)
    train_accuracy = train_accuracy / len(train_dataset)

    return model, train_loss, train_accuracy

def evaluation(model, dataloader, val_dataset, criterion, device, epoch, num_epochs):
    model.eval()  # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
    valid_loss = 0.0
    valid_accuracy = 0

    with torch.no_grad(): # modelì˜ ì—…ë°ì´íŠ¸ ë§‰ê¸°
        tbar = tqdm(dataloader)
        for texts, labels in tbar:
            texts = texts.to(device)
            labels = labels.to(device)

            # ìˆœì „íŒŒ
            outputs = model(texts)
            loss = criterion(outputs, labels)

            # ì†ì‹¤ê³¼ ì •í™•ë„ ê³„ì‚°
            valid_loss += loss.item()
            # torch.maxì—ì„œ dim ì¸ìì— ê°’ì„ ì¶”ê°€í•  ê²½ìš°, í•´ë‹¹ dimensionì—ì„œ ìµœëŒ“ê°’ê³¼ ìµœëŒ“ê°’ì— í•´ë‹¹í•˜ëŠ” ì¸ë±ìŠ¤ë¥¼ ë°˜í™˜
            _, predicted = torch.max(outputs, 1)
            # _, true_labels = torch.max(labels, dim=1)
            valid_accuracy += (predicted == labels).sum().item()


            # tqdmì˜ ì§„í–‰ë°”ì— í‘œì‹œë  ì„¤ëª… í…ìŠ¤íŠ¸ë¥¼ ì„¤ì •
            tbar.set_description(f"Epoch [{epoch+1}/{num_epochs}], Valid Loss: {loss.item():.4f}")

    valid_loss = valid_loss / len(dataloader)
    valid_accuracy = valid_accuracy / len(val_dataset)

    return model, valid_loss, valid_accuracy


def training_loop(model, train_dataloader, valid_dataloader, train_dataset, val_dataset, criterion, optimizer, device, num_epochs, patience, model_name):
    best_valid_loss = float('inf')  # ê°€ì¥ ì¢‹ì€ validation lossë¥¼ ì €ì¥
    early_stop_counter = 0  # ì¹´ìš´í„°
    valid_max_accuracy = -1

    for epoch in range(num_epochs):
        model, train_loss, train_accuracy = training(model, train_dataloader, train_dataset, criterion, optimizer, device, epoch, num_epochs)
        model, valid_loss, valid_accuracy = evaluation(model, valid_dataloader, val_dataset, criterion, device, epoch, num_epochs)

        if valid_accuracy > valid_max_accuracy:
            valid_max_accuracy = valid_accuracy

        # validation lossê°€ ê°ì†Œí•˜ë©´ ëª¨ë¸ ì €ì¥ ë° ì¹´ìš´í„° ë¦¬ì…‹
        if valid_loss < best_valid_loss:
            best_valid_loss = valid_loss
            torch.save(model.state_dict(), f"./model_{model_name}.pt")
            early_stop_counter = 0

        # validation lossê°€ ì¦ê°€í•˜ê±°ë‚˜ ê°™ìœ¼ë©´ ì¹´ìš´í„° ì¦ê°€
        else:
            early_stop_counter += 1

        print(f"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f} Valid Loss: {valid_loss:.4f}, Valid Accuracy: {valid_accuracy:.4f}")

        # ì¡°ê¸° ì¢…ë£Œ ì¹´ìš´í„°ê°€ ì„¤ì •í•œ patienceë¥¼ ì´ˆê³¼í•˜ë©´ í•™ìŠµ ì¢…ë£Œ
        if early_stop_counter >= patience:
            print("Early stopping")
            break

    return model, valid_max_accuracy
```


```python
lr = 1e-3
vocab_size = len(vocab.get_stoi())
embedding_dims = 512
hidden_dims = [embedding_dims, embedding_dims*4, embedding_dims*2, embedding_dims]
model = NextWordPredictionModel(vocab_size = vocab_size, embedding_dims = embedding_dims, hidden_dims = hidden_dims, num_classes = vocab_size, \
            dropout_ratio = 0.2, set_super = True).to(device)

num_epochs = 100
patience = 3
model_name = 'next'

optimizer = optim.Adam(model.parameters(), lr = lr)
criterion = nn.NLLLoss(ignore_index=0) # padding í•œ ë¶€ë¶„ ì œì™¸
model, valid_max_accuracy = training_loop(model, train_dataloader, valid_dataloader, train_dataset, valid_dataset, criterion, optimizer, device, num_epochs, patience, model_name)
print('Valid max accuracy : ', valid_max_accuracy)
```


      0%|          | 0/1159 [00:00<?, ?it/s]



      0%|          | 0/149 [00:00<?, ?it/s]


    Epoch [1/100], Train Loss: 7.3844, Train Accuracy: 0.0641 Valid Loss: 7.2101, Valid Accuracy: 0.0680
    


      0%|          | 0/1159 [00:00<?, ?it/s]



      0%|          | 0/149 [00:00<?, ?it/s]


    Epoch [2/100], Train Loss: 6.7291, Train Accuracy: 0.0776 Valid Loss: 7.2159, Valid Accuracy: 0.0779
    


      0%|          | 0/1159 [00:00<?, ?it/s]



      0%|          | 0/149 [00:00<?, ?it/s]


    Epoch [3/100], Train Loss: 6.3590, Train Accuracy: 0.0871 Valid Loss: 7.3250, Valid Accuracy: 0.0842
    


      0%|          | 0/1159 [00:00<?, ?it/s]



      0%|          | 0/149 [00:00<?, ?it/s]


    Epoch [4/100], Train Loss: 6.0647, Train Accuracy: 0.0950 Valid Loss: 7.4306, Valid Accuracy: 0.0865
    Early stopping
    Valid max accuracy :  0.08653440270156185
    

#### ğŸ“ ì„¤ëª…: Next word prediction í‰ê°€í•˜ê¸°
* í•™ìŠµí•œ DNN ëª¨ë¸ì„ accuracy scoreë¡œ í‰ê°€í•©ë‹ˆë‹¤.


```python
model.load_state_dict(torch.load("./model_next.pt")) # ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
model = model.to(device)
model.eval()
total_labels = []
total_preds = []
with torch.no_grad():
    for texts, labels in tqdm(test_dataloader):
        texts = texts.to(device)
        labels = labels

        outputs = model(texts)
        # torch.maxì—ì„œ dim ì¸ìì— ê°’ì„ ì¶”ê°€í•  ê²½ìš°, í•´ë‹¹ dimensionì—ì„œ ìµœëŒ“ê°’ê³¼ ìµœëŒ“ê°’ì— í•´ë‹¹í•˜ëŠ” ì¸ë±ìŠ¤ë¥¼ ë°˜í™˜
        _, predicted = torch.max(outputs.data, 1)

        total_preds.extend(predicted.detach().cpu().tolist())
        total_labels.extend(labels.tolist())

total_preds = np.array(total_preds)
total_labels = np.array(total_labels)
nwp_dnn_acc = accuracy_score(total_labels, total_preds) # ì •í™•ë„ ê³„ì‚°
print("Next word prediction DNN model accuracy : ", nwp_dnn_acc)
```


      0%|          | 0/143 [00:00<?, ?it/s]


    Next word prediction DNN model accuracy :  0.07239720034995625
    


```python
print(vocab_size)
```

    8618
    

## Required Package

> torch == 2.0.1

> torchvision == 0.15.2

> sklearn == 1.3.0

> torchtext == 0.15.2


```python

```
