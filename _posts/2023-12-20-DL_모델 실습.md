---
layout: post
title: DL ëª¨ë¸ ì‹¤ìŠµ
date: 2023-12-20 15:32 +0900
last_modified_at: 2023-12-20 16:20:00 +0900
tags: [DeepLearning]
toc:  true
---


# (ì‹¤ìŠµ-1) ëª¨ë¸ í•™ìŠµë²• ì‹¤ìŠµ

## ì‹¤ìŠµ ê°œìš”

1) ì‹¤ìŠµ ëª©ì 

&ensp;&ensp;ë³¸ ì‹¤ìŠµì€ [Chapter 2]ì—ì„œ ë°°ìš´ ê°œë…ë“¤ì„ í† ëŒ€ë¡œ **ê°€ì¥ ê¸°ë³¸ì ì¸ ë”¥ëŸ¬ë‹ íŒŒì´í”„ë¼ì¸ì„ ì²˜ìŒë¶€í„° ëê¹Œì§€ êµ¬í˜„**í•˜ëŠ” ê²ƒì„ ëª©ì . <br>
&ensp;&ensp;<u>ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡ (Multi-Layer Perceptron, MLP)</u>ì˜ íŒŒë¼ë¯¸í„°ë“¤ì´
 <u>í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²•(Stochastic Gradient Descent, SGD)</u>ê³¼
 <u>í‰ê· ì œê³±ì˜¤ì°¨(Mean Squared Error, MSE) ë‚´ì§€ëŠ” êµì°¨ì—”íŠ¸ë¡œí”¼(Cross Entropy, CE)</u>ë¥¼ í†µí•´ ì–´ë–»ê²Œ í•™ìŠµì´ ë˜ëŠ”ì§€ ì§ì ‘ êµ¬í˜„í•´ë´„ìœ¼ë¡œì¨ **ë”¥ëŸ¬ë‹ì˜ í•„ìˆ˜ ìš”ì†Œì¸ 'ëª¨ë¸'ì´ 'ìµœì í™” ì•Œê³ ë¦¬ì¦˜'ì„ í†µí•´ 'ì†ì‹¤ í•¨ìˆ˜'ë¥¼ ìµœì†Œí™”í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ í•™ìŠµë˜ëŠ” ê³¼ì •ì— ëŒ€í•œ ì´í•´**.

<br>


### ì‹¤ìŠµ ëª©ì°¨

- 1. MNIST ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸°
  - 1-1. MNIST ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ë° ì••ì¶• í•´ì œ
  - 1-2. MNIST ë°ì´í„°ì…‹ ì „ì²˜ë¦¬

- 2. ê°„ë‹¨í•œ MLP ëª¨ë¸ êµ¬ì„±í•˜ê¸°
  - 2-1. í™œì„±í™” í•¨ìˆ˜ì˜ êµ¬í˜„
  - 2-2. MLP ë ˆì´ì–´ì˜ êµ¬í˜„
  - 2-3. ì†ì‹¤ í•¨ìˆ˜ì˜ êµ¬í˜„
  - 2-4. MLP ëª¨ë¸ì˜ êµ¬í˜„

- 3. ëª¨ë¸ í•™ìŠµí•˜ê¸°
  - 3-1. ëª¨ë¸ í•™ìŠµ ì „ ì¤€ë¹„
  - 3-2. ëª¨ë¸ í•™ìŠµí•˜ê¸°
- 4. ê²°ê³¼ ì‹œê°í™”í•˜ê¸°
  - 4-1. í•™ìŠµ ê³¡ì„  ì‹œê°í™”
  - 4-2. ì¼ë¶€ ë°ì´í„° ì‹œê°í™”

### ì‹¤ìŠµ êµ¬ì„±


### ë°ì´í„°ì…‹ ê°œìš”

<div align="center">
<img src="https://upload.wikimedia.org/wikipedia/commons/f/f7/MnistExamplesModified.png" alt="MNIST Test Data Samples">
</div>


* ë°ì´í„°ì…‹ :
MNIST(Modified National Institute of Standards and Technology) Database

* ë°ì´í„°ì…‹ ê°œìš” :
<br>&ensp;&ensp;MNIST ë°ì´í„°ì…‹ì€ ë¯¸êµ­ì˜ NISTì—ì„œ ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹œìŠ¤í…œì„ ìœ„í•´ ëª¨ì€ ì†ê¸€ì”¨ ì´ë¯¸ì§€ ë°ì´í„°ì…‹ NIST Special Database 3 ì¤‘ ì¼ë¶€ë¥¼ ì¬êµ¬ì„±í•œ ê²ƒìœ¼ë¡œ, 0ë¶€í„° 9ê¹Œì§€ì˜ ìˆ«ì ì´ë¯¸ì§€ì™€ ì´ì— ëŒ€ì‘ë˜ëŠ” ìˆ«ìê°€ í•œ ìŒìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŒ. ì´ ë°ì´í„°ì…‹ì€ ë¨¸ì‹ ëŸ¬ë‹ ë° ë”¥ëŸ¬ë‹ ë¶„ì•¼ì—ì„œ ê°€ì¥ ì˜ ì•Œë ¤ì§„ ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹ ì¤‘ í•˜ë‚˜ë¡œ, íŠ¹íˆ ë”¥ëŸ¬ë‹ ê¸°ì´ˆ ê´€ë ¨ êµë³´ì¬ì— ì£¼ë¡œ í™œìš©.

  - ë°ì´í„°ì…‹ êµ¬ì„±
    - ì…ë ¥(ì´ë¯¸ì§€) : 0ë¶€í„° 9ê¹Œì§€ì˜ ìˆ«ì ì¤‘ í•˜ë‚˜ì— ì†í•˜ëŠ” 28px * 28pxì˜ í‘ë°± ì´ë¯¸ì§€
    - ì¶œë ¥(ìˆ«ì) : ì£¼ì–´ì§„ ì´ë¯¸ì§€ì— ëŒ€ì‘ë˜ëŠ” ìˆ«ì (0~9)
  - ë°ì´í„° ìƒ˜í”Œ ìˆ˜
    - í•™ìŠµ ë°ì´í„°&ensp;&ensp;: 60,000 ê°œ
    - í…ŒìŠ¤íŠ¸ ë°ì´í„°&thinsp;: 10,000 ê°œ

* ë°ì´í„°ì…‹ ì €ì‘ê¶Œ: [CC BY-SA 3.0](https://creativecommons.org/licenses/by-sa/3.0/)



### í™˜ê²½ ì„¤ì •

í•´ë‹¹ ì‹¤ìŠµì—ì„œ í•„ìš”í•œ íŒ¨í‚¤ì§€ ì •ë³´ëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

colabì—ì„œ ì œê³µí•˜ëŠ” ìµœì‹  ë²„ì „ì˜ `numpy` ë° `matplotlib`ë¥¼ ì‚¬ìš©.

`numpy`, `matplotlib` ì™¸ì˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë²„ì „ì€ ìµœí•˜ë‹¨ì˜ **Package**ë¥¼ ì°¸ê³ .
```
numpy >= 1.23.5
matplotlib >= 3.7.1
```



```python
### ëª¨ë¸ í•™ìŠµ ë° ê²°ê³¼ ì‹œê°í™” í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
# numpy : í–‰ë ¬ ì—°ì‚°ì— í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬. MNIST ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ê³ , ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ëŠ”ë° ì‚¬ìš©.
# matplotlib : ëŒ€í‘œì ì¸ ì‹œê°í™” ë¼ì´ë¸ŒëŸ¬ë¦¬. í•™ìŠµ ê²°ê³¼ë¥¼ ì‹œê°í™”í•˜ëŠ”ë° ì‚¬ìš©.
# tqdm : forë¬¸ì˜ ì§„í–‰ ìƒíƒœë¥¼ í‘œì‹œí•˜ëŠ”ë° ì‚¬ìš©í•˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬. í•™ìŠµ ì‹œê°„ ë° ì§„í–‰ ìƒíƒœë¥¼ í™•ì¸í•˜ëŠ”ë° ì‚¬ìš©.
import numpy as np
import matplotlib.pyplot as plt
from tqdm.notebook import tqdm

### ê°ì¢… ìœ í‹¸ë¦¬í‹° ë¼ì´ë¸ŒëŸ¬ë¦¬
# os : íŒŒì¼ ê²½ë¡œë¥¼ í™•ì¸í•˜ëŠ”ë° ì‚¬ìš©í•˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬.
# random : ë°ì´í„°ë¥¼ ëœë¤í•˜ê²Œ ì„ëŠ”ë° ì‚¬ìš©í•˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬.
# platform : í˜„ì¬ ì‚¬ìš©í•˜ëŠ” ìš´ì˜ì²´ì œë¥¼ í™•ì¸í•˜ëŠ”ë° ì‚¬ìš©í•˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬.
# warnings : ê²½ê³  ë©”ì‹œì§€ë¥¼ ë¬´ì‹œí•˜ê±°ë‚˜ ìˆ¨ê¸°ëŠ”ë° ì‚¬ìš©í•˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬.
import os
import random
import platform
import warnings

### MNIST ë°ì´í„°ì…‹ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ë°ì— ì‚¬ìš©í•˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬
# gzip : ì••ì¶•ëœ íŒŒì¼ì„ ì½ëŠ”ë° ì‚¬ìš©í•˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬.
# urlretrieve : urlë¡œë¶€í„° ë°ì´í„°ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ëŠ”ë° ì‚¬ìš©í•˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬.
import gzip
from urllib.request import urlretrieve
```


```python
# ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ê³ ì •
SEED = 0
random.seed(SEED)
np.random.seed(SEED)
os.environ["PYTHONHASHSEED"] = str(SEED)
```


```python
# í˜„ì¬ OS ë° íŒŒì´ì¬ ë²„ì „ í™•ì¸
current_os = platform.system()
print(f"Current OS: {current_os}")
print(f"Python Version: {platform.python_version()}")
```

    Current OS: Linux
    Python Version: 3.10.12
    


```python
# ë¼ì´ë¸ŒëŸ¬ë¦¬ ë²„ì „ í™•ì¸
from importlib.metadata import version
print("numpy version : {}".format(version("numpy")))
print("matplotlib version : {}".format(version("matplotlib")))
```

    numpy version : 1.23.5
    matplotlib version : 3.7.1
    


```python
# ì¤‘ìš”í•˜ì§€ ì•Šì€ ì—ëŸ¬ ë¬´ì‹œ
warnings.filterwarnings(action='ignore')
```


```python
# matplotlibì˜ í•´ìƒë„ ë†’ì´ê¸°
%matplotlib inline
%config InlineBackend.figure_format = 'retina'
```

---

## 1. MNIST ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸°

```
ğŸ’¡ ëª©ì°¨ ê°œìš” : ë”¥ëŸ¬ë‹ ëª¨ë¸ì— ì‚¬ìš©í•  ë°ì´í„°ì…‹ì„ ì™¸ë¶€ì—ì„œ ë‹¤ìš´ë°›ì•„ numpy.ndarrayë¡œ ë³€í™˜í•˜ê³  ì „ì²˜ë¦¬í•˜ëŠ” ê³¼ì •ì„ ìŠµë“.
```


- 1-1. MNIST ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ë° ì••ì¶• í•´ì œ
- 1-2. MNIST ë°ì´í„°ì…‹ ì „ì²˜ë¦¬

### 1-1. MNIST ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ë° ì••ì¶• í•´ì œ



- ğŸ‘¨â€ğŸ’» <font color='green'><b>[ ì½”ë“œ ]</b></font> MNIST ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ë° ì••ì¶• í•´ì œ

#### ğŸ‘¨â€ğŸ’» <font color='green'><b>[ ì½”ë“œ ]</b></font> MNIST ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ë° ì••ì¶• í•´ì œ

Google Driveì—ì„œ Mountí•˜ì‹¤ ê²½ìš°, ì•„ë˜ì˜ ì½”ë“œë¥¼ ì°¸ê³ í•˜ì‹œê¸¸ ë°”ëë‹ˆë‹¤.
```python
from google.colab import drive
drive.mount('/content/drive')
```





```python
# MNISTë¥¼ ë‹¤ìš´ë°›ì„ ê²½ë¡œ
url = 'http://yann.lecun.com/exdb/mnist/'

# MNISTë¥¼ ì €ì¥í•  ë””ë ‰í† ë¦¬ (colab ì‚¬ìš© ì‹œ, ê¸°ë³¸ ë””ë ‰í† ë¦¬ëŠ” `/content`)
# ë§Œì¼ êµ¬ê¸€ ë“œë¼ì´ë¸Œë¥¼ ë§ˆìš´íŠ¸í•´ì„œ ì‚¬ìš©í•  ê²½ìš°, ì‚¬ìš©í•˜ê³ ì í•˜ëŠ” ë“œë¼ì´ë¸Œ ìœ„ì¹˜ë¥¼ `os.getcwd()` ë¶€ë¶„ì— ë®ì–´ì”Œì›Œ ì‘ì„±
dataset_dir = os.path.join(os.getcwd(), 'data')

# MNIST ë°ì´í„°ì…‹ì˜ íŒŒì¼ëª… (ë”•ì…”ë„ˆë¦¬)
key_file = {
    'train_img':'train-images-idx3-ubyte.gz',
    'train_label':'train-labels-idx1-ubyte.gz',
    'test_img':'t10k-images-idx3-ubyte.gz',
    'test_label':'t10k-labels-idx1-ubyte.gz'
}

# í•´ë‹¹ ê²½ë¡œê°€ ì—†ì„ ì‹œ ë””ë ‰í† ë¦¬ ìƒˆë¡œ ìƒì„±
os.makedirs(dataset_dir, exist_ok=True)

# í•´ë‹¹ ê²½ë¡œì— ì¡´ì¬í•˜ì§€ ì•ŠëŠ” íŒŒì¼ì„ ëª¨ë‘ ë‹¤ìš´ë¡œë“œ
for filename in key_file.values():
    if filename not in os.listdir(dataset_dir):
        urlretrieve(url + filename, os.path.join(dataset_dir, filename))
        print("Downloaded %s to %s" % (filename, dataset_dir))
```

    Downloaded train-images-idx3-ubyte.gz to /content/data
    Downloaded train-labels-idx1-ubyte.gz to /content/data
    Downloaded t10k-images-idx3-ubyte.gz to /content/data
    Downloaded t10k-labels-idx1-ubyte.gz to /content/data
    

### 1-2. MNIST ë°ì´í„°ì…‹ ì „ì²˜ë¦¬

#### ğŸ‘¨â€ğŸ’» <font color='green'><b>[ ì½”ë“œ ]</b></font> ì´ë¯¸ì§€ ë°ì´í„°ì˜ ì „ì²˜ë¦¬

- `(n,28,28)`ì´ ì•„ë‹Œ `(n,784)` í¬ê¸°ê°€ ë˜ë„ë¡ ë³€í™˜
- ê¸°ì¡´ì— 0ì—ì„œ 255 ì‚¬ì´ì˜ ì •ìˆ˜ë¡œ í‘œí˜„ë˜ë˜ í‘ë°± ì´ë¯¸ì§€ì˜ í”½ì…€ ê°’ì„ 0ê³¼ 1 ì‚¬ì´ì˜ ì‹¤ìˆ˜ê°’ì´ ë˜ë„ë¡ ì •ê·œí™”


```python
def _images(path):
    '''
    MNIST ë°ì´í„°ì…‹ ì´ë¯¸ì§€ì„ NumPy Arrayë¡œ ë³€í™˜í•˜ì—¬ ë¶ˆëŸ¬ì˜¤ê¸°
    '''
    # gzip íŒŒì¼ì„ ì—´ê³ , ì´ë¯¸ì§€ë¥¼ ì½ì–´ì„œ 1ì°¨ì› ë°°ì—´ë¡œ ë³€í™˜
    with gzip.open(path) as f:
        # ì²« 16 ë°”ì´íŠ¸ëŠ” magic_number, n_imgs, n_rows, n_cols ì˜ ì •ë³´ì´ë¯€ë¡œ ë¬´ì‹œ
        pixels = np.frombuffer(f.read(), 'B', offset=16)

    # 28*28=784 ì´ë¯€ë¡œ 784ì°¨ì›ìœ¼ë¡œ reshapeí•´ì¤€ ë’¤, 0~255ì˜ ê°’ì„ 0~1ë¡œ ì •ê·œí™”
    return pixels.reshape(-1, 28*28).astype('float32') / 255

```


```python
# _images í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ì—¬ ë‹¤ìš´ë°›ì€ í•™ìŠµ/í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ë¥¼ numpy í–‰ë ¬ë¡œ ë³€í™˜ ë° ì „ì²˜ë¦¬
X_train = _images(os.path.join(dataset_dir, key_file['train_img']))
X_test = _images(os.path.join(dataset_dir, key_file['test_img']))
```

#### ğŸ‘¨â€ğŸ’» <font color='green'><b>[ ì½”ë“œ ]</b></font> ë¼ë²¨ ë°ì´í„°ì˜ ì „ì²˜ë¦¬
- one-hot encoding ìˆ˜í–‰
  - ì˜ˆì‹œ)
  <br>`2` -> `[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]`
  <br>`7` -> `[0, 0, 0, 0, 0, 0, 0, 1, 0, 0]`


```python
def _onehot(integer_labels):
    '''
    ë¼ë²¨ ë°ì´í„°ë¥¼ one-hot encoding í•˜ê¸°
    '''
    n_rows = len(integer_labels)        # ë¼ë²¨ ë°ì´í„°ì˜ ê¸¸ì´
    n_cols = integer_labels.max() + 1   # ë¼ë²¨ ë°ì´í„°ì˜ ìµœëŒ“ê°’ + 1

    # 0ìœ¼ë¡œ ì±„ì›Œì§„ (n_rows, n_cols) í¬ê¸°ì˜ í–‰ë ¬ ìƒì„±
    onehot = np.zeros((n_rows, n_cols), dtype='uint8')
    # one-hot í–‰ë ¬ì˜ ê° í–‰ì— í•´ë‹¹í•˜ëŠ” ë¼ë²¨ì„ 1ë¡œ ë³€ê²½
    onehot[np.arange(n_rows), integer_labels] = 1

    return onehot

def _labels(path):
    '''
    MNIST ë°ì´í„°ì…‹ ë¼ë²¨ì„ NumPy Arrayë¡œ ë³€í™˜í•˜ì—¬ ë¶ˆëŸ¬ì˜¤ê¸°
    '''
    # gzip íŒŒì¼ì„ ì—´ê³ , ë¼ë²¨ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¨ ë’¤, integerë¡œ ë³€í™˜
    with gzip.open(path) as f:
        # ì²« 8 ë°”ì´íŠ¸ëŠ” magic_number, n_labels ì˜ ì •ë³´ì´ë¯€ë¡œ ë¬´ì‹œ
        integer_labels = np.frombuffer(f.read(), 'B', offset=8)

    # one-hot ì¸ì½”ë”©í•œ ê²°ê³¼ë¥¼ ë°˜í™˜
    return _onehot(integer_labels)
```


```python
# _labels í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ì—¬ ë‹¤ìš´ë°›ì€ í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¼ë²¨ì„ numpy í–‰ë ¬ë¡œ ë³€í™˜ ë° ì „ì²˜ë¦¬
y_train = _labels(os.path.join(dataset_dir, key_file['train_label']))
y_test = _labels(os.path.join(dataset_dir, key_file['test_label']))
```

#### ğŸ‘¨â€ğŸ’» <font color='green'><b>[ ì½”ë“œ ]</b></font> ìµœì¢… ë°ì´í„°ì…‹ í™•ì¸



```python
# ìµœì¢… ë°ì´í„°ì…‹ì˜ í˜•íƒœ í™•ì¸
print('X_trainì˜ í˜•íƒœ : {}'.format(X_train.shape))
print('y_trainì˜ í˜•íƒœ : {}'.format(y_train.shape))
print('X_testì˜ í˜•íƒœ  : {}'.format(X_test.shape))
print('y_testì˜ í˜•íƒœ  : {}'.format(y_test.shape))
```

    X_trainì˜ í˜•íƒœ : (60000, 784)
    y_trainì˜ í˜•íƒœ : (60000, 10)
    X_testì˜ í˜•íƒœ  : (10000, 784)
    y_testì˜ í˜•íƒœ  : (10000, 10)
    


```python
# í•™ìŠµ ë°ì´í„° ë‚´ì˜ ì„ì˜ì˜ 16ê°œ ìƒ˜í”Œ ì‚´í´ë³´ê¸°

# 7x7 ì‚¬ì´ì¦ˆì˜ ìƒˆë¡œìš´ figure ìƒì„±
plt.figure(figsize=(7,7))
# 16ê°œì˜ ëœë¤í•œ ì •ìˆ˜ ìƒì„±
random_indices = np.random.randint(0, len(X_train), size=16)

# 16ê°œì˜ ëœë¤í•œ ì •ìˆ˜ì— í•´ë‹¹í•˜ëŠ” ì´ë¯¸ì§€ë¥¼ 4x4 ê·¸ë¦¬ë“œì— í•˜ë‚˜ì”© ì¶œë ¥
for n, idx in enumerate(random_indices, start=1):
    # 4x4 ê·¸ë¦¬ë“œì˜ në²ˆì§¸ ìœ„ì¹˜ë¥¼ ì§€ì •
    # í•´ë‹¹ ìœ„ì¹˜ì— ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ì´ë¯¸ì§€ë¥¼ ì¶œë ¥í•˜ê³ , íƒ€ì´í‹€ë¡œëŠ” ë¼ë²¨ ê°’ì„ ì¶œë ¥
    # xì¶•ê³¼ yì¶•ì´ í•„ìš”í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ëˆˆê¸ˆ ì¶œë ¥ ìƒëµ
    plt.subplot(4,4,n)
    plt.imshow(X_train[idx].reshape(28,28), cmap='gray')
    plt.title(f"Label: {y_train[idx].argmax()}")
    plt.axis('off')

plt.suptitle('MNIST Dataset')
plt.tight_layout()
plt.show()
```


    
![png](\..\img\DL_practice_26_0.png)
    


---

## 2. ê°„ë‹¨í•œ MLP ëª¨ë¸ êµ¬ì„±í•˜ê¸°

<br>

![Alt text](\..\img\DL_practice_model.png)


- 2-1. í™œì„±í™” í•¨ìˆ˜ì˜ êµ¬í˜„ ... `Sigmoid`
- 2-2. MLP ë ˆì´ì–´ì˜ êµ¬í˜„ ... `FCLayer`
- 2-3. ì†ì‹¤ í•¨ìˆ˜ì˜ êµ¬í˜„ ... `MSELoss`, `CrossEntropyLoss`
- 2-4. MLP ëª¨ë¸ì˜ êµ¬í˜„ ... `MLP`

### 2-1. í™œì„±í™” í•¨ìˆ˜ì˜ êµ¬í˜„


#### ğŸ“ <font color='orange'><b>[ ì„¤ëª… ]</b></font> ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ ë° ë„í•¨ìˆ˜ì˜ ìˆ˜ì‹

$$ f = \frac{1}{1 + e^{-x}}$$

$$ \frac{\partial f}{\partial x} = \left(1.0 - f(x) \right) f(x) $$


#### ğŸ‘¨â€ğŸ’» <font color='green'><b>[ ì½”ë“œ ]</b></font> ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ì˜ êµ¬í˜„



```python
def sigmoid(x):
    '''ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜'''
    return 1 / (1 + np.exp(-x))
```


```python
def sigmoid_prime(x):
    '''ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ì˜ ê·¸ë˜ë””ì–¸íŠ¸'''
    return (1.0 - sigmoid(x)) * sigmoid(x)
```

#### ğŸ‘¨â€ğŸ’» <font color='green'><b>[ ì½”ë“œ ]</b></font> ì‹œê·¸ëª¨ì´ë“œ ë ˆì´ì–´ì˜ êµ¬í˜„



```python
class Sigmoid():
    '''ì‹œê·¸ëª¨ì´ë“œ ë ˆì´ì–´ (ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ì˜ í´ë˜ìŠ¤ ë²„ì „)'''
    def __init__(self):
        self.out = None

    def forward(self, x):
        '''ì‹œê·¸ëª¨ì´ë“œ ë ˆì´ì–´ì˜ ìˆœì „íŒŒ(forward propagation)'''
        # ìˆœì „íŒŒê°€ íë¥¼ ì‹œ ê·¸ ê²°ê³¼ë¬¼ì„ attributeë¡œ ì €ì¥ ë° ë¦¬í„´
        self.out = sigmoid(x)
        return self.out

    def backward(self, dout):
        '''ì‹œê·¸ëª¨ì´ë“œ ë ˆì´ì–´ì˜ ì—­ì „íŒŒ(backpropagation)'''
        # ì—­ì „íŒŒê°€ íë¥¼ì‹œ ê·¸ ê·¸ë˜ë””ì–¸íŠ¸ ê°’ì„ ë¦¬í„´
        dx = dout * (1.0 - self.out) * self.out
        return dx
```

#### ğŸ‘¨â€ğŸ’» <font color='green'><b>[ ì½”ë“œ ]</b></font> ì‹œê·¸ëª¨ì´ë“œ ë ˆì´ì–´ í…ŒìŠ¤íŠ¸í•˜ê¸°



```python
# ì‹œê·¸ëª¨ì´ë“œ ë ˆì´ì–´ì˜ ìˆœì „íŒŒ/ì—­ì „íŒŒê°€ ì˜ ì‘ë™í•˜ëŠ”ì§€ í…ŒìŠ¤íŠ¸
# ì„ì˜ì˜ ì‚¬ì´ì¦ˆì— ëŒ€í•´ ëª¨ë‘ ë™ì¼í•œ í¬ê¸°ì˜ ê²°ê³¼ê°€ ì¶œë ¥ë˜ì–´ì•¼ í•˜ë©°, ê°ê° ë‹¨ìˆœí•˜ê²Œ ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ë¥¼ ì ìš©í•œ ê°’ê³¼ ë„í•¨ìˆ˜ë¥¼ ì ìš©í•œ ê°’ì´ ëœë‹¤ë©´ ì„±ê³µ

x = np.random.randn(5, 2)
print('x =\n', x)

sigmoid_layer = Sigmoid()
print('\nsigmoid_layer.forward(x) =\n', sigmoid_layer.forward(x))
print('\nsigmoid_layer.backward(1) =\n', sigmoid_layer.backward(1)) # ìµœì¢… ì¶œë ¥ë¬¼ì˜ ë¯¸ë¶„ê°’ì´ 1ì¼ ê²½ìš° ì—­ì „íŒŒì˜ ê²°ê³¼
```

    x =
     [[-0.10321885  0.4105985 ]
     [ 0.14404357  1.45427351]
     [ 0.76103773  0.12167502]
     [ 0.44386323  0.33367433]
     [ 1.49407907 -0.20515826]]
    
    sigmoid_layer.forward(x) =
     [[0.47421817 0.60123138]
     [0.53594876 0.81065526]
     [0.68157899 0.53038128]
     [0.60917918 0.58265313]
     [0.81668973 0.44888958]]
    
    sigmoid_layer.backward(1) =
     [[0.2493353  0.23975221]
     [0.24870769 0.15349331]
     [0.21702907 0.24907698]
     [0.23807991 0.24316846]
     [0.14970761 0.24738772]]
    

### 2-2. MLP ë ˆì´ì–´ì˜ êµ¬í˜„


#### ğŸ‘¨â€ğŸ’» <font color='green'><b>[ ì½”ë“œ ]</b></font> `FCLayer` í´ë˜ìŠ¤ êµ¬í˜„í•˜ê¸°



```python
class FCLayer():
    '''ì™„ì „ ì—°ê²° ë ˆì´ì–´ (Fully-Connected Layer, FC Layer)'''
    def __init__(self, input_size, output_size, weight_init_std=0.01):
        '''ê°€ì¤‘ì¹˜ì™€ í¸í–¥ ì´ˆê¸°í™”
        input_size: ì…ë ¥ ë°ì´í„°ì˜ ì°¨ì› ìˆ˜
        output_size: ì¶œë ¥ ë°ì´í„°ì˜ ì°¨ì› ìˆ˜
        weight_init_std: ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” ì‹œ ì‚¬ìš©í•˜ëŠ” í‘œì¤€ í¸ì°¨'''
        # ê°€ì¤‘ì¹˜ëŠ” ì •ê·œë¶„í¬ë¥¼ ë”°ë¥´ëŠ” ë‚œìˆ˜ë¡œ ì´ˆê¸°í™”
        self.W = np.random.randn(input_size, output_size)
        # í¸í–¥ë„ ì •ê·œë¶„í¬ë¥¼ ë”°ë¥´ëŠ” ë‚œìˆ˜ë¡œ ì´ˆê¸°í™”
        self.b = np.random.randn(output_size)

        # ì—­ì „íŒŒ ê³„ì‚°ì„ ìœ„í•´ ì…ë ¥ ë°ì´í„° ì €ì¥
        self.x = None
        # ëª¨ë¸ í•™ìŠµì„ ìœ„í•´ ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì˜ ê·¸ë˜ë””ì–¸íŠ¸ ì €ì¥
        self.dW = None
        self.db = None


    def forward(self, x):
        '''ì™„ì „ ì—°ê²° ë ˆì´ì–´ì˜ ìˆœë°©í–¥ ì „íŒŒ(forward propagation)'''
        # ì—­ì „íŒŒ ê³„ì‚°ì„ ìœ„í•´ ì…ë ¥ ë°ì´í„° ì €ì¥
        self.x = x
        # y = Wx + b
        out = np.dot(x, self.W) + self.b

        return out

    def backward(self, dout):
        '''ì™„ì „ ì—°ê²° ë ˆì´ì–´ì˜ ì—­ë°©í–¥ ì „íŒŒ(backward propagation; ì—­ì „íŒŒ)'''
        # dy/dx = W  ->  dx = dy * W
        dx = np.dot(dout, self.W.T)
        # dy/dW = x  ->  dW = x * dy
        self.dW = np.dot(self.x.T, dout)
        # dy/db = 1  ->  db = dy * 1
        self.db = np.sum(dout, axis=0)

        return dx
```

#### ğŸ‘¨â€ğŸ’» <font color='green'><b>[ ì½”ë“œ ]</b></font> `FCLayer` í´ë˜ìŠ¤ í…ŒìŠ¤íŠ¸í•˜ê¸°



```python
# FCLayer ë ˆì´ì–´ê°€ ì„ì˜ì˜ ì…ì¶œë ¥ ë°ì´í„°ì— ëŒ€í•´ ì˜ ì‘ë™í•˜ëŠ”ì§€ í…ŒìŠ¤íŠ¸

x = np.random.randn(5, 2)
y = np.random.randn(5, 3)
print('x =\n', x)

fc_layer = FCLayer(2, 3)
print('\nfc_layer.forward(x) =\n', fc_layer.forward(x))
print('\nfc_layer.backward(y) =\n', fc_layer.backward(y))
```

    x =
     [[ 0.3130677  -0.85409574]
     [-2.55298982  0.6536186 ]
     [ 0.8644362  -0.74216502]
     [ 2.26975462 -1.45436567]
     [ 0.04575852 -0.18718385]]
    
    fc_layer.forward(x) =
     [[ 1.46799167 -1.7190734   1.62817886]
     [-6.01190034  0.91385242  0.45042808]
     [ 2.40336148 -1.9130545   1.20599374]
     [ 6.03706376 -3.18300724  1.73977896]
     [ 0.11102737 -1.06432113  0.66895248]]
    
    fc_layer.backward(y) =
     [[ 2.17336759 -1.02791573]
     [ 2.05790811  2.03279805]
     [-1.29734078 -1.42814094]
     [ 2.67540605 -1.319593  ]
     [-0.57430296  2.96331783]]
    

### 2-3. ì†ì‹¤ í•¨ìˆ˜ì˜ êµ¬í˜„



#### ğŸ“ <font color='orange'><b>[ ì„¤ëª… ]</b></font> MSE ì†ì‹¤ í•¨ìˆ˜ ë° ë„í•¨ìˆ˜ì˜ ìˆ˜ì‹
$$
L_{MSE}(y,\hat{y}) = \frac{1}{n}\sum_{i=1}^n \left(y_i - \hat{y}_i \right)^2
$$

$$
\frac{dL}{d\hat y} = \frac{2}{n}\sum_{i=1}^n \left(\hat{y}_i - y\right)
$$

#### ğŸ‘¨â€ğŸ’» <font color='green'><b>[ ì½”ë“œ ]</b></font> MSE ì†ì‹¤ í•¨ìˆ˜ êµ¬í˜„í•˜ê¸°



```python
class MSELoss():
    '''Mean Squared Error Loss'''
    def __init__(self):
        self.loss = None    # ì†ì‹¤ í•¨ìˆ˜ ê°’. ì—­ì „íŒŒ ê³„ì‚°ì„ ìœ„í•´ ì¸ìŠ¤í„´ìŠ¤ ë‚´ì— ì €ì¥ë¨
        self.y = None       # ì¶”ë¡ ê°’ (y)
        self.t = None       # ì‹¤ì œê°’ (target)

    def forward(self, y, t):
        '''Forward Propagation of Mean Squared Error Loss'''
        # L = 1/N * sum((y - t)^2)
        self.y = y
        self.t = t
        self.loss = np.mean((t - y) ** 2)
        return self.loss

    def backward(self, dout=1):
        '''Backward Propagation (Backpropagation) of Mean Squared Error Loss'''
        # dL/dy = 2(y - t) / N  ->  dy = 2(y - t) / N
        dx = dout * (self.y - self.t) * 2 / self.t.shape[0]
        return dx
```

#### ğŸ“ <font color='orange'><b>[ ì„¤ëª… ]</b></font> êµì°¨ì—”íŠ¸ë¡œí”¼(CE) ì†ì‹¤ í•¨ìˆ˜ ë° ë„í•¨ìˆ˜ì˜ ìˆ˜ì‹
$$
L(y,\hat{y}) = \frac{-1}{n}\sum_{i=1}^n y_i\log(\hat{y}_i)+(1-y_i)(\log(1-\hat{y}_i)
$$

$$
\frac{dL}{d\hat y} = \frac{-1}{n}\left(\frac{y_i}{\hat{y}_i} - \frac{1-y_i}{1 - \hat{y}_i}\right) = \frac{\hat{y}_i-y_i}{n(1-\hat{y}_i)\hat{y}_i}
$$

#### ğŸ‘¨â€ğŸ’» <font color='green'><b>[ ì½”ë“œ ]</b></font> êµì°¨ì—”íŠ¸ë¡œí”¼(CE) ì†ì‹¤ í•¨ìˆ˜ êµ¬í˜„í•˜ê¸°



```python
class CrossEntropyLoss():
    '''êµì°¨ ì—”íŠ¸ë¡œí”¼(Cross Entropy) ë ˆì´ì–´'''
    def __init__(self):
        self.loss = None
        self.y = None
        self.t = None

    def forward(self, y, t):
        '''êµì°¨ ì—”íŠ¸ë¡œí”¼ ë ˆì´ì–´ì˜ ìˆœë°©í–¥ ì „íŒŒ'''
        # L = -1/N * sum(t * log(y) + (1 - t) * log(1 - y))
        self.y = y
        self.t = t
        self.loss = - np.sum(t * np.log(y + 1e-7) + (1 - t) * np.log(1 - y + 1e-7)) / len(y)
        return self.loss

    def backward(self, dout=1):
        '''êµì°¨ ì—”íŠ¸ë¡œí”¼ ë ˆì´ì–´ì˜ ì—­ë°©í–¥ ì „íŒŒ'''
        # dL/dy = -t/y + (1 - t)/(1 - y)  ->  dy = (-t/y + (1 - t)/(1 - y))
        dx =  dout * (self.y - self.t) / (self.t.shape[0] * (1-self.y) * self.y + 1e-7)
        return dx
```

### 2-4. MLP ëª¨ë¸ì˜ êµ¬í˜„



#### ğŸ“ <font color='orange'><b>[ ì„¤ëª… ]</b></font> MLP ëª¨ë¸ì˜ êµ¬ì„±

ìœ„ì—ì„œ êµ¬í˜„í•œ í´ë˜ìŠ¤ë“¤ì„ í™œìš©í•˜ì—¬ ìµœì¢…ì ìœ¼ë¡œ MLP ëª¨ë¸ í´ë˜ìŠ¤ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.

ëª¨ë¸ ë‚´ì—ì„œ ì‚¬ìš©í•  ì¸ìëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.
- `input_size` : ì…ë ¥ ë°ì´í„°ì˜ ì°¨ì› ìˆ˜
- `hidden_size_list` : ì€ë‹‰ì¸µ ì°¨ì› ìˆ˜ì˜ ë¦¬ìŠ¤íŠ¸ (e.g. `[100, 100, 100]`)
- `output_size` : ì¶œë ¥ ë°ì´í„°ì˜ ì°¨ì› ìˆ˜
- `loss_type` : ì†ì‹¤ í•¨ìˆ˜ì˜ ì¢…ë¥˜ (`'MSE'` ë˜ëŠ” `'CrossEntropy'`)


![Alt text](\..\img\DL_practice_model.png)

#### ğŸ‘¨â€ğŸ’» <font color='green'><b>[ ì½”ë“œ ]</b></font> MLP ëª¨ë¸ì˜ êµ¬í˜„



```python
class MLP():
    '''ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡ (Multi Layer Perceptron, MLP) ëª¨ë¸'''
    def __init__(self, input_size, hidden_size_list, output_size, loss_type='MSE'):
        '''ì‹ ê²½ë§ì˜ êµ¬ì¡°ì™€ ì†ì‹¤ í•¨ìˆ˜ ì •ì˜
        input_size: ì…ë ¥ ë°ì´í„°ì˜ ì°¨ì› ìˆ˜
        hidden_size_list: ì€ë‹‰ì¸µ ì°¨ì› ìˆ˜ì˜ ë¦¬ìŠ¤íŠ¸ (e.g. [100, 100, 100])
        output_size: ì¶œë ¥ ë°ì´í„°ì˜ ì°¨ì› ìˆ˜
        loss_type: ì†ì‹¤ í•¨ìˆ˜ì˜ ì¢…ë¥˜ ('MSE' or 'CrossEntropy')
        '''

        self.input_size = input_size
        self.hidden_size_list = hidden_size_list
        self.output_size = output_size
        self.hidden_layer_num = len(hidden_size_list)
        self.loss_type = loss_type

        ### ë ˆì´ì–´ ìŒ“ê¸° : FCLayer -> Sigmoid -> ... -> FCLayer -> Sigmoid
        # ì…ë ¥ì¸µ
        self.layers = [
            FCLayer(input_size, hidden_size_list[0]),
            Sigmoid()
        ]
        # ì€ë‹‰ì¸µ
        for idx in range(1, self.hidden_layer_num):
            self.layers.append(FCLayer(hidden_size_list[idx-1], hidden_size_list[idx]))
            self.layers.append(Sigmoid())
        # ì¶œë ¥ì¸µ
        self.layers.append(FCLayer(hidden_size_list[-1], output_size))
        self.layers.append(Sigmoid())

        # ì¸ìë¡œ ë°›ì€ `loss_type`ì— ë§ê²Œ ì†ì‹¤ í•¨ìˆ˜ ë ˆì´ì–´ ì •í•˜ê¸°
        if self.loss_type == 'MSE':
            self.loss_layer = MSELoss()
        elif self.loss_type == 'CrossEntropy':
            self.loss_layer = CrossEntropyLoss()
        else:
            self.loss_layer = None


        self.loss = None

    def predict(self, x):
        '''ì…ë ¥ê°’ì„ ë°›ì•˜ì„ ë•Œ ìˆœë°©í–¥ ì „íŒŒë¥¼ í†µí•œ ì¶œë ¥ë¬¼ ì‚°ì¶œ(ì˜ˆì¸¡)'''
        for layer in self.layers:
            x = layer.forward(x)
        return x

    def forward(self, x, t):
        '''MLP ëª¨ë¸ì˜ ìˆœë°©í–¥ ì „íŒŒ'''
        y = self.predict(x)
        self.loss = self.loss_layer.forward(y, t)
        return self.loss

    def backward(self, dout=1):
        '''MLP ëª¨ë¸ì˜ ì—­ë°©í–¥ ì „íŒŒ'''
        dout = self.loss_layer.backward(dout)
        for layer in reversed(self.layers):
            dout = layer.backward(dout)
        return dout
```

#### ğŸ‘¨â€ğŸ’» <font color='green'><b>[ ì½”ë“œ ]</b></font> MLP ëª¨ë¸ í…ŒìŠ¤íŠ¸í•˜ê¸°



```python
# ì„ì˜ì˜ ì…ì¶œë ¥ ë°ì´í„°ì™€ í•˜ì´í¼íŒŒë¼ë¯¸í„°ì— ëŒ€í•´ MLP ëª¨ë¸ì´ ì—ëŸ¬ ì—†ì´ ì‘ë™í•˜ëŠ”ì§€ í…ŒìŠ¤íŠ¸

x = np.random.randn(5, 2)
y = np.random.randn(5, 1)
print('x =\n', x)

mlp = MLP(input_size=2, hidden_size_list=[10, 5], output_size=1, loss_type='MSE')
print('\nmlp.predict(x) =\n', mlp.predict(x))
print('\nmlp.forward(x, y) =\n', mlp.forward(x, y))
print('\nmlp.backward() =\n', mlp.backward())
```

    x =
     [[-0.51080514 -1.18063218]
     [-0.02818223  0.42833187]
     [ 0.06651722  0.3024719 ]
     [-0.63432209 -0.36274117]
     [-0.67246045 -0.35955316]]
    
    mlp.predict(x) =
     [[0.09179987]
     [0.0957102 ]
     [0.09523428]
     [0.09244939]
     [0.09246448]]
    
    mlp.forward(x, y) =
     1.4714343261962757
    
    mlp.backward() =
     [[-3.38789968e-04 -3.67465930e-04]
     [ 1.17086873e-03  3.64237844e-03]
     [-5.29701613e-05 -1.65101162e-04]
     [-3.87903857e-05  4.80263293e-04]
     [-1.30842487e-04  1.66334466e-03]]
    

---

## 3. ëª¨ë¸ í•™ìŠµí•˜ê¸°


### 3-1. ëª¨ë¸ í•™ìŠµ ì „ ì¤€ë¹„


#### ğŸ‘¨â€ğŸ’» <font color='green'><b>[ ì½”ë“œ ]</b></font> ì •í™•ë„ í•¨ìˆ˜ ì •ì˜í•˜ê¸°


```python
# one-hot ì¸ì½”ë”©ì´ ë˜ì–´ ìˆëŠ” ê°’ì„ ë‹¤ì‹œ ë¼ë²¨ë¡œ ë°”ê¾¸ì–´ ë¹„êµ
def accuracy(y, t):
    '''ì •í™•ë„ í•¨ìˆ˜'''
    y = np.argmax(y, axis=1)
    t = np.argmax(t, axis=1)
    return np.sum(y == t) / float(len(t))
```

#### ğŸ‘¨â€ğŸ’» <font color='green'><b>[ ì½”ë“œ ]</b></font> í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •í•˜ê¸°

- í•™ìŠµë¥ (`learning_rate`) ì„¤ì •
    - MSE ì†ì‹¤ í•¨ìˆ˜ëŠ” vanishing gradientë¬¸ì œê°€ ìƒê¸¸ ìˆ˜ ìˆìœ¼ë¯€ë¡œ **CEë³´ë‹¤ ë” í° ê°’ì˜ í•™ìŠµë¥ ì„ ì„¤ì •**í•´ì£¼ì—ˆë‹¤.
    - ì¦‰ ì´ì „ ê°•ì˜ì—ì„œ ì‚´í´ë³¸ ë°”ì™€ ê°™ì´, ë™ì¼í•œ í•™ìŠµë¥ ì„ ì‚¬ìš©í•œë‹¤ë©´ CEì˜ í•™ìŠµì´ MSEë³´ë‹¤ í›¨ì”¬ ë¹ ë¥¼ ê²ƒì´ë‹¤.
    - MNIST ë°ì´í„°ì…‹ì˜ ê²½ìš° í•™ìŠµì´ ì‰¬ìš´ ë°ì´í„°ì…‹ì´ê¸°ì— ë³¸ ì‹¤ìŠµì—ì„œëŠ” ë¹ ë¥¸ í•™ìŠµì„ ìœ„í•´ í° ê°’ìœ¼ë¡œ ì •í•˜ì˜€ìœ¼ë‚˜, <br>**ì¼ë°˜ì ìœ¼ë¡œ í•™ìŠµë¥ ì€ ì†Œìˆ˜ì  ë‹¨ìœ„(0.01 ~ 0.000001)ë¡œ ì„¤ì •ë˜ëŠ” ë§¤ìš° ì‘ì€ ê°’**ì´ë‹¤.


```python
# ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°ë¥¼ ì •í•˜ê¸° ìœ„í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° (784 -> 64 -> 16 -> 10)
n_input = 784
n_hidden = (64,16,)
n_output = 10

# í•™ìŠµì— í•„ìš”í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°
batch_size = 128
n_epochs = 30
print_every = 1
learning_rate_mse = 3
learning_rate_ce = 0.5
```

#### ğŸ‘¨â€ğŸ’» <font color='green'><b>[ ì½”ë“œ ]</b></font> ëª¨ë¸ ìƒì„±í•˜ê¸°
- MSEì™€ CEì˜ ê²°ê³¼ë¥¼ ë¹„êµí•˜ê¸° ìœ„í•´ ë‘ ê°œì˜ ë‹¤ë¥¸ ëª¨ë¸ì„ ìƒì„±


```python
# MSE ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ëŠ” MLP ëª¨ë¸
mlp_mse = MLP(n_input, n_hidden, n_output, loss_type='MSE')

# Cross Entropy ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ëŠ” MLP ëª¨ë¸
mlp_ce = MLP(n_input, n_hidden, n_output, loss_type='CrossEntropy')
```

### 3-2. ëª¨ë¸ í•™ìŠµí•˜ê¸°

> 3-2ì—ì„œ ë§Œë“  ëª¨ë¸ì„ SGD ì•Œê³ ë¦¬ì¦˜ì„ í†µí•´ í•™ìŠµí•˜ëŠ” ê³¼ì •ì„ êµ¬í˜„í•©ë‹ˆë‹¤.

- ğŸ‘¨â€ğŸ’» <font color='green'><b>[ ì½”ë“œ ]</b></font> ëª¨ë¸ í•™ìŠµí•˜ê¸°

> <font color='#8b71ff'>(ì°¸ê³ ) Chapter 3 ì‹¤ìŠµì—ì„œëŠ” ìµœì í™” ì•Œê³ ë¦¬ì¦˜ì— í•´ë‹¹í•˜ëŠ” ì˜µí‹°ë§ˆì´ì €(Optimizer)ì™€ <br>&ensp;&ensp;&ensp;&ensp;&ensp;&nbsp;í•™ìŠµ ê³¼ì •ì„ ë‹´ì€ íŠ¸ë ˆì´ë„ˆ(Trainer)ê°€ ëª¨ë‘ í´ë˜ìŠ¤ í˜•íƒœë¡œ êµ¬í˜„ë˜ì–´ ìˆìŠµë‹ˆë‹¤.</font>



#### ğŸ‘¨â€ğŸ’» <font color='green'><b>[ ì½”ë“œ ]</b></font> ëª¨ë¸ í•™ìŠµí•˜ê¸°


```python
def training(model, learning_rate, X_train, y_train, X_test, y_test):
    '''ëª¨ë¸ í•™ìŠµ í•¨ìˆ˜'''
    training_errors, training_accs = [], [] # í•™ìŠµ ì†ì‹¤ ë° ì •í™•ë„
    test_errors, test_accs = [], []         # í…ŒìŠ¤íŠ¸ ì†ì‹¤ ë° ì •í™•ë„

    # í•™ìŠµ ê³¼ì • : n_epochsë§Œí¼ ë°˜ë³µ
    for epoch in tqdm(range(n_epochs)):
        # í•™ìŠµ ë°ì´í„°ë¥¼ ëœë¤í•˜ê²Œ ì„ìŒ
        idx = np.random.permutation(X_train.shape[0])
        X_train = X_train[idx]
        y_train = y_train[idx]

        # ë¯¸ë‹ˆë°°ì¹˜ í•™ìŠµ
        for i in range(0, X_train.shape[0], batch_size):
            X_batch = X_train[i:i+batch_size]
            y_batch = y_train[i:i+batch_size]

            # ìˆœì „íŒŒ
            model.forward(X_batch, y_batch)

            # ì—­ì „íŒŒ
            model.backward()

            # ëª¨ë¸ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸
            for layer in model.layers:
                if isinstance(layer, FCLayer):
                    layer.W -= learning_rate * layer.dW
                    layer.b -= learning_rate * layer.db

        # í•™ìŠµ ê³¼ì • ì¶œë ¥ : print_everyì˜ ë°°ìˆ˜ì— í•´ë‹¹í•  ë•Œë§ˆë‹¤ ì¶œë ¥
        if (epoch+1) % print_every == 0:
            # ëª¨ë“  í•™ìŠµ ë°ì´í„°ì— ëŒ€í•œ ì†ì‹¤ê³¼ ì •í™•ë„ ê³„ì‚°
            model.forward(X_train, y_train)
            training_errors.append(model.loss)
            training_accs.append(accuracy(model.predict(X_train), y_train))

            # ëª¨ë“  í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•œ ì†ì‹¤ê³¼ ì •í™•ë„ ê³„ì‚°
            model.forward(X_test, y_test)
            test_errors.append(model.loss)
            test_accs.append(accuracy(model.predict(X_test), y_test))

            # í•™ìŠµ ê³¼ì • ì¶œë ¥
            print('[Epoch {}/{}] Training Loss = {:.4f} / Training Acc = {:.2f}% /'
                  'Test Loss = {:.4f} / Test Acc = {:.2f}%'.format(
                      epoch+1, n_epochs,
                      training_errors[-1], training_accs[-1]*100,
                      test_errors[-1], test_accs[-1]*100))

    return np.asarray([training_errors, test_errors, training_accs, test_accs])
```


```python
# MSE ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ëŠ” ëª¨ë¸ í•™ìŠµ
training_result_mse = training(mlp_mse, learning_rate_mse, X_train, y_train, X_test, y_test)
```


      0%|          | 0/30 [00:00<?, ?it/s]


    [Epoch 1/30] Training Loss = 0.0264 / Training Acc = 83.52% /Test Loss = 0.0255 / Test Acc = 84.25%
    [Epoch 2/30] Training Loss = 0.0183 / Training Acc = 88.44% /Test Loss = 0.0179 / Test Acc = 88.67%
    [Epoch 3/30] Training Loss = 0.0155 / Training Acc = 90.11% /Test Loss = 0.0154 / Test Acc = 90.36%
    [Epoch 4/30] Training Loss = 0.0131 / Training Acc = 91.72% /Test Loss = 0.0135 / Test Acc = 91.28%
    [Epoch 5/30] Training Loss = 0.0118 / Training Acc = 92.64% /Test Loss = 0.0126 / Test Acc = 91.95%
    [Epoch 6/30] Training Loss = 0.0109 / Training Acc = 93.25% /Test Loss = 0.0118 / Test Acc = 92.42%
    [Epoch 7/30] Training Loss = 0.0100 / Training Acc = 93.82% /Test Loss = 0.0114 / Test Acc = 92.60%
    [Epoch 8/30] Training Loss = 0.0092 / Training Acc = 94.32% /Test Loss = 0.0107 / Test Acc = 93.07%
    [Epoch 9/30] Training Loss = 0.0091 / Training Acc = 94.48% /Test Loss = 0.0108 / Test Acc = 93.04%
    [Epoch 10/30] Training Loss = 0.0081 / Training Acc = 95.08% /Test Loss = 0.0100 / Test Acc = 93.61%
    [Epoch 11/30] Training Loss = 0.0077 / Training Acc = 95.30% /Test Loss = 0.0097 / Test Acc = 93.89%
    [Epoch 12/30] Training Loss = 0.0073 / Training Acc = 95.56% /Test Loss = 0.0094 / Test Acc = 94.22%
    [Epoch 13/30] Training Loss = 0.0074 / Training Acc = 95.52% /Test Loss = 0.0096 / Test Acc = 93.96%
    [Epoch 14/30] Training Loss = 0.0069 / Training Acc = 95.88% /Test Loss = 0.0092 / Test Acc = 94.10%
    [Epoch 15/30] Training Loss = 0.0065 / Training Acc = 96.05% /Test Loss = 0.0090 / Test Acc = 94.21%
    [Epoch 16/30] Training Loss = 0.0062 / Training Acc = 96.30% /Test Loss = 0.0090 / Test Acc = 94.38%
    [Epoch 17/30] Training Loss = 0.0061 / Training Acc = 96.39% /Test Loss = 0.0088 / Test Acc = 94.41%
    [Epoch 18/30] Training Loss = 0.0061 / Training Acc = 96.38% /Test Loss = 0.0091 / Test Acc = 94.26%
    [Epoch 19/30] Training Loss = 0.0057 / Training Acc = 96.61% /Test Loss = 0.0088 / Test Acc = 94.48%
    [Epoch 20/30] Training Loss = 0.0058 / Training Acc = 96.53% /Test Loss = 0.0090 / Test Acc = 94.26%
    [Epoch 21/30] Training Loss = 0.0054 / Training Acc = 96.85% /Test Loss = 0.0086 / Test Acc = 94.55%
    [Epoch 22/30] Training Loss = 0.0053 / Training Acc = 96.91% /Test Loss = 0.0087 / Test Acc = 94.48%
    [Epoch 23/30] Training Loss = 0.0051 / Training Acc = 96.94% /Test Loss = 0.0086 / Test Acc = 94.52%
    [Epoch 24/30] Training Loss = 0.0049 / Training Acc = 97.12% /Test Loss = 0.0084 / Test Acc = 94.65%
    [Epoch 25/30] Training Loss = 0.0048 / Training Acc = 97.18% /Test Loss = 0.0083 / Test Acc = 94.72%
    [Epoch 26/30] Training Loss = 0.0047 / Training Acc = 97.22% /Test Loss = 0.0084 / Test Acc = 94.73%
    [Epoch 27/30] Training Loss = 0.0045 / Training Acc = 97.35% /Test Loss = 0.0083 / Test Acc = 94.68%
    [Epoch 28/30] Training Loss = 0.0045 / Training Acc = 97.35% /Test Loss = 0.0081 / Test Acc = 94.88%
    [Epoch 29/30] Training Loss = 0.0043 / Training Acc = 97.50% /Test Loss = 0.0082 / Test Acc = 94.74%
    [Epoch 30/30] Training Loss = 0.0043 / Training Acc = 97.47% /Test Loss = 0.0082 / Test Acc = 94.68%
    


```python
# Cross Entropy ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ëŠ” ëª¨ë¸ í•™ìŠµ
training_result_ce = training(mlp_ce, learning_rate_ce, X_train, y_train, X_test, y_test)
```


      0%|          | 0/30 [00:00<?, ?it/s]


    [Epoch 1/30] Training Loss = 1.2817 / Training Acc = 78.35% /Test Loss = 1.2592 / Test Acc = 79.07%
    [Epoch 2/30] Training Loss = 0.9221 / Training Acc = 85.28% /Test Loss = 0.9070 / Test Acc = 85.67%
    [Epoch 3/30] Training Loss = 0.7580 / Training Acc = 88.09% /Test Loss = 0.7579 / Test Acc = 87.85%
    [Epoch 4/30] Training Loss = 0.6576 / Training Acc = 89.78% /Test Loss = 0.6735 / Test Acc = 89.48%
    [Epoch 5/30] Training Loss = 0.5894 / Training Acc = 90.89% /Test Loss = 0.6122 / Test Acc = 90.49%
    [Epoch 6/30] Training Loss = 0.5411 / Training Acc = 91.71% /Test Loss = 0.5706 / Test Acc = 91.30%
    [Epoch 7/30] Training Loss = 0.5020 / Training Acc = 92.35% /Test Loss = 0.5382 / Test Acc = 91.67%
    [Epoch 8/30] Training Loss = 0.4738 / Training Acc = 92.78% /Test Loss = 0.5154 / Test Acc = 92.13%
    [Epoch 9/30] Training Loss = 0.4426 / Training Acc = 93.34% /Test Loss = 0.4928 / Test Acc = 92.31%
    [Epoch 10/30] Training Loss = 0.4239 / Training Acc = 93.69% /Test Loss = 0.4703 / Test Acc = 92.66%
    [Epoch 11/30] Training Loss = 0.3990 / Training Acc = 94.11% /Test Loss = 0.4594 / Test Acc = 93.09%
    [Epoch 12/30] Training Loss = 0.3824 / Training Acc = 94.38% /Test Loss = 0.4462 / Test Acc = 93.12%
    [Epoch 13/30] Training Loss = 0.3677 / Training Acc = 94.62% /Test Loss = 0.4353 / Test Acc = 93.43%
    [Epoch 14/30] Training Loss = 0.3595 / Training Acc = 94.71% /Test Loss = 0.4331 / Test Acc = 93.41%
    [Epoch 15/30] Training Loss = 0.3485 / Training Acc = 94.86% /Test Loss = 0.4274 / Test Acc = 93.48%
    [Epoch 16/30] Training Loss = 0.3341 / Training Acc = 95.18% /Test Loss = 0.4145 / Test Acc = 93.70%
    [Epoch 17/30] Training Loss = 0.3254 / Training Acc = 95.28% /Test Loss = 0.4119 / Test Acc = 93.75%
    [Epoch 18/30] Training Loss = 0.3126 / Training Acc = 95.48% /Test Loss = 0.4050 / Test Acc = 93.86%
    [Epoch 19/30] Training Loss = 0.3018 / Training Acc = 95.64% /Test Loss = 0.3918 / Test Acc = 94.15%
    [Epoch 20/30] Training Loss = 0.2981 / Training Acc = 95.74% /Test Loss = 0.3927 / Test Acc = 94.21%
    [Epoch 21/30] Training Loss = 0.2881 / Training Acc = 95.87% /Test Loss = 0.3849 / Test Acc = 94.17%
    [Epoch 22/30] Training Loss = 0.2793 / Training Acc = 96.05% /Test Loss = 0.3797 / Test Acc = 94.26%
    [Epoch 23/30] Training Loss = 0.2729 / Training Acc = 96.20% /Test Loss = 0.3779 / Test Acc = 94.43%
    [Epoch 24/30] Training Loss = 0.2687 / Training Acc = 96.20% /Test Loss = 0.3792 / Test Acc = 94.26%
    [Epoch 25/30] Training Loss = 0.2590 / Training Acc = 96.33% /Test Loss = 0.3752 / Test Acc = 94.29%
    [Epoch 26/30] Training Loss = 0.2513 / Training Acc = 96.54% /Test Loss = 0.3678 / Test Acc = 94.50%
    [Epoch 27/30] Training Loss = 0.2477 / Training Acc = 96.61% /Test Loss = 0.3719 / Test Acc = 94.42%
    [Epoch 28/30] Training Loss = 0.2400 / Training Acc = 96.65% /Test Loss = 0.3657 / Test Acc = 94.48%
    [Epoch 29/30] Training Loss = 0.2398 / Training Acc = 96.64% /Test Loss = 0.3667 / Test Acc = 94.42%
    [Epoch 30/30] Training Loss = 0.2402 / Training Acc = 96.61% /Test Loss = 0.3690 / Test Acc = 94.44%
    

---

## 4. ê²°ê³¼ ì‹œê°í™”í•˜ê¸°


### 4-1. í•™ìŠµ ê³¡ì„  ì‹œê°í™”

> ë”¥ëŸ¬ë‹ ë¶„ì•¼ì—ì„œì˜ í•™ìŠµ ê³¡ì„ (Learning Curve)ì€ ëª¨ë¸ì´ ì˜ í•™ìŠµë˜ê³  ìˆëŠ”ì§€ë¥¼ ê° epochì— ëŒ€í•´ ì„ ê·¸ë˜í”„ë¡œ ë„ì‹í™”í•œ ê²ƒì„ ì˜ë¯¸.

-  ğŸ‘¨â€ğŸ’» <font color='green'><b>[ ì½”ë“œ ]</b></font> í•™ìŠµ ê³¡ì„  ê·¸ë¦¬ê¸° - MSE
-  ğŸ‘¨â€ğŸ’» <font color='green'><b>[ ì½”ë“œ ]</b></font> í•™ìŠµ ê³¡ì„  ê·¸ë¦¬ê¸° - CE
-  ğŸ‘¨â€ğŸ’» <font color='green'><b>[ ì½”ë“œ ]</b></font> í•™ìŠµ ê³¡ì„  ê·¸ë¦¬ê¸° - ì •í™•ë„


#### ğŸ‘¨â€ğŸ’» <font color='green'><b>[ ì½”ë“œ ]</b></font> í•™ìŠµ ê³¡ì„  ê·¸ë¦¬ê¸° - MSE



```python
# í•™ìŠµ ë° í…ŒìŠ¤íŠ¸ ì—ëŸ¬ ì‹œê°í™”í•˜ê¸°
plt.figure(figsize=(10, 5))
plt.plot(range(1,n_epochs+1), training_result_mse[0], 'bo-', label='Training Error')
plt.plot(range(1,n_epochs+1), training_result_mse[1], 'ro-', label='Test Error')
plt.xlabel('Epoch')
plt.ylabel('Mean Square Error (MSE)')
plt.grid(True)
plt.legend()
plt.show()
```


    
![png](\..\img\DL_practice_75_0.png)
    


#### ğŸ‘¨â€ğŸ’» <font color='green'><b>[ ì½”ë“œ ]</b></font> í•™ìŠµ ê³¡ì„  ê·¸ë¦¬ê¸° - CE



```python
# í•™ìŠµ ë° í…ŒìŠ¤íŠ¸ ì—ëŸ¬ ì‹œê°í™”í•˜ê¸°
plt.figure(figsize=(10, 5))
plt.plot(range(1,n_epochs+1), training_result_ce[0], 'bo-', label='Training Error')
plt.plot(range(1,n_epochs+1), training_result_ce[1], 'ro-', label='Test Error')
plt.xlabel('Epoch')
plt.ylabel('Cross Entropy (CE)')
plt.grid(True)
plt.legend()
plt.show()
```


    
![png](\..\img\DL_practice_77_0.png)
    


#### ğŸ‘¨â€ğŸ’» <font color='green'><b>[ ì½”ë“œ ]</b></font> í•™ìŠµ ê³¡ì„  ê·¸ë¦¬ê¸° - ì •í™•ë„



```python
# í•™ìŠµ ë° í…ŒìŠ¤íŠ¸ ì •í™•ë„ ì‹œê°í™”í•˜ê¸°
plt.figure(figsize=(10, 5))
plt.plot(range(1,n_epochs+1), training_result_mse[2]*100, 'b.-', label='MSE Training Accuracy')
plt.plot(range(1,n_epochs+1), training_result_mse[3]*100, 'r.-', label='MSE Test Accuracy')
plt.plot(range(1,n_epochs+1), training_result_ce[2]*100, 'b.--', label='CE Training Accuracy')
plt.plot(range(1,n_epochs+1), training_result_ce[3]*100, 'r.--', label='CE Test Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy (%)')
plt.grid(True)
plt.legend()
plt.show()
```


    
![png](\..\img\DL_practice_79_0.png)
    


### 4-2. ì¼ë¶€ ë°ì´í„° ì‹œê°í™”

> ë°ì´í„° ìƒ˜í”Œ ì¼ë¶€ë¥¼ ì¶”ì¶œí•˜ì—¬ ì‹œê°í™”í•¨ìœ¼ë¡œì¨ ëª¨ë¸ì˜ ì„±ëŠ¥ì— ëŒ€í•œ ë³´ë‹¤ ì§ê´€ì ì¸ ì´í•´.
>
> ê°„ë‹¨í•˜ê²Œ 25ê°œì˜ ë°ì´í„° ìƒ˜í”Œì— ëŒ€í•´ ì‹¤ì œê°’ê³¼ ì˜ˆì¸¡ê°’ì„ í”„ë¦°íŠ¸í•´ë´…ë‹ˆë‹¤.
> <br>(MSE ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•œ ëª¨ë¸ì„ ì‚¬ìš©)


#### ğŸ‘¨â€ğŸ’» <font color='green'><b>[ ì½”ë“œ ]</b></font> ì„ì˜ì˜ 25ê°œ ìƒ˜í”Œ ì‹œê°í™”í•˜ê¸°



```python
# 25ê°œì˜ ì„ì˜ì˜ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•œ ë¶„ë¥˜ ê²°ê³¼ ì‹œê°í™”í•˜ê¸°
n_sample = 25  # number of samples

# í…ŒìŠ¤íŠ¸ì…‹ìœ¼ë¡œë¶€í„° ì„ì˜ì˜ 25ê°œì˜ ì´ë¯¸ì§€ ì¶”ì¶œ ë° ì´ì— ëŒ€ì‘í•˜ëŠ” ë¼ë²¨ê³¼ ëª¨ë¸ ê²°ê³¼ ìƒì„±
sample_indices = np.random.choice(len(y_test), n_sample, replace=False)
sample_images = X_test[sample_indices].reshape((-1,28,28))
sample_labels = np.argmax(y_test[sample_indices], axis=1)
sample_pred = np.argmax(mlp_mse.predict(X_test[sample_indices]), axis=1)

# 7x7 ì‚¬ì´ì¦ˆì˜ ìƒˆë¡œìš´ figure ìƒì„±
plt.figure(figsize=(7,7))
for idx in range(n_sample):
    # 5x5 ê·¸ë¦¬ë“œì˜ (idx+1)ë²ˆì§¸ ìœ„ì¹˜ì— ê·¸ë˜ì´ìŠ¤ì¼€ì¼ ì´ë¯¸ì§€ë¥¼ ì¶œë ¥
    # íƒ€ì´í‹€ë¡œëŠ” ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ì„ ì¶œë ¥
    plt.subplot(5, 5, idx+1)
    plt.imshow(sample_images[idx], cmap='gray')
    plt.axis('off')
    plt.title("Pred:%d, Label:%d"%(sample_pred[idx], sample_labels[idx]), fontsize=10)

plt.tight_layout()
plt.show()

```


    
![png](\..\img\DL_practice_82_0.png)
    


#### ğŸ‘¨â€ğŸ’» <font color='green'><b>[ ì½”ë“œ ]</b></font> ì •ë‹µê³¼ ë‹¤ë¥´ê²Œ ì˜ˆì¸¡í•œ 25ê°œ ìƒ˜í”Œ ì‹œê°í™”í•˜ê¸°



```python
# 25ê°œì˜ ì˜¤ë‹µ ë°ì´í„°ì— ëŒ€í•œ ë¶„ë¥˜ ê²°ê³¼ ì‹œê°í™”í•˜ê¸°
n_sample = 25  # number of samples

# í…ŒìŠ¤íŠ¸ì…‹ìœ¼ë¡œë¶€í„° í‹€ë¦° 25ê°œì˜ ì´ë¯¸ì§€ ì¶”ì¶œ ë° ì´ì— ëŒ€ì‘í•˜ëŠ” ë¼ë²¨ê³¼ ëª¨ë¸ ê²°ê³¼ ìƒì„±
pred_result = np.argmax(mlp_mse.predict(X_test), axis=1)
wrong_indices = np.where(pred_result != np.argmax(y_test, axis=1))[0]
sample_indices = np.random.choice(wrong_indices, n_sample, replace=False)
sample_images = X_test[sample_indices].reshape((-1,28,28))
sample_labels = np.argmax(y_test[sample_indices], axis=1)
sample_pred = pred_result[sample_indices]

# 7x7 ì‚¬ì´ì¦ˆì˜ ìƒˆë¡œìš´ figure ìƒì„±
plt.figure(figsize=(7,7))
for idx in range(n_sample):
    # 5x5 ê·¸ë¦¬ë“œì˜ (idx+1)ë²ˆì§¸ ìœ„ì¹˜ì— ê·¸ë˜ì´ìŠ¤ì¼€ì¼ ì´ë¯¸ì§€ë¥¼ ì¶œë ¥
    # íƒ€ì´í‹€ë¡œëŠ” ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ì„ ì¶œë ¥
    plt.subplot(5, 5, idx+1)
    plt.imshow(sample_images[idx], cmap='gray')
    plt.axis('off')
    plt.title("Pred:%d, Label:%d"%(sample_pred[idx], sample_labels[idx]), fontsize=10)

plt.tight_layout()
plt.show()

```


    
![png](\..\img\DL_practice_84_0.png)
    


#Reference

- [MNIST ë°ì´í„°ì…‹ ì´ë¯¸ì§€ ì¶œì²˜](https://upload.wikimedia.org/wikipedia/commons/f/f7/MnistExamplesModified.png)
- [hwalsuklee/numpy-neuralnet-exercise](https://github.com/hwalsuklee/numpy-neuralnet-exercise)

## Required Package

numpy version >= 1.23.5 <br>
matplotlib version >= 3.7.1





```python

```
