---
layout: post
title: DL Recurrent Neural Network
date: 2023-12-21 16:28 +0900
last_modified_at: 2023-12-21 17:14:00 +0900
tags: [DeepLearning, rnn]
toc:  true
---

# RNN

## 1. 순환 신경망

### 1.1 순환 신경망의 등장

크기가 고정되지 않는 데이터를 다룰 때에는 DNN, CNN을 쓸수가 없었다.
<br>텍스트, 음성, 주식 가격처럼 길이가 고정되어 있지 않은 Sequential data를 다룰려면 어떻게 해야할까

$\Rightarrow$ 이러한 배경아래 <mark>순환 신경망 (Recurrent Neural Network)</mark>이 등장

![Alt text](\..\img\DL4-2.png)

### 1.2 순환 신경망의 구성 요소

#### 순환 신경망의 구조

핵심 : 이전 시점의 정보를 현재 시점의 입력과 함께 처리하는 순환구조

**은닉 상태** (Hidden State) : 순환 신경망의 핵심적인 요소
<br> 네트워크가 시간에 따라 어떤 정보를 기억할지 결정한다.

![Alt text](\..\img\DL4-3.png)

1. one-to-one :
<br>입력값이 1개, 출력값이 1개인 경우

2. one-to-many :
<br>입력값이 1개, 출력값이 여러 개인 경우
<br>ex. 1개의 이미지를 입력으로 넣고, 이미지를 설명하는 문장(단어의 집합)을 출력하는 경우

3. many-to-one :
<br>입력값이 여러 개, 출력값이 여러개인 경우
<br>ex. 문장을 입력 받아, 문장의 긍정 부정 여부를 확인

4. many-to-many :
<br>입력값이 여러 개, 출력값이 여러 개인 경우
<br>ex. 기계 번역 : 문장을 다른 언어의 문장으로 출력
<br>ex. 비디오 scene 분류 : 이미지의 집합인 비디오의 장면에서 분류

![Alt text](\..\img\DL4-4.png)

$W_h, W_x, W_y$는 학습해야할 가중치이다.

현재 은닉 상태 : $h_t = tanh\left( W_x x_t + W_h h_{t-1} + b \right)$

출력 : $y_t = f(W_y h_t + b)$

---
#### 순환 신경망의 한계

시퀀스가 길어질수록, 앞 정보를 잊어버리는 문제가 발생 (*장기 의존성의 문제*)

이러한 장기 의존성 문제를 완화하기 위해 LSTM이나 GRU와 같은 순환 신경망 구조가 등장함

---
#### LSTM (Long Short-Term Memory)

gate가 3개로 구성되어있음

![Alt text](\..\img\DL4-5.png)

1. forget gate (파란색)
<br>가장 첫 단계
<br>기존 정보중 어떤 정보를 버릴지 선택한다.

2. input gate (빨간색)
<br>입력 데이터 중 어떤 정보를 다음 상태로 기억할지 선택한다.

3. output gate (노란색)
<br>다음 상태로 어떤 정보를 내보낼지 선택한다.

이렇게 3 gates를 만드니 장기 의존성을 해결할 수 있더라

---
#### GRU (Gated Recurrent Unit)

LSTM을 보다 단순화한 구조로 2개의 gate만 사용한다.
<br>1. Reset gate, 2. Update gate
<br>적은 파라미터를 가지게 된다.

![Alt text](\..\img\DL4-6.png)

1. Reset gate
<br>이전 상태의 정보가 얼마나 현재 상태의 계산에 사용될지 결정한다.
<br>즉, 이전 정보를 얼마나 '리셋'할지 선택하는 gate

2. Update Gate
<br>이전 상태를 얼마나 현재 상태에 보존할지, 새로운 정보를 얼마나 현재 상태에 반영할지 결정한다.
<br>LSTM의 forget gate와 input gate의 기능을 하나로

 