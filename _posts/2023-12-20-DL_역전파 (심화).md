---
layout: post
title: DL 모델 학습법 III, 역전파 (심화)
date: 2023-12-20 11:50 +0900
last_modified_at: 2023-12-20 12:40:00 +0900
tags: [DeepLearning, backpropagation]
toc:  true
---

# 역전파 (심화)

## 1. 손실함수의 기본 가정

note

활성화 함수 입력이 z, 출력이 a

a는 다음 레이어의 입력

l 번째 layer의 출력 matrix 식 : $a^l = \sigma (w^l a^{l-1} + b^l)$

---
*손실함수의 가정 1*

<mark>학습 데이터 샘플에 대한 신경망의 총 손실은, 각 데이터 샘플에 대한 손실의 합과 같다.</mark>

비용함수는 설계하기 나름이지만, 샘플 별 합의 형태로 표현했을 경우에만 역전파 기법을 사용할 수 있다.

이 가정을 통해 우리는 총 손실의 편미분 값을 샘플별 편미분 값으로 계산할 수 있다.

---
*손실함수의 가정 2*

<mark>각 학습 데이터 샘플에 대한 손실은 a^L에 대한 함수이다.</mark>

> note : *l*번째 레이어의 *j*번째 뉴런에 대한 에러 = $\delta^l_j$


## 2. 역전파의 기본 방정식

*기본 방정식 1*

출력층의 error $\delta^L$

출력층의 j 번째 node $z^L_j$에 대한 C의 편미분 값은

$\displaystyle \frac{\partial C}{\partial z^L_j} = \displaystyle \frac{\partial C}{\partial a^L_j} \times \displaystyle \frac{\partial a^L_j}{\partial z^L_j}$

$=\displaystyle \frac{\partial C}{\partial a^L_j} \times \sigma^{\prime} (z^L_j)= \delta^L_j$

$where\;\sigma = activation\;function$

sample에 대한 cost, C는 아래와 같다.

$where\;K = (num\;of\;output\;node)$

$C = \displaystyle \frac{1}{2} \displaystyle \sum_K (y^L_k - a^L_K)^2$

$\displaystyle \frac{\partial C}{\partial z^L_j}$   *(C 대입)*

$= \displaystyle \frac{\partial}{\partial z^L_j} \left(\displaystyle \frac{1}{2} \displaystyle \sum_K \left(y^L_k - a^L_K \right)^2 \right)$   *(a는 활성화함수의 출력값)*

$= \displaystyle \frac{\partial}{\partial z^L_j} \left(\displaystyle \frac{1}{2} \displaystyle \sum_K \left(y^L_k - \sigma (z^L_K) \right)^2 \right)$ *(각 층의 노드간 연결은 없음)*

$= \displaystyle \frac{\partial}{\partial z^L_j} \left(\displaystyle \frac{1}{2} \left(y^L_k - \sigma (z^L_j) \right)^2 \right)$

$= - \left(y_j - \sigma(z^L_j) \right)\sigma^{\prime} \left( z^L_j \right) = \left( a^L_j - y_j \right) \sigma^{\prime} \left( z^L_j \right)$

matrix로 표현하면 아래와 같다.

$\delta^L = \nabla_a C \bigodot \sigma^{\prime} (z^L)$

> $\bigodot$은 Hadamard Product로, 벡터 간 연산 중 하나.
>
> 같은 위치의 원소의 곱을 출력한다.

---
*기본 방정식 2*

l+1 번째 layer error에서의 l 번째 layer의 error

엄청 복잡하지만 천천히 진행해보자

일단 가중치를 곱한 후 합하는 연산으로 인해 Layer 간 관계는 기본적으로 합으로 표현된다. $\left( \sum \right)$

$\delta^l_j$ *(l 번째 layer의 j번째 node)*

$ = \displaystyle \sum_k \displaystyle \frac{\partial C}{\partial z^{l+1}_k} \frac{\partial z^{l+1}_k}{\partial z^l_j}$

$ = \displaystyle \sum_k \delta^{l+1}_k \frac{\partial z^{l+1}_k}{\partial z^l_k}$

$ = \displaystyle \sum_k \delta^{l+1}_k w^{l+1}_{kj} \sigma^{\prime}(z^l_j)$

> $\because z^{l+1}_k = \displaystyle \sum w^{l+1}_{kj} a^l_j + b^{l+1}_k = \displaystyle \sum_j w^{l+1}_{kj} \sigma(z^l_k) + b^{l+1}_k $
>
> $\displaystyle \frac{\partial z^{l+1}_k}{\partial z^l_k} = w^{l+1}_{kj} \sigma^{\prime}(z^l_k)$

$ \delta^l_k = \displaystyle \sum_k \delta^{l+1}_k w^{l+1}_{kj} \sigma^{\prime} (z^l_j) = \sigma^{\prime}(z^l_j) \displaystyle \sum_k \delta^{l+1}_k w^{l+1}_{kj}$

matrix로 표현하면 아래와 같다.

$ \delta^l = \sigma^{\prime} (z^l) \bigodot \left( (w^{l+1})^T \delta^{l+1} \right)$

---
*기본 방정식 3*

bias에 대한 Cost의 변화량

$\displaystyle \frac{\partial C}{\partial b^l_j} = \frac{\partial C}{\partial z^l_j} \frac{\partial z^l_j}{\partial b^l_j} = \frac{\partial C}{\partial z^l_j} = \delta^l_j$

$\because \displaystyle \frac{\partial z^l_j}{\partial b^l_j} = 1$

matrix 형태의 식은 아래와 같다.

$\nabla_{b^l} C = \delta^l$

---
*기본 방정식 4*

weight에 대한 Cost의 변화량

$\displaystyle \frac{\partial C}{\partial w^l_{jk}} = \frac{\partial C}{\partial z^l_j}  \frac{\partial z^l_j}{\partial w^l_{jk}} = \frac{\partial C}{\partial z^l_j} a^{l-1}_k = \delta^l_k a^{l-1}_k$

matrix 형태의 식은 아래와 같다.

$\nabla_{W^l} C = \delta^l \left( a^{l-1} \right)^T$

- insight 1 : 뉴런 a의 활성화 정도가 작다면 ($ a^{l-1}_k $)가 작다면 $w^l_{jk}$의 변화가 C에 영향을 주지 않는다.
- insight 2 : 출력층에서의 출력값이 거의 0 또는 1일 경우, 관련 뉴런의 가중치, bias는 느리게 학습된다. (이미 잘 맞추는 문제는 더 이상 풀지 않는다)

---
*summary*

![Alt text](\..\img\DL2-11.png)

## 3. 역전파 알고리즘

Input : $x$

FeedForward : L개의 layer에 대해서 $z^l = w^la^{l-1} + b^l$, $a^l = \sigma(z^l)$을 계산한다.

output error : $\delta^L = \nabla_aC \bigodot \sigma^{\prime}(z^L)$ 계산

역전파 : 2,3,...,l-1 layer에 대해 $\delta^l = \sigma^{\prime} (z^l) \bigodot \left( (w^{l+1})^T \delta^{l+1} \right)$ 계산

output : $\delta^l_j$로 비용함수의 gradient 계산

여기서 나온 gradient를 통해서, 경사하강법에서는 $w^l$을 업데이트한다.

$w^l \rightarrow w^l - \displaystyle \frac{\eta}{m} \sum_x \delta^{x,l}(a^{x,l-1})^T$

$b^l \rightarrow b^l - \displaystyle \frac{\eta}{m} \sum_x \delta^{x,l}$