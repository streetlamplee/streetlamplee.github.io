---
title: ML. 파생 변수 선택방법
layout: post
date: 2024-01-12 11:00 +0900
last_modified_at: 2024-01-12 12:00:00 +0900
tag: [ML, feature engineering]
toc: true
---

# 파생 변수 선택 방법

모델에 들어갈 변수가 많으면, 추론 시간이 늘어나고, 모델의 복잡도가 높아지고, 메모리 상의 문제가 생길 수도 있다.

따라서 들어갈 변수를 선택해서 시간을 줄이고, 비용을 줄일 수 있게 된다.

## 1. feature selection

### 1.1 feature selection의 중요성

**feature selection**<br>
학습에 필요한 변수를 중요도에 따라 선택하는 과정

---

*이유*
1. 차원의 저주 해소
2. 모델의 성능 향상 및 과적합 완화
3. 학습 및 추론 시간, 메모리 개선
4. 해석 가능성 증대

## 2. feature selection의 방법

### 2.1 overview

3가지 접근법이 있다.

1. filter : 변수들간 통계적 관계를 기반으로
2. wrapper : 실제 머신러닝 모델의 성능을 기반으로
3. 모델 훈련 과정에서

### 2.2 filter methods

변수간의 통계적 관계를 평가해서 변수의 중요도를 고려하는 방법

e.g. <mark>상관관계</mark>, <mark>분산</mark><br>
상관관계가 너무 높은 변수는 제거해서 차원을 줄이고,<br>
분산이 너무 적은 변수는 변동성이 낮기 때문에 이를 제거한다.

연속형 데이터의 경우는 상관계수를 확인해서 제거할 수 있고,<br>
범주형 데이터의 경우에는 카이제곱 검정을 진행한다.

*카이제곱 검정*<br>
target variable과 독립변수들의 관계성을 카이제곱 통계량으로 판단.

### 2.3 wrapper methods

실제 모델의 성능을 활용하여 변수를 선택하는 방법

성능에 도움이 되는 feature는 유지하고, 그렇지 않은 feature는 제거한다.

이를 이용해서 최적의 변수 조합을 찾는 방법이다.

1. 순차적 특성 선택 : 변수를 하나씩 추가하면서 탐색<br>
하나하나 유의미한 feature를 추가해나감<br>
'유의미'의 기준은 모델의 성능 및 평가지표를 이야기한다.<br>
즉 feature를 넣어보고, 모델의 성능이 오르면 유지, 아니면 다시 제거한다.<br>
모든 경우에 대해 처리하기 때문에, 시간은 오래 걸리지만 성능은 좋은 모델을 찾을 수 있다.
2. 재귀적 특성 제거 : 변수를 하나씩 제거하면서 탐색<br>
후진적으로 제거 하는 방법<br>
전제 feature 중에서 가장 무의미한 feature를 제거하는 방법<br>
'무의미'의 기준은 해당 feature를 제거했을 때, 모델의 성능 하락폭이 얼마나 되는지

### 2.4 embedded methods

모델의 훈련 과정에서 변수의 중요도를 평가해서, 모델에 대한 변수의 기여도를 결정하는 방법

ex. 트리 모델의 feature_importances_, Regularization 기반

모델의 특성을 잘 반영하고, 변수의 중요도와 모델의 복잡성을 동시에 고려한다.

---

*feature importances*

트리 node 분할에서 각 feature의 기여도로 변수의 중요도를 판단하는 방법

기준은 보통 Gini, Entropy

트리 split을 할 때, 각 변수의 중요도를 이해할 수 있다.

따라서 모델의 학습 과정에서 해당 feature의 기여도를 줄이는 방법으로 학습할 수 있다.

특정 feature가 **트리의 순수도(Gini, Entropy)가 높은 분할에 도움**이 된다면, **해당 feature의 중요도가 높다** 라고 이해할 수 있다.

---

*regularization 기반 selection*

L1, L2 등의 정규화를 사용해서 특정 feature의 weight를 0 또는 0으로 가깝게 만들어서 feature 제거의 효과를 만든다.

### 2.5 summary

![Alt text](\..\img\ML2-1.png)
