---
layout: post
title: ISLP chapter 3 정리
date: 2023-11-09 19:00 +0900
last_modified_at: 2023-11-09 19:00:00 +0900
tags: [Statistics, ISLP]
toc:  true
---

## 3. Linear Regression

많은 다양한 통계적 기법이 나왔음에도 불구하고 아직 많이 쓰이는 방법이다.
또한 많은 통계적 기법 자체가 Linear regression에 기저를 두고 있다.

데이터를 통해 무언가를 제안하기 위해서는 아래와 같은 질문에 답할 수 있어야한다.
    1. 독립변수와 종속변수 간에 관계성이 있는가.
    2. 독립변수와 종속변수 간의 관계성이 얼마나 강한가.
    3. 어떤 독립변수가 종속변수와 연관되어 있는가 (전부 혹은 일부)
    4. 독립변수의 특정 분위 (구간) 과 종속변수의 관계성이 얼마나 강한가
    5. 예측 종속변수의 정확도는 얼마인가
    6. 관계성이 선형인가
    7. 독립 변수에 시너지 효과가 있는가.

### 3.1. Simple Linear Regression

독립변수 1개와 종속변수 1개인 선형회귀모델

Y = B0 + B1 * X 의 수식으로 표현 가능
이때 B0 는 intercept, B1 은 Slope

#### 3.1.1 Estimating the Coefficients

주어진 데이터를 통해서 선형 모델을 가장 가깝게 Coefficient를 조정한다.
이때 거리를 계산하는 방법으로는 Least square을 사용하며
이 값이 적을수록 거리가 가깝다고 표현한다.

각 관측값(Y)과 Y^ = B0^ + B1^ * X 에 X값을 넣어나온 예측값(Y^)의 차이를
**residual**이라고 한다.
(residual = Y - Y^)

이렇게 나온 각 residual을 제곱하여 더한것이 Residual Sum of Squares(RSS)이다.
Let Y^ = B0^ + B1^ * X and e = Y - Y^
RSS = e1 ^2 + e2 ^2 + ... + en ^2

Least Squares 방법은 이 RSS가 최소가 되도록 하는 B0^과 B1^을 선택한다.

#### 3.1.2 Assessing the Accuracy of the Coefficient Estimates

chap 2에서 말한 Y = F(X) + e 라는 수식은 linear regression에서는

Y = B0 + B1 * X + e로 바뀐다

B0 : intercept (y절편), X가 0일때의 Y 예측값
B1 : slope (기울기), X가 1 단위 증가 할때 Y의 증가량
e : simple model에서 잡지 못한 모든 변화 값 (Y에 영향을 주지만 반영되지 않은 독립변수, 선형적이지 않은 원본 데이터 등등)
**e는 X와 독립이라고 가정한다**


물론 모집단에서 어떤 샘플을 가지고 오냐에 따라 coefficient가 달라지기는 하지만 많은 데이터를 통해 학습한다면 맞춰질 수 있다.

그럼 샘플을 통해 예측한 평균이 모집단의 평균과 얼마나 정확한지는 어떻게 파악할 수 있을까?
-> 예측 평균의 standard error를 계산한다. (SE(m^))

standard error는 m^ (예측 평균)이 m(실제 평균)과 얼마나 차이가 있는지를 알려준다.

같은 의미로 linear regression의 B0^과 B1^이 실제 값과 얼마나 차이가 나는지도 standard error로 알 수 있으며, 이 수식은 책의 3.8 수식과 같다.

결국 수식에서도 n이 분모의 자리에 들어가 있어서, 데이터의 크기가 클수록 정확성이 높아진다고 볼 수 있다.
또한 standard error는 신뢰구간의 추정에도 사용된다.
이때 신뢰구간은 예측하고자 하는 값의 실제값이 이 구간에 들어있을 확률이 1-(유의수준)인 구간을 의미한다.
대부분의 신뢰구간은 B1^ +- 2 * SE(B1^)의 구간을 가진다.

또한 stadard error는 귀무 검증에서도 사용가능하다.
귀무 가설 : 확인해보고자 하는 가정의 반대
대립 가설 : 확인해보고자 하는 가정

p-value : 귀무 가설을 참이라고 가정 하에서 실제로 관측된 값, 혹은 그보다 더 극단적인 값을 얻을 확률
이 p-value가 유의수준보다 낮다면, 우리는 귀무가설을 기각하고 대립 가설을 채택한다.
보통 유의수준은 5% 혹은 1%로 한다.

#### 3.1.3 Assessing the Accuracy of the Model

3.1.2에서는 계수가 얼마나 데이터를 잘 나타내는지를 확인해보기 위해 standard error를 사용했다.
그럼 이제 모델이 데이터를 얼마나 잘 나타내는지는 어떻게 알 수 있을까
**Residual Standard Error** and **R^2**

1. Residual standard error
Residual Standard Error는 관측값과 예측값의 차이(Residual Sum of Square) (Y - Y^) (e) 의 표준편차
결국 Residual Standard Error는 예측값와 관측값이 평균적으로 얼마나 차이가 있는지를 나타낸다.
이 값을 통해 2가지 정도로 생각할 수 있다.
    - 실제 관측값은 모델을 통해 예측한 값과 RSE 정도의 오차가 있다.
    - 아무리 모델이 잘 만들어져서 모든 계수를 **완벽하게** 예측해도 RSE만큼의 오차가 발생할 수 있다.
결국 잘 만들어진 모델은 RSE의 값이 작게 나올것이다.
그러나 이렇게 만들어진 RSE의 값도 결국 절대적인 수를 통해 나타내므로, 어느 정도가 '잘 만들어진 모델'인지 명백하게 파악할 수 없다.

2. R^2
그래서 나온게 이 R^2
비율 형태를 띄고 있어 0에서 1사이의 값으로 모델이 잘 만들어진지를 파악할 수 있다. (Y의 크기와 독립이다.)
Let TSS (Total Sum of Square) = 관측치의 분산, RSS (Residual Sum of Square) 
R^2 = (TSS - RSS) / TSS
TSS는 모델링의 영향을 받기전에 관측치들이 얼마나 흩어져 있는지를 파악하고
RSS 는 모델링 후에 잔차들이 얼마나 흩어져있는지를 파악한다. (regression model이 설명하지 못하는 분산)
따라서 TSS - RSS 는 관측치의 흩어진 정도에서 regression model에 의해서 설명되어지지 않는 흩어진 정도를 빼게 된다.
이를 TSS로 나누게 되면서 비율로 표현할 수 있게 됨
결국 R^2는 1에 가까울수록 모델이 RSS가 최소가 되도록 만들어졌다는 것을 의미함.
    - 1에 가까울수록 TSS 중에서 모델이 많은 부분을 설명해줄 수 있다
    - 0에 가까울수록 TSS 중에서 모델이 설명해줄 수 있는 분산이 적다.
이렇게 0과 1중에 가까운지 정도로 모델이 잘 만들어 졌는지 확인할 수 있어서 좋긴하지만 그래도 어느 정도의 R^2값을 통해 모델을 채택할지는 아직 미지수임
그리고 그 정도는 모델이 사용되어질 분야에 따라서 달라진다.
R^2 통계량은 X와 Y의 선형 관계를 나타내는 측도이기도 하다. Correlation도 마찬가지다.
이 말인 즉슨, Correlation도 R^2 대신에 잘 만들어진 모델을 평가하기에 사용할 수 있다.
(r = Corr(X, Y) 일 때, r^2 = R^2 이므로)
하지만 이건 단순선형회귀일때이고, 다중선형회귀로 넘어가면 단순히 r = Corr(X, Y)로 나타낼 수 없으니까 R^2를 알려줬다.

### 3.2 Multiple Linear Regression

현실은 1개의 독립변수만 있지 않다.

만약 3개의 독립변수가 있을 때, 그 3개의 독립변수에 대해 모두 Simple Linear Regression을 할 수 있지만 3개의 regression model을 합치할 때에 문제가 생긴다.
    1. 독립변수가 다른 독립변수와 독립이 아닐 때 (ex. 뉴스 광고비 = 총 광고비 - 라디오 광고비)
    2. 각 regression model을 만들 때의 데이터가 다른 독립변수로 인한 영향을 받지 않았음이 불분명할 때.

그러니까 그냥 Simple linear regression 을 3번하지 말고, Multiple linear regression을 통해 한번에 3개의 독립변수를 학습시키자.

Let.
X_j = j 번째 독립변수
B_j = X_j 이외의 독립변수가 고정되어 있을 때, X_j 의 단위증가로 인한 Y의 증가량 (B_0 은 상수항)
e = 독립변수와 상수항의 영향을 받지 않는 오차

Y = B0 + (B1 * X1) + (B2 * X2) + (B3 * X3) + ... + (Bp * Xp) + e

#### 3.2.1 Estimating the Regression Coefficeints

왜 다중 회귀를 써야하는 지 알았다.

그럼 다중 선형 회귀에 들어가는 저 계수 (B_j) 는 어떻게 예측할까
똑같이 least squares 방법을 이용한다.
다만 least squares의 수식이 달라질 뿐
Y^ = B_0 + B_1 * X_1 + B_2 * X_2 + ... + B_n * X_n + e 만 바뀐다고 생각하자
각 변수의 예측값은 행렬로 표현해야할 정도로 복잡함

각 변수의 Correlation을 비교해서 각 변수간의 관계를 알 수도 있음
각 변수간의 관계를 통해 종속변수에 영향을 주는 독립변수를 확인할 수 있으며, 또한 어떤 독립변수가 종속변수와 관계성이 없는지도 알 수 있음

#### 3.2.2 Some Important Questions

    1. 적어도 하나의 독립변수라도 종속변수를 예측하는데 유용한가
    2. 모든 독립변수가 종속변수를 설명하는가 혹은 그 중 일부만 유용한가
    3. 모델은 데이터에 얼마나 부합하는가
    4. 독립변수가 주어지면 종속변수의 예측값은 무엇이고 그 정확도는 얼마인가

1. 적어도 하나의 독립변수라도 종속변수를 예측하는데 유용한가
결국 관계성이 있다는 것을 증명하기 위해서는 **귀무가설 : 모든 계수가 0이 참이다 / 대립가설 : 하나의 계수라도 0이 아니다**
이러한 귀무 검정은 F 통계량을 이용해서 계산이 된다.
F 통계량은 ((TSS - RSS) / p) / (RSS / (n - p - 1))
선형 모델이라는 가정이 참이라는 가정하에 ((TSS - RSS) / p) 의 기대 값도 σ^2, (RSS / (n - p - 1)) 의 기대값도 σ^2이다. 따라서 귀무가설이 채택된다면 F 통계량은 1에 가까운 값일 것.
만약 대립가설이 참 (모든 독립변수와 종속변수 간 관계가 있다) 이라면, ((TSS - RSS) / p)의 기대값이 1보다 훨씬 커지게 된다. 그러면 F 통계량이 1보다 커지게 된다.
그러면 어느정도로 F 통계량이 커야 대립가설을 채택할까 -> n 과 p의 값을 확인해야한다.
n이 크면 F 통계량이 1보다 조금만 커도 귀무가설을 채택하지 않을 명분이 생기지만, n이 작으면 F 통계량이 좀 많이 커야함
오차(e)가 정규분포를 따르고, n과 p가 주어진다면, 통계 관련 프로그램에서 F 분포에 대한 p value를 구할 수 있으므로 p value에 대해서 귀무가설을 채택할지 기각할지 결정할 수 있다.

### 3.3 Other Considerations in the Regression Model

#### 3.3.1 Qualitative Predictors

특정 독립변수가 categorical 이라면 어떻게 하는가

- 독립변수가 binary

    where X = 0 or 1:

    Y = B_0 + B_i * X_i + e

    B_0 : X에 에 영향을 받지 않는 Y 값
    
    B_1 : X에 영향을 받는 Y 값, X_1의 기준에 따라 변화하는 Y값
    
    결국 X_i = 1 일때, B_0 + B_i 가 Y의 기댓값
    
        X_i = 0 일때, B_0 가 Y의 기댓값
    
    B_0 = 100, B_1 = 50일 때, X_1 = 0이면 Y = 100, X_1 = 1이면 Y = 150 **(1)**
    
    where X = -1 or 1:
    
    Y = B_0 + B_i * X_i + e
    
    B_0 : 모든 Y의 평균
    
    B_i : X_i의 차이에 대한 average difference
    
    (1)의 경우를 이렇게 처리할 경우,
    
    B_0 = 125, B_1 = 25로 예측될 것임
    
    **X를 어떻게 설정을 해도 같은 관측 데이터에 대한 Y의 예측값은 같다.**
    
    **결국 X를 어떻게 설정을 해도 결과는 변하지 않으며, 대신 계수의 해석이 달라질 뿐이다.**

- 독립변수가 3개 이상의 범주를 가지는 경우

    -> dummy 변수를 생성해야한다.

    ex) 지역이 동부, 서부, 남부, 북부로 나뉜 경우,

    X_i1은 1일 때 동부, 0일 때 동부가 아님
    
    X_i2는 1일 때 서부, 0일 때 서부가 아님

    X_i3는 1일 때 남부, 0일 때 남부가 아님

    -> 셋다 0이면 북부

    위와 같이 (각 범주의 수) - 1개의 dummy 변수를 만들어서 regression을 진행하게 된다.

    dummy 변수마다 계수를 붙여 예측하는 방식으로 regression이 진행된다.

    Y = B_0 + B_1 * X_i1 + B_2 * X_i2 + e 로 수식이 정리되며 (level = 3)

    B_0 : baseline 일 때의 Y 예측값

    B_0 + B_1 : X_i1 = 1 일 때의 Y 예측값

    B_0 + B_2 : X_i2 = 1 일 때의 Y 예측값

    (c.f baseline : dummy 변수를 이용해서 나타내지 않은 categorical 변수의 한 범주)

독립변수가 binary 할 경우와 마찬가지로 dummy 변수를 어떻게 설정하느냐에 따라서 해석이 달라질 수 있다.
그리고 p value도 변화할 수 있으므로 이러한 상황에서 통계적 유의미성을 검증하기는 어려우니 F 통계량을 사용한다.
따라서 더미 변수의 계수를 예측하는 것은 **특정한 대비**를 파악하는 것으로 생각해야한다.

#### 3.3.2 Extensions of the Linear Model

앞서 배운 모든 선형 모델들은 현실의 데이터에 적용하기에는 많은 가정을 필요로 한다. (제한적)
    가정 1. linear : 독립변수와 종속변수의 관계는 선형성이다.
    가정 2. additive :  독립변수는 서로 독립이다.
                        독립변수 외의 어떠한 다른 변수도 종속변수에 영향을 미치지 않는다.

이후 챕터에서 이러한 가정을 가지지 않는 방법을 소개하지만 여기서 간단하게라도 언급하겠다.

- 독립성 가정(가정 2)을 하지 않는 경우
1. 정량적 변수
    3.2에서 본 sales 와 advertising data 에서 TV와 radio가 서로 관계성을 가지는 것을 알 수 있다.
    (additive 위반 : 
    모델은 TV가 1 단위 오를 때, sales가 B_1 단위 오른다고 표현, 그러나 radio에 영향을 주어 B_2까지 영향을 준다.)
    (marketing 에서는 시너지 효과 / 통계학에서는 상호작용 효과)
    상호작용 효과가 발생할 경우, 선형 모델은 이런 상호작용 효과를 underestimate 한다.
    (더 작은 수치를 덜 작게, 더 큰 수치를 덜 크게)
    이런 경우, 통계학에서는 상호작용 효과까지 고려하여 아래와 같이 선형 모델을 작성한다.

    !['interaction term 1'](\..\img/image-0.png)
    let X_1 * X_2 = (interation term),
    Y = B_0 + (B_1 * X_1) + (B_2 * X_2) + (B_3 * X_1 * X_2) + e


    !['interaction term 2'](\..\img/image-1.png)
    위의 수식을 풀어서 쓰면 (X_1의 1차항을 묶으면)
    Y = B_0 + (B_1 + B_3 * X_2) * X_1 + (B_2 * X_2) + e

    이 식을
    where ~B_1 = B_1 + B_3 * X_2,
    Y = B_0 + (~B_1 * X_1) + (B_2 * X_2) + e
    (~B_2로 조건을 걸 수도 있다.)

    이 경우, 더이상 ~B_1이 상수가 아닌 X_2에 종속된 값이 되면서 상호작용 효과를 표현할 수 있게 된다.
    실제 sales 와 advertising data에 대해 interaction term을 적용한 결과 모든 계수의 p value가 0.05 이하이며,
    R^2도 89.7%에서 96.8%로 증가할 정도로 성능이 개선되었다.
    (이때 interaction term의 p value를 통해 data가 additive 가정에 맞는지 확인할 수도 있다.)
    **독립변수가 종속변수에 영향을 주지 못하는 것처럼 보이더라도, 상호작용 효과가 있을 것이라고 예상된다면 모두 모델에 포함시켜야 한다.**
    (성능의 개선이 이루어질 수 있기 때문에)

2. 정성적 변수 (categorical)
        정성적 변수에서는 정성적 변수와 상호작용 효과가 있을 것 같은 변수를 곱함으로써 dummy를 만들어 해결한다.

    !['interaction term (qualitive) 1'](\..\img/image-2.png)
    where X_2 = 0 or 1,
    Y = B_0 + B_1 * X_1 + B_2 * X_2
    (additive)

    !['interaction term (qualitive) 2'](\..\img/image-3.png)
    where X_2 = 0 or 1,
    Y = B_0 + B_1 * X_1 + B_2 * X_2 + B_3 * X_1 * X_2
    (not additive)
    (B_3 * X_1 * X_2 가 dummy 변수이면서 X_1 과 X_2의 interaction term)

    풀어쓰면,
    where X_2 = 0 or 1,
    (if X_2 = 0) Y = B_0 + B_1 * X_1
    (if X_2 = 1) Y = B_0 + B_2 + (B_1 + B_3 * X_2) * X_1

    위의 식을 보면 X_2가 0이냐 1이냐에 따라 y절편도 다르고, 기울기도 다름
    위의 식을 적용하고나서 그래프의 모양이 X_2(categorical)에 대해 바뀐다면 addictive하고 모델의 해석도 바뀔 것임

- 선형성 가정(가정 1)을 하지 않는 경우
    선형 모델을 바로 비선형 관계에 fit 시키는 방법으로 polynomial regression 이 있다. (더 복잡한 방법은 이후 설명)
    단순하게 식의 차수를 늘려 이를 선형 모델에 집어 넣는 것

    (ex) Y = B_0 + B_1 * X_1 + e => 
        Y = B_0 + B_1 * X_1 + B_2 * X_1^2 + e
        (degree : 2)
        X_1^2 를 마치 X_2로 생각해서 그대로 선형회귀를 해버린다.

    이렇게 하면 특정 degree 까지는 성능이 향상될 수 있다.
    (실제로 교재 예시에서 degree 2에서 R^2가 0.606에서 0.688)
    그러나 degree가 높아지면 높아질수록 모델이 필요 이상으로 예측을 하기도 하며 점점 구불구불해져 모델 해석력을 잃음

    비선형에 대한 모델의 확장은 chapter 7에서 마저 배우도록 하자

#### 3.3.3 Potential Problems

1. 독립변수와 종속변수의 비선형성
2. error terms의 상관관계
3. 잔차의 등분산성이 되지 않음
4. outliers
5. 
6. 