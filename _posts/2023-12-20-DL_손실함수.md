---
layout: post
title: DL 모델 학습법 IV, 손실함수
date: 2023-12-20 13:46 +0900
last_modified_at: 2023-12-20 14:40:00 +0900
tags: [DeepLearning, loss function]
toc:  true
---

# 손실함수

## 1. 손실함수

### 1.1 손실함수의 중요성

손실함수를 작아지도록 모델을 설정하므로, 손실함수를 어떤걸로 사용하느냐에 따라 모델의 학습 방향이 달라지게 된다.

### 1.2 대표적인 손실함수

- Continuous
    - MSE
    - MSA
    - Huber
    - Quantile 

- Demonstration of fitting a mooth GBM
    - original sinc(x)
    - MSE & MSA
    - Huber
    - Quantile

---
*평균 제곱 오차 (MSE)*

실제값과 모델의 추정값 사이의 차이를 제곱하여 평균낸 값 또는 함수

Quandratic Loss or L2 Loss라고도 불린다.

실제값과 추론값 사이의 오차가 제곱항으로 더해지므로, <mark>초반 학습이 빠르나, 이상치에 민감하다.</mark>

---
*평균 절대 오차 (MSA)*

실제값과 모델의 추정값 사이의 차이에 대한 절대값을 평균낸 값 또는 함수

L1 Loss라고도 불린다.

손실 함수의 값이 최소값에 가까워져도 미분값은 동일하기 때문에 점핑이 일어날 수 있으며, <mark>손실 함수의 크기를 직접 줄여주어야한다.</mark>

(최소값 근방에서 미분값이 작아야 멀리 못 가므로)

---
*Huber Loss*

L2 와 L1의 특징을 합쳐보자

오차가 일정 수준 이하일 때는 MSE, 그렇지 않을 때는 MAE를 사용해서 두 손실함수의 장점을 결합한 방법

---
*교차 엔트로피 (Cross Entropy, CE)*

주어진 확률 변수 또는 사건 집합에 대한 두 확률 분포간의 차이를 측정하는 함수

$CE = - \displaystyle \sum_{x \in X} Q(x) \cdot \log \left( P(x) \right)$

(Q는 정답 분포, P는 예측 분포)

이진 분류 문제에 적용되는 CE는 BCE(Binary CE)라고 하며, 이는 Log Loss라고도 불린다.

---
*Hinge Loss*

모델이 만드는 결정 경계(decision Boundary) 와 데이터 사이의 margin을 최대화하는 것을 목적으로 하는 손실 함수

$Hinge = max(0, 1-y \cdot \hat y)$

이진 분류 문제에 사용하게 된다.

y는 $+1(pos)$ 혹은 $-1(neg)$로 설정한다.

SVM Loss라고 한다. (Support Vector Machine)
 
## 2. 손실함수의 해석

### 2.1 Backpropagation 관점

레이어 수가 증가할수록, 활성화 함수의 미분값 (0~1)이 계속 곱해져 가중치에 따른 미분값이 0에 수렴하게 된다. (Vanishing Gradient Problem)

---
*Loss Type : MSE*

파라미터를 서로 다른 값으로 초기화 한후, epoch 수가 지난 후 멈췄더니 학습 경향이 많이 다름.

![Alt text](\..\img\DL2-12.png)

학습이 느리다는 것은 $\displaystyle \frac{\partial C}{\partial w}$와 
$\displaystyle \frac{\partial C}{\partial b}$가 작음을 의미

활성화 함수인 sigmoid가 곱해지기 때문에 작아지게 된다.

---
*Loss Type : CE*

MSE와는 달리 CE는 출력 레이어에서의 에러값에 활성화 함수의 미분값이 곱해지지 않는다.

(학습이 더 빨리된다. >> 학습을 빨리 하고 싶다면 CE 사용 권장)

히든 레이어의 노드들에서는 활성화함수의 미분값이 계속해서 곱해지므로 레이어가 여럿 사용될 경우에는 결국 Gradient Vanishing Problem에서 완전히 자유로울 수는 없다.

이런 관점이면 활성화 함수의 미분값이 0 or 1만 표현되는 ReLU는 훌륭한 선택지이다.

---
*MSE vs. CE*

![Alt text](\..\img\DL2-13.png)

### 2.2 Maximum Likelihood 관점

'모델의 출력이 y의 값이 나오는 확률을 최대화 하고 싶다'

$\Rightarrow$ y 값이 나올 확률이 가장 큰 확률 분포가 되도록 변수 예측

ex. 확률 분포는 가우시안이고, 편차는 1, 모델은 평균값을 예측한다.

Maximum Likelihood 관점에서는, 학습 데이터를 잘 설명하면 확률 분포를 찾을 수 있기 때문에, 새로운 데이터를 샘플링을 통해서 생성이 가능하다.

생성모델에서 확률 분포를 활용해서 조금 더 다양한 출력을 이루어낼 수 있다.

---
*모델 학습을 위한 역전파 이론의 가정*

$p\left(y \| f_{\theta}(x) \right)$는 독립 항등 분포 가정

1. 모든 데이터는 서로 독립이다.
2. 모든 데이터는 동일한 확률 분포를 가지고 있다.

---
*Univariate Cases*

Loss는 이렇게 표현할 수 있다.

$- \log \left( p \left( y\|f_{\theta} (x_i) \right) \right)$

위의 수식을 가우시안 분포로 가정을 하게 되면 MSE와 식이 같아진다.

위의 수식을 베르누이 분포로 가정하게 되면 결국 Cross-entropy랑 같아진다.

---
*Multivariate Cases*

위와 마찬가지로 가우시안이면 MSE, 카테고리컬 분포면 Cross-entropy와 같다.

다만 해석만 확률적으로 진행할 수 있다는 점을 알 수 있다.