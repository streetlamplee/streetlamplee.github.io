---
title: backbone 이해하기 _ ResNet
layout: post
date: 2024-01-30 17:00 +0900
last_modified_at: 2024-01-30 17:00:00 +0900
tag: [DL, CV, CNN, ResNet]
toc: true
---

# Backbone 이해하기 : CNN

## 1. ResNet

### 1.1 Revolution of Depth

2015년 이후부터 ILSVRC에서 152 layer 이상의 모델이 등장하기 시작

### 1.2 Residual Connection

단순하게 layer를 많이 늘리고, filter를 줄여서 쌓으면 좋지 않을까??<br>
$\rightarrow$ 아니다

일정 수준 이상으로는 더 깊은 레이어를 쌓아도 training error와 test error 모두 정확도가 오르지 않는 문제

이러한 문제를 residual connection을 이용해서 처리

---

**Residual Connection** : <br>
input $x$와 conv layer를 통과한 output $f(x)$를 더한 값 $x + f(x)$를 activation func에 input으로 하는 방법

![Alt text](\..\img\CV5.png)

성능이 오르지 않는 것은 Overfitting이 아니다 (training error도 떨어졌으니까)

그럼 왜 성능이 오르지 않는가 (= 학습 자체가 되지 않아서, 최적화가 되지 않아서)

가설 : *깊은 네트워크일수록 Optimization이 어렵다* (학습이 어렵다)

만약 일반적인 block에서 $x \rightarrow F(x) = H$를 하기 위해서는, 함수 F는 상당히 많은 양의 연산을 필요로 할 것이다.<br>
그러나, Residual Connection에서는, $x \rightarrow F(x) + x = H$를 수행한다.<br>
이때, $F(x)$는 **추가로 학습되어지는 정보**를 의미하게 되고, 이는 **잔차(Residual)**이라고도 한다.

따라서 직접적으로 H를 예측하는 것보다, 입력 x와 H의 잔차를 학습하는 편이 더 쉽다.<br>
먼저 잔차는 0에 수렴하는 값이라 그 값이 작기 때문이다.<br>
$\because$ layer가 너무 많으면, 어떤 layer는 있으나 마나하다.<br>
즉, input과 output이 같은 편이 오히려 학습에 더 의미가 있다.(**= 잔차가 0이어야 한다.**)

또한 이 방식의 구현 또한 추가적인 파라미터도 필요하지 않고, **shortcut Connection**만 추가되어서 복잡한 곱셈 연산도 필요하지 않다는 장점을 가지고 있다.

### 1.3 Bottleneck Layer

깊은 네트워크의 단점

Feature Map의 차원이 커지거나 유지시키면서 다양한 특징에 대한 학습이 가능함<br>
연산에 필요한 filter의 수가 많아진다.

하지만 계속해서 차원을 늘리는 것은(= channel을 늘리는 것은) 연산에 필요한 파라미터 수가 급격하게 늘어나기 때문에 학습에 어려움을 준다.

1x1 Conv를 이용해서 input의 channel을 줄인다.<br>
이를 이용해서 공간적인 특징을 추출하고 (= 2x2 이상의 filter를 사용한다.)<br>
마지막으로 다시 1x1 Conv를 이용해서 input의 Channel과 같은 값으로 증강한다.

이를 통해 연산에 필요한 파라미터의 수를 굉장히 줄일 수 있다.

### 1.4 Batch Normalization

#### 필요성

input이 정규화가 되어 있지 않을 수 있다.(Bias가 클 수 있다.)

따라서 특정 레이어에서는 매우 큰 weight를 가질 수 있게되므로, **input이 각 레이어 마다 일정하게 scaling 되어 계산**할 수 있도록 하는 장치가 필요하다.

각 차원에서 정규 분포를 따르게 하기 위해서는 아래의 식을 거쳐야한다.

$\hat{x}^{(k)} = \displaystyle \frac{x^{(k)} - E[x^{(k)}] } {\sqrt(Var[x^{(k)}]) }$

이때, 평균은 하나의 batch에 대한 각 channel마다, 표준편차도 다음과 같이 진행한다. 이때, scale과 shift에 대해 선형식으로 진행한다.

#### 역할

- deep network가 잘 학습하도록한다.
- gradient flow를 개선
- 더 빠르게 학습
- 학습 시 정규화한 것 같은 효과를 얻음
- 좀더 강건한 모델이 되도록 기여


### 1.5 Overview of ResNet

ResNet은 Batch Normalization을 모든 conv layer에 적용함

ResNet을 이후로, 더 많은 모델들이 deep network이 많이 등장했다.

또한, 학습의 효율면에서도 파라미터의 수를 줄이면서 더 빠르게 진행하게 되었다.

성능의 개선, 효율의 개선 모두 잡은 모델이다.