---
layout: post
title: DL 성능 고도화 방법 I
date: 2023-12-20 17:14 +0900
last_modified_at: 2023-12-20 17:50:00 +0900
tags: [DeepLearning]
toc:  true
---

# 성능 고도화 방법 II

## 1. 가중치 초기화

<mark>필요한 이유</mark>

1. 진행하지 않으면, 모델의 층이 깊어질수록 활성화 함수 이후 데이터의 분포가 한 쪽으로 쏠릴 수 있다.<br>
$\Rightarrow$ 이러한 현상은 효율적이고 원활한 모델 학습을 방해한다.

2. Plateau와 같은 지점에서 초기화가 된다면 모델의 학습이 비효율적으로 이뤄질 수 있다.<br>
최대한 Global minima의 근처에서 초기화가 되도록 해야한다.

3. 활성화 함수의 미분값이 가중치의 변화량에 큰 영향을 줌에 따라, 역전파로 가중치를 갱신해야하는 모델의 학습이 효율적으로 진행되지 않는다.

<mark>어떻게 초기화를 해야할까?</mark>

모델의 가중치를 단순히 **모두 0으로 초기화하거나 균등하게 초기화하면 최악의 방법**이다.

0으로 초기화하면 손실함수로부터의 기울기가 있어도, 업데이트가 안되기 때문

균등하게 초기화하면 layer의 출력 범위가 점점 커짐<br>
어떤 출력값을 보이는 노드는 학습이 되고, <br>어떤 출력값이 보이는 노드는 학습이 안되고,<br>결국 안정적인 학습을 기대하기 어려움

표준 정규분포로 초기화하면 가중치의 값이 너무 크거나 너무 작게 초기화되어 학습에 방해가 된다.

<mark>적당히 넓게 고루 분포해야한다.</mark>

---
<ins> Xavier 초기화</ins>

대표적인 가중치 초기화 전략 중 하나

sigmoid나 Tanh 같은 선형 활성화 함수에서 효과적이다.

표준편차가 $\displaystyle \frac{1}{\sqrt{n}}$인 정규 분포로 초기화

따라서 가중치의 개수(node의 개수)가 많을수록, 편차를 더 줄이면서 초기화한다.

---
<ins>HE 초기화</ins>

ReLU 활성화 함수를 사용하는 경우, 가중치를 초기화하기 위한 방법

Xavier 초기화와 같은 맥락을 가지나, He 초기화는 $\displaystyle \frac{2}{\sqrt n}$을 표준편차로 하는 정규 분포를 사용한다.

ReLU가 음수일 때 0을 출력하므로, 더 큰 가중치로 시작해서 소실성을 줄인다.

---
<ins>그래서 뭐 쓰나요?</ins>

sigmoid를 사용할 땐 Xavier,<br>
ReLU를 사용할 땐 He<br>
사실 막 서로서로 압도하는 성능을 내지는 않다보니, 다양한 실험이 필요하다.



## 2. 과적합 방지를 위한 규제화 및 학습률 조정

### 2.1 가중치 감쇠

큰 가중치에 대한 패널티를 부과해서 모델의 가중치를 작게 유지할려고 한다.

(Linear Regression에서 Ridge와 Lasso 같은 맥락인 것 같다.)

$\begin{aligned}where\;W &= (weights\; of\; model) \newline where\;\lambda &= (penalty\;parameter) \end{aligned}$<br>
$L \leftarrow L+\displaystyle \frac{\lambda}{2} \sum W^2$

$W = \| W \|$ 인경우, L1 규제화<br>
$W = W^2$인 경우, L2 규제화

### 2.2 학습 조기 종료, 학습 스케쥴러

#### <ins>학습 조기 종료</ins>

(= Early Stopping)<br>
모델 학습 시 과적합을 방지해주는 방법 중 하나

학습 오차는 줄어들지만, 검증 오차가 줄어들지 않고 <br>늘어나는 시점에서 학습을 중지하는 방법


#### <ins>학습 스케쥴러</ins>

딥러닝 훈련 과정에서 사용되는 학습률을 동적으로 조절하는 역할

1. 학습 속도를 빠르게 하고
2. 지역 최소값을 벗어나게 하고,
3. 일반적으로 더 나은 성능의 모델을 얻게 도와준다.

종류

1. Constant<br>
초기의 설정한 학습률을 학습 과정 전체에서 변경하지 않음
2. **Step Decay**<br>
일정한 주기(epoch, iter)마다 학습률을 일정 비율로 감소
3. **Exponential Decay**<br>
학습률을 지수적으로 감소
4. Cosine Annealing<br>
코사인 함수를 따라 학습률이 감소하도록 설정<br>
학습률이 안정적인 감소를 보이게 한다. (일정 주기로 재시작 가능)
5. One-Cycle Policy<br>
학습률이 먼저 증가한 다음 감소하도록 설정<br>
빠르게 수렴하게 만들고, 끝에서 안정적인 학습을 하도록