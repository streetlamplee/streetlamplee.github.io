---
title: backbone 이해하기 _ CNN
layout: post
date: 2024-01-30 15:00 +0900
last_modified_at: 2024-01-30 15:00:00 +0900
tag: [DL, CV, CNN]
toc: true
---

# Backbone 이해하기 : CNN

## 1. CNN 기본 개념 다지기

### 1.1 Convolutional Neural Network

*Convolution Layer, Activation Function, Pooling Layer로 구성된 neural network*

### 1.2 Convolution Layer

*네트워크가 vision task를 수행하는 데에 유용한 feature들을 학습할 수 있도록 함*

filter와 bias로 구성되어 있음

**filter** : (=Kernal) input image를 특정 크기의 fileter를 이용하여 탐색하면서 Convolution 연산을 시행하여 filter 영역에 대한 특징을 추출하게 된다.

![2024-01-30 16;30;03](https://github.com/streetlamplee/streetlamplee.github.io/assets/119946138/82fc0633-9540-4259-92cf-d0a93c4ea397)

2 by 2의 feature를 얻었다고 말할 수 있다.

feature를 학습한다 : filter의 숫자와 bias의 숫자를 바꿔가며 계산되는 feature 확인해서 적절한 결과를 내는 것을 뜻한다.

---

**Stride** (S) : Filter를 어느 정도의 간격으로 움직이는지를 나타냄

![Alt text](\..\img\CV2.png)

$output\;size = \displaystyle \frac{(N-F)}{S} + 1$<br>
N : input의 크기<br>
F : filter의 크기<br>
S : Stride의 크기

---

그러면 Convolution Layer를 하게 되면 출력 tensor가 줄어드니까 무한정 적용은 못하겠네<br>
$\rightarrow$ <mark>**NO**</mark>

**Padding** (P) : Feature Map의 크기를 일정 수준으로 유지하기 위해서 임의의 값을 넣은 Pixel을 얼마나 추가했는지를 나타냄

ex. zero padding : 0으로 채운 pixel을 주변에 채워 넣는 것

$output\;size = \displaystyle \frac{(N-F+2P)}{S} + 1$<br>
N : input의 크기<br>
F : filter의 크기<br>
S : Stride의 크기<br>
P : Padding의 크기

### 1.3 Activation Function

*네트워크에 비선형성을 가해주는 역할을 함*

복잡한 문제를 풀 때에는 비선형 모델이 더 좋기 때문

Convolution 연산으로는 각 pixel에 대한 가중치의 합을 구하므로, **선형적인 모델만 만들 수 있음**

따라서 Activation을 이용해서 비선형성을 추가

#### 종류

- sigmoid
- tanh
- ReLU
- Leaky ReLU
- Maxout
- ELU

### 1.4 Pooling Layer

*feature map에 Spatial aggregation을 시켜줌*

입력의 크기를 줄일 수 있다. (= Down Sampling)

Feature Map이 줄어들 때의 장점
- 모델의 파라미터 수가 줄어들게 된다.
- Receptive Field가 달라지게 된다.