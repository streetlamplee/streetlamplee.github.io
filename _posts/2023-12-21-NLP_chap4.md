---
layout: post
title: NLP 4. 워드 임베딩 만들기
date: 2023-12-14 21:00 +0900
last_modified_at: 2023-12-14 21:00:00 +0900
tags: [NLP]
toc:  true
---

## 워드 임베딩이란?

### 워드 임베딩

단어를 컴퓨터가 이해하고 효율적으로 처리할 수 있도록 단어를 벡터화

이때, 밀집 벡터(dense vector)의 형태로 표현한다.

*임베딩 벡터* : 워드 임베딩 과정을 통해 나온 결과

### 인코딩

데이터를 기계가 이해할 수 있도록 숫자 등으로 변환해주는 작업이 필요하다.

자연어를 수치화된 벡터로 변환하는 작업

### 희소 표현(sparse representation)

ex. one-hot encoding

벡터 또는 행렬의 값이 대부분 0으로 표현되는 방법을 희소표현이라고 한다.

*희소 벡터* : one-hot encoding에 의해 만들어지는 벡터

<ins>희소 표현의 문제점</ins>

1. 희소문제<br>
단어의 개수가 늘어나면 벡터의 차원이 무한히 커지게 된다.<br>
단어사전이 커질수록 one-hot encoding 기반의 임베딩을 저장하기<br>
위한 공간이 계속늘어나서 컴퓨터의 성능이 저하될 것이다.

2. 단어의 의미를 표현하지 못함<br>
벡터의 표현방식이 너무 단순하다.<br>
단어의 중의성이나 모호성을 표현하기에 충분하지 않다.

### 밀집표현

희소표현의 반대 표현

더이상 벡터의 차원을 단어 집합의 크기로 설정하지 않으며, 사용자가 설정한 값으로 모든 단어의 벡터 표현의 차원을 맞추는 표현방식

인코딩 과정에서 연속적인 실수 값을 가질 수 있다.

<ins>장점</ins>

1. 밀집 표현은 적은 차원으로 대상을 표현할 수 있다.
2. 밀집 표현은 더 큰 일반화 능력을 가지고 있다.<br>
학습되지 않은 단어에 대해서도 비슷한 벡터값을 가지게 된다면, 비슷한 단어로 생각하고 학습, 추론을 진행할 수 있다.

### 차원축소

희소벡터를 밀집 벡터의 형태로 변환하는 방법

#### 잠재 의미 분석

기존 행렬에 특이값 분해를 사용하여 행렬에 속한 벡터의 차원을 축소

## Word2Vec

### 분산 표현

분포 가설이라는 가정 하에 만들어진 표현방법

<mark>비슷한 문맥에서 등장하는 단어들은 비슷한 의미를 가진다</mark>는 가설

벡터의 차원이 상대적으로 저차원으로 줄어듦

### Word2Vec

분포가설 하에 표현한 분산표현을 따르는 워드 임베딩 모델

중심 단어와 주변의 단어들을 사용하여 단어를 예측하는 방식으로 임베딩을 만들 수 있음

1. Continuous Bag of Words (CBOW)<br>
주변에 있는 단어들을 보고 중간에 있는 단어를 예측하는 방법<br>
주변 단어(context)로 target word를 예측하는 문제를 풀게 됨<br>
주변단어는 target 직전 n개의 단어 + 직후 n개의 단어를 의미하며, 이를 window라고 부르고, n을 window size라고 함<br><br>
문장 하나에 대해서, sliding window 방식을 사용해서 여러 개의 학습 데이터 set을 만들게 된다.
2. Skip_gram<br>
중심 단어에서 주변 단어를 예측하는 방법<br>
마찬가지로 중심 단어를 sliding window하면서 학습 데이터를 증강한다.

#### <ins>CBOW vs. Skip-gram</ins>

Skip-gram이 CBOW에 비해 여러 문맥을 고려하기 때문에 일반적으로 성능이 더 좋다.

#### <ins>한계점</ins>

1. 단어의 형태학적 특성을 반영하지 못함<br>
ex. teach, teacher, teachers를 서로 다른 단어로 인식한다.
2. 단어 빈도수의 영향을 많이 받아 희소 단어를 임베딩하기 어렵다.
3. OOV의 처리가 어렵다.
4. 단어 사전의 크기가 클수록 학습하는데 오래 걸린다.

#### 학습 트릭

1. Subsampling Frequent Words<br>
자연어 코퍼스에서 자주 등장하는 단어의 학습량을 확률적인 방식으로 줄이는 것<br>
많이 등장하는 단어의 학습량을 줄임으로서, 희소단어의 학습률을 높인다.
2. Negative Sampling<br>
Word2Vec은 역전파 과정에서 모든 단어의 임베딩 벡터값을 업데이트<br>
근데 중심 단어와 context 외에 다른 단어의 임베딩까지 업데이트는 비효율적<br>
따라서 무작위 Negative sample을 뽑아 한개의 중심 단어에 대해 <mark>주변단어 + 네거티브 샘플</mark>로만 구성된 단어 집합을 만들어서 학습하면 효율적으로 학습할 수 있다.

## Fast Text

분포가설 하에 표현한 분산표현을 따르는 또 다른 임베딩 모델

위의 한계점을 해결하기 위해서, FastText는 각 단어을 Bag of Characters로 보고, 개별 단어가 아닌 n-gram의 연속된 걸자들을 임베딩한다.

![Alt text](\..\img\NLP3-1.png)

단어를 먼저 <, >로 감싼 후, 설정한 n-gram의 값에 따라 단어를 앞에서 부터 쪼갬

마지막에, 전체 단어 하나 추가

이때, n-gram은 최소, 최대 값을 설정할 수 있음

target word의 벡터 t는 분리된 모든 토큰들의 벡터 합으로 표현 됨

$v_t = z_{t_1} + z_{t_2} + \cdots + z_{t_n}$

#### <ins>학습방법</ins>

Skip-gram과 같은 방법을 사용한다.

#### 장점

1. 오타나 OOV에 대한 강건한 대응<br>
서브워드 토큰화와 마찬가지로 각 n-gram에 대한 워드 임베딩 구축이 가능<br>
분절된 서브워드들을 통해 모르는 단어,오타,미등록 단어의 유사도를 파악할 수 있다.
2. 희소 단어에 대한 강건한 대응<br>
단어의 출현 빈도가 낮더라도, 그 단어의 n-gram이 다른 단어의 n-gram과 겹치는 경우가 있어 비교적 높은 정확도의 임베딩 벡터값을 얻을 수 있음
3. 자연어 코퍼스 내 노이즈에 강건함<br>
*노이즈* : 오타, 맞춤법이 틀린단어<br>
이러한 노이즈는 일종의 희소 단어로 볼 수 있고, 장점 2와 마찬가지로 일정 수준의 성능을 확보할 수 있다.

#### 한국어에서의 FastText

한국어는 자소 단위로 분해 가능, 자소 각각을 문자로 보고 FastText를 사용할 수 있다.

ex.<br>
![Alt text](\..\img\NLP3-2.png)

## 워드 임베딩 만들기

잠재 의미 분석(LSA) : 단어 의미 유추 작업에서의 성능이 낮음

Word2Vec : 코퍼스 전체의 통계 정보가 반영되기 어려움

### GloVe

Global Vectors for Word Representation

단어 빈도와 예측 기반의 방법론을 모두 활용하는 방법론

두 단어 백터의 내적 = 코퍼스 전체에서의 동시 등장 확률의 로그 값<br>
$\Rightarrow$ <ins>임베딩된 단어 벡터들 간 유사도 측정을 수월</ins>하게 하고, <ins>코퍼스 전체의 통계 정보를 더 잘 반영</ins>해볼 수 있음

#### 작동 원리

1. 윈도우 기반 동시 등장 행렬<br>
행과 열을 전체 단어 집합의 단어들로 구성<br>
i번째 단어의 윈도의 크기 내에서, k 단어가 등장한 횟수를<br>
i 행 k 열에 기재한 행렬<br>
![Alt text](\..\img\NLP3-3.png)<br>
2. 동시 등장 확률<br>
동시 등장 확률 $P(k\|i)$는 동시 등장 행렬로부터 특정 단어 i의 전체 등장 횟수를 카운트 하고, 특정 단어 i가 등장했을 때, 어떤 단어 k가 등장한 횟수를 count한 조건부 확률<br><br>
$=\displaystyle \frac{(i\; 행\; k 열의\; 값)}{(중심단어\; i의\; 행의\; 모든\; 값을\; 더한\; 값)}$<br>
3. 손실 함수<br>
특정 단어 k가 주어졌을 때, 두 단어 백터 간의 내적 값이 두 단어의 동시 등장 확률과 같은 비율이 되도록 임베딩 설계<br><br>
일반화된 손실함수 $= \displaystyle \sum_{m,n=1}^V \left( w_m^T \bar{w_n} + b_m + \bar{b_m} + - \log{X_{mn}} \right)^2$<br><br>
근데 위의 함수가 $\log{X_{mn}}$에서 $X_{mn}$이 0일 확률이 높아서 가중치를 추가로 도입<br><br>
$\begin{aligned} Loss\;Function&=\displaystyle\sum_{m,n=1}^V f\left( X_{mn} \right) \left(w_m^T \bar{w_n} + b_m + \bar{b_m} + - \log{X_{mn}} \right)^2 \newline f(x) &= min(1, \left( \frac{x}{x_{max}} \right)^{\frac{3}{4}} \end{aligned}$

#### 시사점

근데 논문을 통해 Skip-gram 모델이 코퍼스 전체의 global 통계량을 반영하는 것으로 밝혀짐<br>
그럼에도 불구하고 Word2Vec의 단점을 극복하려했다는 시도 자체와 matrix factorization을 활용하려했다는 점에서 주목<br>
그러나 여전히 martrix factorization의 계산 복잡성이 크다는 점에서 한계가 있음