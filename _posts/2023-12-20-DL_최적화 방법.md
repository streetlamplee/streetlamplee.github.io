---
layout: post
title: DL 다양한 최적화 방법
date: 2023-12-20 17:14 +0900
last_modified_at: 2023-12-20 19:09:00 +0900
tags: [DeepLearning, optimizer]
toc:  true
---

# 다양한 최적화 방법

## 1. 다양한 최적화 방법

1. 옵티마이저의 스텝 **방향**을 변경
2. 옵티마이저의 스텝 **사이즈**를 변경
3. 두 장점을 모두 반영
---
### 1.1 기본적인 옵티마이저
---
#### 경사하강법

$W \leftarrow W - \eta \displaystyle \frac{\partial L}{\partial W}$

---
#### SGD (확률적 경사하강법)

GD와 같으나, 확률적으로 전체 데이터 셋이 아닌 <br>무작위로 선택된 하나의 sample을 이용해서 가중치를 갱신

---
#### mini-batch GD

GD와 SGD의 절충안<br>
미니 배치라는 작은 단위의 데이터 셋을 활용해서 기울기 계산 > 가중치 갱신

DL에서의 SGD는 이 방법을 보통 뜻한다.

단점 : 지그재그 형태의 비효율적인 경로로 도착하게 된다.

---
### 1.2 스텝 방향을 개선한 옵티마이저
---
#### Momentum

SGD + 관성효과

$\begin{aligned} \upsilon & \leftarrow \alpha \upsilon - \eta \displaystyle \frac{\partial L}{\partial W} \newline W & \leftarrow W + \upsilon \end{aligned}$

이전 기울기를 고려해서 $\alpha$로 scaling 하고, 이를 추가해서 반영

최적점에 더 빠르게 도달하고, local minima를 잘 피하게 해준다.

---
#### Nestrov Accelerated Gradient (NAG)

momentum의 변형, 관성 효과를 조금 더 똑똑하게 적용한다.

$\begin{aligned} \upsilon &\leftarrow \alpha \upsilon - \eta \displaystyle \frac{\partial L}{\partial W} \left( W + \alpha \upsilon \right) \newline W &\leftarrow W + \upsilon \end{aligned}$

momentum은 현재 위치의 미분값을 계산해서 처리하는것이지만,<br>
NAG는 관성효과를 적용한 위치에서의 미분값을 이용하게 된다.

---
### 1.3 스텝 사이즈를 개선한 옵티마이저
---
#### AdaGrad

학습률을 자동을 조정하면서, 가중치를 갱신하는 방법이다.

갱신 방향 + 갱신 크기를 모두 집중한다.

$\begin{aligned} h &\leftarrow h + \frac{\partial L}{\partial W} \odot \frac{\partial L}{\partial W} \newline W &\leftarrow W - \eta \frac{1}{\sqrt h} \frac{\partial L}{\partial W} \end{aligned}$

$\displaystyle \frac{\partial L}{\partial W} \odot \frac{\partial L}{\partial W}$ : 가중치별 업데이트 된 크기<br>
$\eta$ : 학습률<br>
$\displaystyle \frac{1}{\sqrt h}$ : 업데이트가 많이 된 가중치 값일 수록 적게 업데이트 되게끔


빈번하게 발생하는 특성에 대한 학습률을 낮추고,드물게 발생하는 특성에 대한<br> 학습률을 높인다.

이러한 특징으로 인해 적응력이 있다(**Ada**ptive)고 한다.

---
#### RMSProp

AdaGrad는 과거의 기울기를 계속 반영해서 더하기 때문에 학습이 진행될수록 업데이트가 많이 되지 않는다.

RMSProp은 이러한 단점을 해결<br>
과거의 모든 기울기 정보를 고려하지 않고, 최근 기울기 위주로 업데이트를 진행<br>
이때, 지수 이동 평균 (EMA) 방식을 이용한다.

$\begin{aligned} h &\leftarrow \beta h + (1 - \beta) \left( \displaystyle \frac{\partial L}{\partial W} \odot \frac{\partial L}{\partial W} \right)\newline W &\leftarrow W - \eta \frac{1}{\sqrt {h+\epsilon}} \odot \frac{\partial L}{\partial W}\end{aligned}$

$\beta$ : 감쇠율 (Decay rate)<br><br>
$\left( \displaystyle \frac{\partial L}{\partial W} \odot \frac{\partial L}{\partial W} \right)$ : 가중치 별 업데이트 된 크기<br><br>
$\eta$ : 학습률 <br><br>
$\displaystyle \frac{1}{\sqrt {h+\epsilon}}$ : AdaGrad와 마찬가지로 업데이트가 많이 될수록, 적게 업데이트

AdaGrad보다 훨씬 빠르고 효율적으로 학습한다.

---
### 1.4 스텝 방향과 사이즈 모두 개선한 옵티마이저
---
#### Adam(Adaptive Moment Estimation)

Momentum의 장점 *(스텝 방향)*과 RMSProp의 장점 *(스텝 사이즈)*를 모두 결합한 방식

Adam은 기울기의 1차 moment(평균)과 2차 moment(분산)을 추정하여 학습률을 조정한다.

일반적으로 성능이 제일 우수해서 초기 옵티마이저 세팅으로 추천

$\begin{aligned} m &\leftarrow \beta_1 m + (1 - \beta_1) \frac{\partial L}{\partial W}  \newline \upsilon &\leftarrow \beta_2 \upsilon + (1-\beta_2)(\frac{\partial L}{\partial W})^2 \newline W &\leftarrow W-\eta \frac{m}{\sqrt \upsilon + \epsilon} \end{aligned}$

1. 1차 moment : 이전 속도를 어느 정도 유지하면서 학습
2. 2차 moment : 최근에 업데이트가 많았다면 업데이트량을 적게 만든다.
---

### 1.5 요약 및 정리

![Alt text](\..\img\DL3-2.png)

데이터셋이나 특정 문제의 복잡성에 따라 최적 옵티마이저가 다르다

원리와 특징을 이해하고, 적용해서 최적의 옵티마이저를 찾아야한다.

아래는 이 post에 소개된 옵티마이저들을 한눈에 볼 수 있는 graph이다.

![Alt text](\..\img\DL3-3.png)