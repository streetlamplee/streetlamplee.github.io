---
layout: post
title: DL 모델 학습법 III, 역전파 (기초)
date: 2023-12-20 10:52 +0900
last_modified_at: 2023-12-20 11:49:00 +0900
tags: [DeepLearning, backpropagation]
toc:  true
---
# 역전파 (기초)

최적화 알고리즘에서 손실 함수의 미분값을 어떻게 모델을 업데이트하는지 알 수 있다.

## 1. 역전파의 등장

*손실함수의 기울기* : 해당 파라미터의 잘못된 예측에 얼마나 기여했는지 나타낼 수 있다.

그러나 모든 파라미터에 대한 편미분을 구해 그래디언트를 구하는 것은 모델의 크기가 커질수록 불가능하다.

ex. GPT-3 : parameter가 약 100 Billion개 인데 이걸 다 편미분?

이를 해결하기 위해 등장한 개념이 **역전파**

---
*손실함수의 기울기 구하기*

선형식일 경우, 크게 어렵지 않고, 손실함수는 간단한 이차함수 미분이 된다.

그러나 딥러닝 모델일 경우, 이를 일일이 수식으로 전개하는 것은 매우 복잡한 일이 된다.

## 2. 계산 그래프

일련의 연산과정을 하나의 방향 그래프로 나타낸 것

node 와 edge로 구성됨

![Alt text](\..\img\DL2-5.png)

이를 이용하면, **편미분 계산**이 가능하며, **중간 결과를 보관하는 것이 가능**하다.

전체 편미분에 필요한 연산들을 작은 단위로 쪼개고, 각각 편미분을 적용해서 합쳐서 최종 결과 도출

---
*example*

1000원 짜리 과자 3개 샀다. 지불액은? (VAT 10%)

![Alt text](\..\img\DL2-6.png)

## 3. 연쇄 법칙

둘 이상의 연산이 수행된 합성 함수를 미분하기 위해 사용된다.

합성함수의 미분은 <mark>합성 함수를 구성하는 각 함수의 미분의 곱으로 나타낼 수 있다.</mark>

$\displaystyle \frac{df(g(x))}{dx} = \displaystyle \frac{df(g(x))}{dg(x)} \times \displaystyle \frac{dg(x)}{dx}$

---
*연쇄 법칙과 뉴럴넷의 연관성*

4개의 노드로 구성된 간단한 뉴럴넷 구성

![Alt text](\..\img\DL2-7.png)

![Alt text](\..\img\DL2-8.png)

## 4. 역전파의 이해

신경망의 추론 방향과는 반대되는 방향으로, 순차적으로 오차에 의한 편미분을 수행하여

각 레이어의 파라미터를 업데이트하는 과정

![Alt text](\..\img\DL2-9.png)

연쇄 법칙을 이용하여 그래프의 각 노드에 대한 편미분 ($\frac{\partial y}{\partial x}$)을 기존 신호 ($E$)에 곱하여 이전 레이어의 노드에 전달한다.

- 곱셈 노드 : 다른 edge의 값을 곱해서 넘어간다.
- 덧셈 노드 : 그대로 넘어간다.

---
*example. 주어진 함수의 역전파 계산*

각 parameter에 대해서, 함수($L$)에 대한 편미분 값을 작성한다. (연쇄 효과 활용)

함수 : $f(x,y,z) = (x +y)z$

데이터 : $x=-2,y=5,z=-4,target=-10$

손실함수 : $L=(target-f)^2$

![Alt text](\..\img\DL2-10.png)

이후 파라미터의 업데이트에서는,

<mark>각 파라미터에 대해 편미분한 값에 학습률을 곱한다음, 원래 파라미터에서 빼서 모델을 업데이트한다.</mark>

$where\;lr=0.01$

$x \leftarrow lr \times \partial L / \partial x$

>$x = (-2) - 0.01 \times 16 =-1.84$

$y \leftarrow lr \times \partial L / \partial y$

>$y = 5 - 0.01 \times 16 = 4.84$

$z \leftarrow lr \times \partial L / \partial z$

>$z = (-4) - 0.01 \times (-12) = -3.88$