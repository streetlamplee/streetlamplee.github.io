---
layout: post
title: Pytorch_í…ì„œ ì‹¤ìŠµ 2
date: 2023-12-26 17:00 +0900
last_modified_at: 2023-12-26 17:00:00 +0900
tags: [deeplearning, Pytorch, tensor]
toc:  true
---

# í…ì„œ ì¡°ì‘ (2)

### í™˜ê²½ ì„¤ì •
> PyTorch ì„¤ì¹˜ ë° ë¶ˆëŸ¬ì˜¤ê¸°

<font color = blue><b>
- íŒ¨í‚¤ì§€ ì„¤ì¹˜ ë° ì„í¬íŠ¸
</font><b>


```python
!pip install torch==2.0.1 # PyTorch ë¥¼ ê°€ì¥ ìµœê·¼ ë²„ì „ìœ¼ë¡œ ì„¤ì¹˜
```

    Requirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)
    Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.12.2)
    Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (4.7.1)
    Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (1.11.1)
    Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.1)
    Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.1.2)
    Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (2.0.0)
    Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1) (3.25.2)
    Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1) (16.0.6)
    Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1) (2.1.3)
    Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1) (1.3.0)
    


```python
import torch # PyTorch ë¶ˆëŸ¬ì˜¤ê¸°
import numpy as np # numpy ë¶ˆëŸ¬ì˜¤ê¸°
import warnings # ê²½ê³  ë¬¸êµ¬ ì œê±°
warnings.filterwarnings('ignore')
```

## 1. í…ì„œ ì—°ì‚° ë° ì¡°ì‘

- 1-1. í…ì„œ ê°„ì˜ ê³„ì‚° ì‹¤ìŠµ
- 1-2. Broadcasting ì„ ì´ìš©í•œ í…ì„œ ê°’ ë³€ê²½
- 1-3. Broadcasting ì„ ì´ìš©í•œ ì°¨ì›ì´ ë‹¤ë¥¸ í…ì„œ ê°„ì˜ ê³„ì‚° ì‹¤ìŠµ


### 1-1 í…ì„œ ê°„ì˜ ê³„ì‚° ì‹¤ìŠµ
> í…ì„œ ê°„ì˜ ê³„ì‚°ê³¼ í…ì„œ ë‚´ì˜ ê³„ì‚° ê³¼ì •ì„ ì•Œì•„ë´…ë‹ˆë‹¤.



#### ğŸ“ ì„¤ëª… : í…ì„œ ê°„ì˜ ì‚¬ì¹™ì—°ì‚°
* add : í…ì„œ ê°„ì˜ ë§ì…ˆì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. (+)
    * torch.add(a, b)
    * a.add(b)
    * a + b
* sub : í…ì„œ ê°„ì˜ ëº„ì…ˆì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. (-)
    * torch.sub(a, b)
    * a.sub(b)
    * a - b
* mul : í…ì„œ ê°„ì˜ ê³±ì…ˆì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. (*)
    * torch.mul(a, b)
    * a.mul(b)
    * a * b
* div : í…ì„œ ê°„ì˜ ë‚˜ëˆ—ì…ˆì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. (/)
    * torch.div(a, b)
    * a.div(b)
    * a / b

ğŸ“š ì°¸ê³ í• ë§Œí•œ ìë£Œ:
* [add] https://pytorch.org/docs/stable/generated/torch.add.html
* [sub] https://pytorch.org/docs/stable/generated/torch.add.html
* [mul] https://pytorch.org/docs/stable/generated/torch.add.html
* [div] https://pytorch.org/docs/stable/generated/torch.add.html



```python
tensor_a = torch.tensor([[1, -1], [2, 3]])
tensor_b = torch.tensor([[2, -2] ,[3, 1]])

print('ë§ì…ˆ')
print("a+b : \n", tensor_a + tensor_b)
print('\n')
print("torch.add(a,b) : \n", torch.add(tensor_a, tensor_b))

print('---'*10)

print('ëº„ì…ˆ')
print("a-b : \n", tensor_a - tensor_b)
print('\n')
print("torch.sub(a,b) : \n", torch.sub(tensor_a, tensor_b))

print('---'*10)

print('ê³±ì…ˆ')
print("a*b : \n", tensor_a * tensor_b)
print('\n')
print("torch.mul(a,b) : \n", torch.mul(tensor_a, tensor_b))

print('---'*10)

print('ë‚˜ëˆ—ì…ˆ')
print("a/b : \n", tensor_a / tensor_b)
print('\n')
print("torch.div(a,b) : \n", torch.div(tensor_a, tensor_b))
```

    ë§ì…ˆ
    a+b : 
     tensor([[ 3, -3],
            [ 5,  4]])
    
    
    torch.add(a,b) : 
     tensor([[ 3, -3],
            [ 5,  4]])
    ------------------------------
    ëº„ì…ˆ
    a-b : 
     tensor([[-1,  1],
            [-1,  2]])
    
    
    torch.sub(a,b) : 
     tensor([[-1,  1],
            [-1,  2]])
    ------------------------------
    ê³±ì…ˆ
    a*b : 
     tensor([[2, 2],
            [6, 3]])
    
    
    torch.mul(a,b) : 
     tensor([[2, 2],
            [6, 3]])
    ------------------------------
    ë‚˜ëˆ—ì…ˆ
    a/b : 
     tensor([[0.5000, 0.5000],
            [0.6667, 3.0000]])
    
    
    torch.div(a,b) : 
     tensor([[0.5000, 0.5000],
            [0.6667, 3.0000]])
    

#### ğŸ“ ì„¤ëª… : í…ì„œì˜ í†µê³„ì¹˜
í•¨ìˆ˜ì˜ dim íŒŒë¼ë¯¸í„° ê°’ì— ë”°ë¼ ê²°ê³¼ê°€ ë‹¬ë¼ì§€ëŠ” ê²ƒì„ ìœ ì˜í•˜ì„¸ìš”â—
* sum : í…ì„œì˜ ì›ì†Œë“¤ì˜ í•©ì„ ë°˜í™˜

ğŸ“š ì°¸ê³ í• ë§Œí•œ ìë£Œ:
* [sum] : https://pytorch.org/docs/stable/generated/torch.sum.html


```python
tensor_a = torch.tensor([[1, 2], [3, 4]])
print(tensor_a)
print("Shape : ", tensor_a.size())

print('\n')

print("dimension ì§€ì • ì•ˆí–ˆì„ ë•Œ : ", torch.sum(tensor_a))  # ëª¨ë“  ì›ì†Œì˜ í•©ì„ ë°˜í™˜ í•¨
print("dim = 0 ì¼ ë•Œ : ", torch.sum(tensor_a, dim=0))  # í–‰ì„ ê¸°ì¤€ (í–‰ ì¸ë±ìŠ¤ ë³€í™”)ìœ¼ë¡œ í•©í•¨ (0í–‰ 0ì—´ + 1í–‰ 0ì—´, 0í–‰ 1ì—´ + 1í–‰ 1ì—´)
print("dim = 1 ì¼ ë•Œ : ", torch.sum(tensor_a, dim=1)) # ì—´ì„ ê¸°ì¤€ (ì—´ ì¸ë±ìŠ¤ ë³€í™”)ìœ¼ë¡œ í•©í•¨ (0í–‰ 0ì—´ + 0í–‰ 1ì—´, 1í–‰ 0ì—´ + 1í–‰ 1ì—´)
```

    tensor([[1, 2],
            [3, 4]])
    Shape :  torch.Size([2, 2])
    
    
    dimension ì§€ì • ì•ˆí–ˆì„ ë•Œ :  tensor(10)
    dim = 0 ì¼ ë•Œ :  tensor([4, 6])
    dim = 1 ì¼ ë•Œ :  tensor([3, 7])
    

#### ğŸ“ ì„¤ëª… : í…ì„œì˜ í†µê³„ì¹˜
í•¨ìˆ˜ì˜ dim íŒŒë¼ë¯¸í„° ê°’ì— ë”°ë¼ ê²°ê³¼ê°€ ë‹¬ë¼ì§€ëŠ” ê²ƒì„ ìœ ì˜í•˜ì„¸ìš”â—
* mean : í…ì„œì˜ ì›ì†Œë“¤ì˜ í‰ê· ì„ ë°˜í™˜
ğŸ“š ì°¸ê³ í• ë§Œí•œ ìë£Œ:
* [mean] : https://pytorch.org/docs/stable/generated/torch.mean.html


```python
tensor_a = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32) # mean ì€ ì‹¤ìˆ˜ê°€ ë‚˜ì˜¬ ìˆ˜ ìˆìœ¼ë¯€ë¡œ float ë¡œ ì§€ì •í•´ì£¼ì–´ì•¼ í•¨.
print(tensor_a)
print("Shape : ", tensor_a.size())

print('\n')

print("dimension ì§€ì • ì•ˆí–ˆì„ ë•Œ : ", torch.mean(tensor_a))  # ëª¨ë“  ì›ì†Œì˜ í‰ê· ì„ ë°˜í™˜ í•¨
print("dim = 0 ì¼ ë•Œ : ", torch.mean(tensor_a, dim=0))  # í–‰ì„ ê¸°ì¤€ (í–‰ ì¸ë±ìŠ¤ ë³€í™”)ìœ¼ë¡œ í‰ê·  êµ¬í•¨ ((0í–‰ 0ì—´ + 1í–‰ 0ì—´)/2, (0í–‰ 1ì—´ + 1í–‰ 1ì—´)/2)
print("dim = 1 ì¼ ë•Œ : ", torch.mean(tensor_a, dim=1)) # ì—´ì„ ê¸°ì¤€ (ì—´ ì¸ë±ìŠ¤ ë³€í™”)ìœ¼ë¡œ í‰ê·  êµ¬í•¨ ((0í–‰ 0ì—´ + 0í–‰ 1ì—´)/2, (1í–‰ 0ì—´ + 1í–‰ 1ì—´)/2)
```

    tensor([[1., 2.],
            [3., 4.]])
    Shape :  torch.Size([2, 2])
    
    
    dimension ì§€ì • ì•ˆí–ˆì„ ë•Œ :  tensor(2.5000)
    dim = 0 ì¼ ë•Œ :  tensor([2., 3.])
    dim = 1 ì¼ ë•Œ :  tensor([1.5000, 3.5000])
    

#### ğŸ“ ì„¤ëª… : í…ì„œì˜ í†µê³„ì¹˜
í•¨ìˆ˜ì˜ dim íŒŒë¼ë¯¸í„° ê°’ì— ë”°ë¼ ê²°ê³¼ê°€ ë‹¬ë¼ì§€ëŠ” ê²ƒì„ ìœ ì˜í•˜ì„¸ìš”â—
* max : í…ì„œì˜ ì›ì†Œë“¤ì˜ ê°€ì¥ í° ê°’ì„ ë°˜í™˜
* min : í…ì„œì˜ ì›ì†Œë“¤ì˜ ê°€ì¥ ì‘ì€ ê°’ì„ ë°˜í™˜

ğŸ“š ì°¸ê³ í• ë§Œí•œ ìë£Œ:
* [max] : https://pytorch.org/docs/stable/generated/torch.max.html
* [min] : https://pytorch.org/docs/stable/generated/torch.min.html


```python
import torch
tensor_a = torch.tensor([[1, 2], [3, 4]])
print(tensor_a)
print("Shape : ", tensor_a.size())
print('\n')

print("dimension ì§€ì • ì•ˆí–ˆì„ ë•Œ : ", torch.max(tensor_a))  # ëª¨ë“  ì›ì†Œ ì¤‘ ìµœëŒ“ê°’ ë°˜í™˜
print("dim = 0 ì¼ ë•Œ : ", torch.max(tensor_a, dim=0).values)  # í–‰ì„ ê¸°ì¤€ (í–‰ ì¸ë±ìŠ¤ ë³€í™”)ìœ¼ë¡œ max ë¹„êµ (max(0í–‰ 0ì—´ , 1í–‰ 0ì—´), max(0í–‰ 1ì—´ , 1í–‰ 1ì—´))
print("dim = 1 ì¼ ë•Œ : ", torch.max(tensor_a, dim=1).values) # ì—´ì„ ê¸°ì¤€ (ì—´ ì¸ë±ìŠ¤ ë³€í™”)ìœ¼ë¡œ max ë¹„êµ (max(0í–‰ 0ì—´ , 0í–‰ 1ì—´), max(1í–‰ 0ì—´ , 1í–‰ 1ì—´))
print('\n')

print("dimension ì§€ì • ì•ˆí–ˆì„ ë•Œ : ", torch.min(tensor_a))  # ëª¨ë“  ì›ì†Œì˜ ìµœì†Ÿê°’ ë°˜í™˜ í•¨
print("dim = 0 ì¼ ë•Œ : ", torch.min(tensor_a, dim=0).values)  # í–‰ì„ ê¸°ì¤€ (í–‰ ì¸ë±ìŠ¤ ë³€í™”)ìœ¼ë¡œ min ë¹„êµ (min(0í–‰ 0ì—´ , 1í–‰ 0ì—´), min(0í–‰ 1ì—´ , 1í–‰ 1ì—´))
print("dim = 1 ì¼ ë•Œ : ", torch.min(tensor_a, dim=1).values) # ì—´ì„ ê¸°ì¤€ (ì—´ ì¸ë±ìŠ¤ ë³€í™”)ìœ¼ë¡œ min ë¹„êµ (min(0í–‰ 0ì—´ , 0í–‰ 1ì—´), min(1í–‰ 0ì—´ , 1í–‰ 1ì—´))
```

    tensor([[1, 2],
            [3, 4]])
    Shape :  torch.Size([2, 2])
    
    
    dimension ì§€ì • ì•ˆí–ˆì„ ë•Œ :  tensor(4)
    dim = 0 ì¼ ë•Œ :  tensor([3, 4])
    dim = 1 ì¼ ë•Œ :  tensor([2, 4])
    
    
    dimension ì§€ì • ì•ˆí–ˆì„ ë•Œ :  tensor(1)
    dim = 0 ì¼ ë•Œ :  tensor([1, 2])
    dim = 1 ì¼ ë•Œ :  tensor([1, 3])
    

#### ğŸ“ ì„¤ëª… : í…ì„œì˜ í†µê³„ì¹˜
í•¨ìˆ˜ì˜ dim íŒŒë¼ë¯¸í„° ê°’ì— ë”°ë¼ ê²°ê³¼ê°€ ë‹¬ë¼ì§€ëŠ” ê²ƒì„ ìœ ì˜í•˜ì„¸ìš”â—
* argmax : í…ì„œì˜ ì›ì†Œë“¤ì˜ ê°€ì¥ í° ê°’ì˜ **ìœ„ì¹˜** ë°˜í™˜
* argmin : í…ì„œì˜ ì›ì†Œë“¤ì˜ ê°€ì¥ ì‘ì€ ê°’ì˜ **ìœ„ì¹˜** ë°˜í™˜

ğŸ“š ì°¸ê³ í• ë§Œí•œ ìë£Œ:
* [argmax] : https://pytorch.org/docs/stable/generated/torch.argmax.html
* [argmin] : https://pytorch.org/docs/stable/generated/torch.argmin.html


```python
tensor_a = torch.tensor([[1, 2], [3, 4]])
print(tensor_a)
print("Shape : ",tensor_a.size())
print('\n')

print("dimension ì§€ì • ì•ˆí–ˆì„ ë•Œ : ", torch.argmax(tensor_a))  # ëª¨ë“  ì›ì†Œ ì¤‘ ìµœëŒ“ê°’ ìœ„ì¹˜ ë°˜í™˜í•¨
print("dim = 0 ì¼ ë•Œ : ", torch.argmax(tensor_a, dim=0))  # í–‰ì„ ê¸°ì¤€ (í–‰ ì¸ë±ìŠ¤ ë³€í™”)ìœ¼ë¡œ max ë¹„êµ (max(0í–‰ 0ì—´ , 1í–‰ 0ì—´), max(0í–‰ 1ì—´ , 1í–‰ 1ì—´)) => ìœ„ì¹˜ ë°˜í™˜
print("dim = 1 ì¼ ë•Œ : ", torch.argmax(tensor_a, dim=1)) # ì—´ì„ ê¸°ì¤€ (ì—´ ì¸ë±ìŠ¤ ë³€í™”)ìœ¼ë¡œ max ë¹„êµ (max(0í–‰ 0ì—´ , 0í–‰ 1ì—´), max(1í–‰ 0ì—´ , 1í–‰ 1ì—´)) => ìœ„ì¹˜ ë°˜í™˜

print('\n')

print("dimension ì§€ì • ì•ˆí–ˆì„ ë•Œ : ", torch.argmin(tensor_a))  # ëª¨ë“  ì›ì†Œì˜ ìµœì†Ÿê°’ ìœ„ì¹˜ ë°˜í™˜ í•¨
print("dim = 0 ì¼ ë•Œ : ", torch.argmin(tensor_a, dim=0))  # í–‰ì„ ê¸°ì¤€ (í–‰ ì¸ë±ìŠ¤ ë³€í™”)ìœ¼ë¡œ min ë¹„êµ (min(0í–‰ 0ì—´ , 1í–‰ 0ì—´), min(0í–‰ 1ì—´ , 1í–‰ 1ì—´)) => ìœ„ì¹˜ ë°˜í™˜
print("dim = 1 ì¼ ë•Œ : ", torch.argmin(tensor_a, dim=1)) # ì—´ì„ ê¸°ì¤€ (ì—´ ì¸ë±ìŠ¤ ë³€í™”)ìœ¼ë¡œ min ë¹„êµ (min(0í–‰ 0ì—´ , 0í–‰ 1ì—´), min(1í–‰ 0ì—´ , 1í–‰ 1ì—´)) => ìœ„ì¹˜ ë°˜í™˜
```

    tensor([[1, 2],
            [3, 4]])
    Shape :  torch.Size([2, 2])
    
    
    dimension ì§€ì • ì•ˆí–ˆì„ ë•Œ :  tensor(3)
    dim = 0 ì¼ ë•Œ :  tensor([1, 1])
    dim = 1 ì¼ ë•Œ :  tensor([1, 1])
    
    
    dimension ì§€ì • ì•ˆí–ˆì„ ë•Œ :  tensor(0)
    dim = 0 ì¼ ë•Œ :  tensor([0, 0])
    dim = 1 ì¼ ë•Œ :  tensor([0, 0])
    

#### ğŸ“ ì„¤ëª… : í–‰ë ¬ ë° ë²¡í„° ê³„ì‚°
* dot : **ë²¡í„°**ì˜ ë‚´ì  (inner product) ë°˜í™˜
  * torch.dot(a,b)
  * a.dot(b)

ğŸ“š ì°¸ê³ í• ë§Œí•œ ìë£Œ:
* [dot] : https://pytorch.org/docs/stable/generated/torch.dot.html


```python
v1 = torch.tensor([1, 2])
u1 = torch.tensor([3, 4])

print("v1.dot(u1) : ", v1.dot(u1)) # v1 ê³¼ u1 ë‚´ì  (torch.tensor ì—ë„ dot í•¨ìˆ˜ ì¡´ì¬)
print("torch.dot(v1, u1) : ", torch.dot(v1, u1)) # v1 ê³¼ u1 ë‚´ì 
```

    v1.dot(u1) :  tensor(11)
    torch.dot(v1, u1) :  tensor(11)
    

#### ğŸ“ ì„¤ëª… : í–‰ë ¬ ë° ë²¡í„° ê³„ì‚°
* matmul : ë‘ í…ì„œ ê°„ì˜ í–‰ë ¬ê³± ë°˜í™˜ ***â€» ì›ì†Œ ê³±ê³¼ ë‹¤ë¦„ ì£¼ì˜â—***
  * torch.matmul(a,b)
  * a.matmul(b)

ğŸ“š ì°¸ê³ í• ë§Œí•œ ìë£Œ:
* [matmul] : https://pytorch.org/docs/stable/generated/torch.matmul.html


```python
A = torch.tensor([[1, 2], [3, 4]])  # (2,2) Tensor
B = torch.tensor([[-1, 2], [1, 0]])  # (2,2) Tensor
print("A: ", A)
print("B: ", B)

print('\n')

print("AB : ", torch.matmul(A, B)) # Aì—ì„œ Bë¥¼ í–‰ë ¬ê³±
print("BA : ", B.matmul(A))  # Bì—ì„œ Aë¥¼ í–‰ë ¬ê³±
```

    A:  tensor([[1, 2],
            [3, 4]])
    B:  tensor([[-1,  2],
            [ 1,  0]])
    
    
    AB :  tensor([[1, 2],
            [1, 6]])
    AB :  tensor([[1, 2],
            [1, 6]])
    BA :  tensor([[5, 6],
            [1, 2]])
    

### 1-2. Broadcasting ì„ ì´ìš©í•œ í…ì„œ ê°’ ë³€ê²½
> Broadcasting ì„ ì´ìš©í•˜ì—¬ í…ì„œì˜ ì›ì†Œë¥¼ ë³€ê²½í•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ ì´í•´í•˜ê³  ì‹¤ìŠµí•©ë‹ˆë‹¤.

#### ğŸ“ ì„¤ëª… : Broadcasting ì„ ì´ìš©í•œ í…ì„œ ì›ì†Œ ë³€ê²½
* scalar ê°’ìœ¼ë¡œ í…ì„œ ì›ì†Œ ë³€ê²½í•˜ê¸°
  * Indexingìœ¼ë¡œ í…ì„œ ì›ì†Œì— ì ‘ê·¼ í›„ scalar ê°’ìœ¼ë¡œ ì›ì†Œ ë³€ê²½

ğŸ“š ì°¸ê³ í• ë§Œí•œ ìë£Œ:
* [Broadcasing semantics] : https://pytorch.org/docs/stable/notes/broadcasting.html


```python
tensor_a = torch.randn(3, 2)
print("Original : \n", tensor_a)

print('\n')

## 0 í–‰ì˜ ëª¨ë“  ì—´ì„ 10 ìœ¼ë¡œ ë³€ê²½í•˜ê¸°
tensor_a[0, :] = 10 # 0í–‰ì˜ ëª¨ë“  ì—´ì— broadcasting ì„ í†µí•œ scalar ê°’ ëŒ€ì…
print("ë³€ê²½ëœ í…ì„œ : \n", tensor_a)
```

    Original : 
     tensor([[ 0.8310, -0.0577],
            [ 1.3267,  0.9531],
            [ 0.4545, -0.8515]])
    
    
    ë³€ê²½ëœ í…ì„œ : 
     tensor([[10.0000, 10.0000],
            [ 1.3267,  0.9531],
            [ 0.4545, -0.8515]])
    

#### ğŸ“ ì„¤ëª… : Broadcasting ì„ ì´ìš©í•œ í…ì„œ ì›ì†Œ ë³€ê²½
* í…ì„œ ê°’ìœ¼ë¡œ í…ì„œ ì›ì†Œ ë³€ê²½í•˜ê¸°
  * Indexingìœ¼ë¡œ í…ì„œ ì›ì†Œì— ì ‘ê·¼ í›„ í…ì„œ ê°’ìœ¼ë¡œ ì›ì†Œ ë³€ê²½

ğŸ“š ì°¸ê³ í• ë§Œí•œ ìë£Œ:
* [Broadcasing semantics] : https://pytorch.org/docs/stable/notes/broadcasting.html


```python
tensor_a = torch.randn(3, 2)
print("Original : \n", tensor_a)

print('\n')

## ëª¨ë“  ê°’ì„ tensor [0,1]ë¡œ ë³€ê²½í•˜ê¸°
tensor_a[:, :] = torch.tensor([0, 1]) # ëª¨ë“  ê°’ì— ì ‘ê·¼í•˜ì—¬ [0,1] ë¡œ ë³€ê²½
print("ë³€ê²½ëœ Tensor : \n", tensor_a)
```

    Original : 
     tensor([[ 0.2746,  1.4863],
            [ 0.4195,  1.0571],
            [-1.6873,  2.0483]])
    
    
    ë³€ê²½ëœ Tensor : 
     tensor([[0., 1.],
            [0., 1.],
            [0., 1.]])
    

### 1-3. Broadcasting ì„ ì´ìš©í•œ ì°¨ì›ì´ ë‹¤ë¥¸ í…ì„œ ê°„ì˜ ê³„ì‚° ì‹¤ìŠµ
> Broadcasting ì„ ì´ìš©í•˜ì—¬ ì°¨ì›ì´ ë‹¤ë¥¸ í…ì„œ ê°„ì˜ ê³„ì‚° ë°©ì‹ì— ëŒ€í•´ ì´í•´í•˜ê³  ì‹¤ìŠµí•©ë‹ˆë‹¤.

#### ğŸ“ ì„¤ëª… : Broadcasting ì„ ì´ìš©í•œ ê³„ì‚°
* ì°¨ì›ì´ ì„œë¡œ ë‹¤ë¥¸ í…ì„œ ê°„ì˜ ê³„ì‚°ì„ broadcasting ì„ í†µí•´ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ğŸ“š ì°¸ê³ í• ë§Œí•œ ìë£Œ:
* [Broadcasing semantics] : https://pytorch.org/docs/stable/notes/broadcasting.html


```python
tensor_a = torch.eye(3)
print("Tensor A : \n",tensor_a)

print('\n')

tensor_b = torch.tensor([1, 2, 3])
print("Tensor B : \n", tensor_b)

print('\n')

print('A + B : \n', tensor_a + tensor_b) # broadcastingì„ í†µí•´ (3,) ì¸ Bê°€ (3,3)ìœ¼ë¡œ ë³€í™˜ë˜ì–´ ê³„ì‚° (í–‰ì˜ í™•ì¥)
```

    Tensor A : 
     tensor([[1., 0., 0.],
            [0., 1., 0.],
            [0., 0., 1.]])
    
    
    Tensor B : 
     tensor([1, 2, 3])
    
    
    A + B : 
     tensor([[2., 2., 3.],
            [1., 3., 3.],
            [1., 2., 4.]])
    


```python
tensor_a = torch.eye(3)
print("Tensor A : \n", tensor_a)

print('\n')

tensor_b = torch.tensor([1, 2, 3]).reshape(3, 1) # í–‰ ë²¡í„°ë¡œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
print("Tensor B : \n", tensor_b)

print('\n')

print('A + B : \n', tensor_a + tensor_b) # broadcastingì„ í†µí•´ (3,1) ì¸ Bê°€ (3,3)ìœ¼ë¡œ ë³€í™˜ë˜ì–´ ê³„ì‚° (ì—´ì˜ í™•ì¥)
```

    Tensor A : 
     tensor([[1., 0., 0.],
            [0., 1., 0.],
            [0., 0., 1.]])
    
    
    Tensor B : 
     tensor([[1],
            [2],
            [3]])
    
    
    A + B : 
     tensor([[2., 1., 1.],
            [2., 3., 2.],
            [3., 3., 4.]])
    

#### ğŸ“ ì„¤ëª… : Broadcasting ì„ ì´ìš©í•œ ê³„ì‚°
* ì°¨ì›ì˜ ë§ì§€ ì•ŠëŠ” ê²½ìš°, ì°¨ì›ì„ ì¶”ê°€í•˜ì—¬ broadcasting ìœ¼ë¡œ í…ì„œ ê°„ì˜ ê³„ì‚°ì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ğŸ“š ì°¸ê³ í• ë§Œí•œ ìë£Œ:
* [Broadcasing semantics] : https://pytorch.org/docs/stable/notes/broadcasting.html


```python
tensor_a = torch.randn(3, 2, 5)
mean_a = tensor_a.mean(2) # ì—´ ê¸°ì¤€ í‰ê· ê°’
print(f"Tensor size : {tensor_a.size()}, mean size : {mean_a.size()}")

print('\n')

print(tensor_a - mean_a)  # ì—ëŸ¬ ë°œìƒ! ì°¨ì›ì´ ë‹¬ë¼ì„œ ê³„ì‚°ì´ ë˜ì§€ ì•ŠìŒ
```

    Tensor size : torch.Size([3, 2, 5]), mean size : torch.Size([3, 2])
    
    
    


    ---------------------------------------------------------------------------

    RuntimeError                              Traceback (most recent call last)

    <ipython-input-15-1f36eac833e5> in <cell line: 7>()
          5 print('\n')
          6 
    ----> 7 print(tensor_a - mean_a )  # ì—ëŸ¬ ë°œìƒ! ì°¨ì›ì´ ë‹¬ë¼ì„œ ê³„ì‚°ì´ ë˜ì§€ ì•ŠìŒ
    

    RuntimeError: The size of tensor a (5) must match the size of tensor b (2) at non-singleton dimension 2



```python
# ì°¨ì› ìƒì„± í›„ broadcasting
unseq_mean = mean_a.unsqueeze(-1) # ë§ˆì§€ë§‰ ì¶• ì¶”ê°€
print(unseq_mean.size())

print('\n')

print(tensor_a - unseq_mean)
```

    torch.Size([3, 2, 1])
    
    
    tensor([[[-1.0898, -0.3991, -0.3112,  1.4851,  0.3149],
             [-1.0677, -0.1642, -0.2781, -0.3280,  1.8380]],
    
            [[-0.8618, -0.0473,  0.9947,  0.6992, -0.7848],
             [ 0.2205, -1.5032,  1.0109,  1.1358, -0.8640]],
    
            [[-0.9111, -0.7767,  0.6328,  0.3092,  0.7459],
             [-0.5014, -1.0947,  1.4581, -0.8344,  0.9724]]])
    

## 2. Sparse Tensor ì¡°ì‘ ë° ì‹¤ìŠµ

- 2-1. COO Tensor ì— ëŒ€í•œ ì´í•´ ë° ì‹¤ìŠµ
- 2-2. CSC/CSR Tensor ì— ëŒ€í•œ ì´í•´ ë° ì‹¤ìŠµ
- 2-3. Sparse Tensorì˜ í•„ìš”ì„± ì´í•´ ë° ì‹¤ìŠµ
- 2-4. Sparse Tensor ì˜ ì¡°ì‘ ì˜ˆì‹œ


### 2-1 COO Sparse Tensorì— ëŒ€í•œ ì‹¤ìŠµ

> Sparse tensor ë¡œ ë³€í™˜í•˜ëŠ” ë°©ë²• ì¤‘ COO ë°©ì‹ì— ëŒ€í•´ ì•Œì•„ë³´ê³  ì‹¤ìŠµí•©ë‹ˆë‹¤.


```python
a = torch.tensor([[0, 2.], [3, 0]])

a.to_sparse() # COO Sparse tensor ë¡œ ë³€í™˜
```




    tensor(indices=tensor([[0, 1],
                           [1, 0]]),
           values=tensor([2., 3.]),
           size=(2, 2), nnz=2, layout=torch.sparse_coo)



#### ğŸ“ ì„¤ëª… : COO Sparse Tensor ìƒì„±
* sparse_coo_tensor : COO í˜•ì‹ì˜ sparse tensor ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
  * indices : 0 ì´ ì•„ë‹Œ ê°’ì„ ê°€ì§„ í–‰,ì—´ì˜ ìœ„ì¹˜
  * values : 0 ì´ ì•„ë‹Œ ê°’
  * nnz : 0 ì´ ì•„ë‹Œ ê°’ì˜ ê°œìˆ˜


ğŸ“š ì°¸ê³ í• ë§Œí•œ ìë£Œ:
* [sparse_coo_tensor] : https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html


```python
indices = torch.tensor([[0, 1, 1],[2, 0, 1]]) # 0ì´ ì•„ë‹Œ ê°’ì˜ (index, column) pairs
values = torch.tensor([4, 5, 6]) # 0 ì´ ì•„ë‹Œ ê°’ì˜ values, valuesì˜ ì‚¬ì´
sparse_tensor = torch.sparse_coo_tensor(indices = indices, values = values, size=(2, 3)) # (2,3)ì˜ sparse tensor

print(sparse_tensor)
print('\n')
print(sparse_tensor.to_dense())
```

    tensor(indices=tensor([[0, 1, 1],
                           [2, 0, 1]]),
           values=tensor([4, 5, 6]),
           size=(2, 3), nnz=3, layout=torch.sparse_coo)
    
    
    tensor([[0, 0, 4],
            [5, 6, 0]])
    

### 2-2 CSR/CSC Sparse Tensorì— ëŒ€í•œ ì‹¤ìŠµ

> Sparse tensor ë¡œ ë³€í™˜í•˜ëŠ” ë°©ë²• ì¤‘ CSR/CSC ë°©ì‹ì— ëŒ€í•´ ì•Œì•„ë³´ê³  ì‹¤ìŠµí•©ë‹ˆë‹¤.

#### ğŸ“ ì„¤ëª… : CSR Sparse Tensor ë¡œ ë³€í™˜
* to_sparse_csr : Dense tensorë¥¼ CSR í˜•ì‹ì˜ Sparse tensorë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
  * crow_indices : 0 ì´ ì•„ë‹Œ ê°’ì„ ê°€ì§„ í–‰ì˜ ìœ„ì¹˜ (ì²«ë²ˆì§¸ëŠ” ë¬´ì¡°ê±´ 0)
  * col_indices : 0 ì´ ì•„ë‹Œ ê°’ì„ ê°€ì§„ ì—´ì˜ ìœ„ì¹˜
  * values : 0 ì´ ì•„ë‹Œ ê°’
  * nnz : 0 ì´ ì•„ë‹Œ ê°’ì˜ ê°œìˆ˜
  
ğŸ“š ì°¸ê³ í• ë§Œí•œ ìë£Œ:
* [csr] : https://pytorch.org/docs/stable/sparse.html#sparse-csr-docs



```python
t = torch.tensor([[0, 0, 4, 3], [5, 6, 0, 0]])
print("Shape : ", t.size())
print(t)

print('\n')

t.to_sparse_csr()  # Dense Tensorë¥¼ CSR Sparse Tensor í˜•ì‹ìœ¼ë¡œ ë³€í™˜
```

    Shape :  torch.Size([2, 4])
    tensor([[0, 0, 4, 3],
            [5, 6, 0, 0]])
    
    
    




    tensor(crow_indices=tensor([0, 2, 4]),
           col_indices=tensor([2, 3, 0, 1]),
           values=tensor([4, 3, 5, 6]), size=(2, 4), nnz=4,
           layout=torch.sparse_csr)



#### ğŸ“ ì„¤ëª… : CSC Sparse Tensor ë¡œ ë³€í™˜
* to_sparse_csc : Dense tensorë¥¼ CSC í˜•ì‹ì˜ Sparse tensorë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
  * ccol_indices : 0 ì´ ì•„ë‹Œ ê°’ì˜ ì—´ ìœ„ì¹˜ (ì²«ë²ˆì§¸ ì›ì†ŒëŠ” ë¬´ì¡°ê±´ 0)
  * row_indices : 0 ì´ ì•„ë‹Œ ê°’ì˜ í–‰ ìœ„ì¹˜
  * values : 0 ì´ ì•„ë‹Œ ê°’ë“¤
  * nnz : 0 ì´ ì•„ë‹Œ ê°’ì˜ ê°œìˆ˜
  
ğŸ“š ì°¸ê³ í• ë§Œí•œ ìë£Œ:
* [csc] : https://pytorch.org/docs/stable/sparse.html#sparse-csc-docs


```python
t = torch.tensor([[0, 0, 4, 3], [5, 6, 0, 0]])
print("Shape : ", t.size())
print(t)

print('\n')

t.to_sparse_csc()  # Dense Tensor ë¥¼ CSC Spare tensor í˜•ì‹ìœ¼ë¡œ ë³€í™˜
```

    Shape :  torch.Size([2, 4])
    tensor([[0, 0, 4, 3],
            [5, 6, 0, 0]])
    
    
    




    tensor(ccol_indices=tensor([0, 1, 2, 3, 4]),
           row_indices=tensor([1, 1, 0, 0]),
           values=tensor([5, 6, 4, 3]), size=(2, 4), nnz=4,
           layout=torch.sparse_csc)



#### ğŸ“ ì„¤ëª… : CSR Sparse Tensor ìƒì„±
* sparse_csr_tensor : CSR í˜•ì‹ì˜ Sparse tensor ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜

ğŸ“š ì°¸ê³ í• ë§Œí•œ ìë£Œ:
* [sparse_csr_tensor] : https://pytorch.org/docs/stable/generated/torch.sparse_csr_tensor.html


```python
crow_indices = torch.tensor([0, 2, 2]) # 0ì´ ì•„ë‹Œ í–‰ì˜ ìœ„ì¹˜ (ì²«ë²ˆì¨°ëŠ” ë¬´ì¡°ê±´ 0), ì¦‰ row_pointer
col_indices = torch.tensor([0, 1]) # 0ì´ ì•„ë‹Œ ì—´ì˜ ìœ„ì¹˜
values = torch.tensor([1, 2]) # 0ì´ ì•„ë‹Œ ê°’
csr = torch.sparse_csr_tensor(crow_indices = crow_indices, col_indices = col_indices, values = values)

print(csr)
print('\n')
print(csr.to_dense())
```

    tensor(crow_indices=tensor([0, 2, 2]),
           col_indices=tensor([0, 1]),
           values=tensor([1, 2]), size=(2, 2), nnz=2, layout=torch.sparse_csr)
    
    
    tensor([[1, 2],
            [0, 0]])
    

#### ğŸ“ ì„¤ëª… : CSC Sparse Tensor ìƒì„±
* sparse_csc_tensor : CSC í˜•ì‹ì˜ Sparse tensor ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜

ğŸ“š ì°¸ê³ í• ë§Œí•œ ìë£Œ:
* [sparse_csc_tensor] : https://pytorch.org/docs/stable/generated/torch.sparse_csc_tensor.html


```python
ccol_indices = torch.tensor([0, 2, 2]) # 0ì´ ì•„ë‹Œ ì—´ì˜ ìœ„ì¹˜ (ì²«ë²ˆì¨°ëŠ” ë¬´ì¡°ê±´ 0), ì¦‰ column_pointer
row_indices = torch.tensor([0, 1]) # 0ì´ ì•„ë‹Œ í–‰ì˜ ìœ„ì¹˜
values = torch.tensor([1, 2]) # 0ì´ ì•„ë‹Œ ê°’
csc = torch.sparse_csc_tensor(ccol_indices = ccol_indices, row_indices = row_indices, values = values)

print(csc)
print('\n')
print(csc.to_dense())
```

    tensor(ccol_indices=tensor([0, 2, 2]),
           row_indices=tensor([0, 1]),
           values=tensor([1, 2]), size=(2, 2), nnz=2, layout=torch.sparse_csc)
    
    
    tensor([[1, 0],
            [2, 0]])
    

### 2-3 Sparse Tensorì˜ í•„ìš”ì„± ì´í•´ ë° ì‹¤ìŠµ

> Dense tensorê°€ ê°€ì§€ëŠ” í•œê³„ì ì— ëŒ€í•´ ì´í•´í•˜ê³ , Sparse tensor ê°€ í•„ìš”í•œ ì´ìœ ì— ëŒ€í•´ ì•Œì•„ë´…ë‹ˆë‹¤.

#### ğŸ“ ì„¤ëª… : Sparse Tensor ì˜ í•„ìš”ì„±
* ì•„ì£¼ í° í¬ê¸°ì˜ matrix ë¥¼ êµ¬ì„±í•  ë•Œ, ì¼ë°˜ì ì¸ dense tensor ëŠ” ë©”ëª¨ë¦¬ ì•„ì›ƒ í˜„ìƒì´ ë°œìƒí•˜ì§€ë§Œ, sparse tensor ëŠ” ë©”ëª¨ë¦¬ ì•„ì›ƒí˜„ìƒì´ ë°œìƒí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
  * to_dense() : sparse tensor ë¥¼ dense tensor ë¡œ ë§Œë“œëŠ” í•¨ìˆ˜


```python
i = torch.randint(0, 100000, (200000,)).reshape(2, -1)
v = torch.rand(100000)
coo_sparse_tensor = torch.sparse_coo_tensor(indices = i, values = v, size = [100000, 100000]) # COO Sparse Tensor (100000 x 100000)
```


```python
crow = torch.randint(0, 100000, (100000,))
col = torch.randint(0, 100000, (100000,))
v = torch.rand(100000)
csr_sparse_tensor = torch.sparse_csr_tensor(crow_indices = crow, col_indices = col, values = v) # CSR Sparse Tensor (100000 x 100000)
```

    <ipython-input-2-f025b39a270c>:4: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at ../aten/src/ATen/SparseCsrTensorImpl.cpp:54.)
      csr_sparse_tensor = torch.sparse_csr_tensor(crow_indices = crow, col_indices = col, values = v) # CSR Sparse Tensor (100000 x 100000)
    


```python
coo_sparse_tensor.to_dense() # COO í˜•ì‹ìœ¼ë¡œ ë§Œë“¤ì–´ì§„ Sparse Tensor ë¥¼ Dense Tensor ë¡œ ë³€í™˜ , ë©”ëª¨ë¦¬ ì•„ì›ƒ
```


```python
import torch # ì»¤ë„ ì¬ì‹œì‘ í•˜ê¸°ì— ë‹¤ì‹œ torch library ë¡œë“œ
```


```python
csr_sparse_tensor.to_dense() # CSR í˜•ì‹ìœ¼ë¡œ ë§Œë“¤ì–´ì§„ Sparse Tensor ë¥¼ Dense Tensor ë¡œ ë³€í™˜ , ë©”ëª¨ë¦¬ ì•„ì›ƒ
```


```python
import torch # ì»¤ë„ ì¬ì‹œì‘ í•˜ê¸°ì— ë‹¤ì‹œ torch library ë¡œë“œ
```

### 2-4 Sparse Tensorì˜ ì¡°ì‘ ë°©ë²•

> Sparse tensor ì˜ Indexing ê³¼ ì—°ì‚° ë°©ë²•ì— ëŒ€í•´ ì•Œì•„ë´…ë‹ˆë‹¤.

#### ğŸ“ ì„¤ëª… : Sparse Tensor ì˜ ì—°ì‚° (=2ì°¨ì›)
* 2ì°¨ì› sparse tensor ê°„ì—ëŠ” ì¼ë°˜ í…ì„œì™€ ë™ì¼í•˜ê²Œ ì‚¬ì¹™ì—°ì‚° í•¨ìˆ˜ë“¤ê³¼ í–‰ë ¬ê³±ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ğŸ“š ì°¸ê³ í• ë§Œí•œ ìë£Œ:
* [sparse] : https://pytorch.org/docs/stable/sparse.html


```python
# Sparse ì™€ Sparse Tensor ê°„ì˜ ì—°ì‚° (2ì°¨ì›)
a = torch.tensor([[0, 1], [0, 2]], dtype=torch.float)
b = torch.tensor([[1, 0],[0, 0]], dtype=torch.float)

sparse_a = a.to_sparse()
sparse_b = b.to_sparse()

print('ë§ì…ˆ')
print(torch.add(a, b).to_dense() == torch.add(sparse_a, sparse_b).to_dense())

print('\n')
print('ê³±ì…ˆ')
print(torch.mul(a, b).to_dense() == torch.mul(sparse_a, sparse_b).to_dense())

print('\n')

print('í–‰ë ¬ê³±')
print(torch.matmul(a, b).to_dense() == torch.matmul(sparse_a, sparse_b).to_dense())
```

    ë§ì…ˆ
    tensor([[True, True],
            [True, True]])
    
    
    ê³±ì…ˆ
    tensor([[True, True],
            [True, True]])
    
    
    í–‰ë ¬ê³±
    tensor([[True, True],
            [True, True]])
    

#### ğŸ“ ì„¤ëª… : Sparse Tensor ì˜ ì—°ì‚° (=3ì°¨ì›)
* 3ì°¨ì› sparse tensor ì—ëŠ” ì¼ë°˜ í…ì„œì™€ ë™ì¼í•˜ê²Œ ì‚¬ì¹™ì—°ì‚° í•¨ìˆ˜ë“¤ì€ ì‚¬ìš© ê°€ëŠ¥í•˜ì§€ë§Œ í–‰ë ¬ê³±ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.
  * CSR/CSC í˜•ì‹ì—ì„œëŠ” ê³±ì…ˆë„ 3ì°¨ì›ì—ì„  ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.
* ì´ëŠ” sparse tensor ì™€ sparse tensor ê°„ì—ë„ ì ìš©ì´ ë˜ê³ , sparse tensor ì™€ dense tensor ê°„ì˜ ì—°ì‚°ì—ë„ ì ìš©ì´ ë©ë‹ˆë‹¤.


```python
# Sparse ì™€ Sparse Tensor ê°„ì˜ ì—°ì‚° (3ì°¨ì›)
a = torch.tensor([[[0, 1], [0, 2]], [[0, 1], [0, 2]]], dtype=torch.float)
b = torch.tensor([[[1, 0],[0, 0]], [[1, 0], [0, 0]]], dtype=torch.float)

sparse_a = a.to_sparse()
sparse_b = b.to_sparse()

print('ë§ì…ˆ')
print(torch.add(a, b).to_dense() == torch.add(sparse_a, sparse_b).to_dense())

print('\n')
print('ê³±ì…ˆ')
print(torch.mul(a, b).to_dense() == torch.mul(sparse_a, sparse_b).to_dense())

print('\n')

print('í–‰ë ¬ê³±')
print(torch.matmul(a, b).to_dense() == torch.matmul(sparse_a, sparse_b).to_dense()) # ì—ëŸ¬ ë°œìƒ
```

    ë§ì…ˆ
    tensor([[[True, True],
             [True, True]],
    
            [[True, True],
             [True, True]]])
    
    
    ê³±ì…ˆ
    tensor([[[True, True],
             [True, True]],
    
            [[True, True],
             [True, True]]])
    
    
    í–‰ë ¬ê³±
    


    ---------------------------------------------------------------------------

    NotImplementedError                       Traceback (most recent call last)

    <ipython-input-3-55a4bf6b86d8> in <cell line: 18>()
         16 
         17 print('í–‰ë ¬ê³±')
    ---> 18 print(torch.matmul(a, b).to_dense() == torch.matmul(sparse_a, sparse_b).to_dense()) # ì—ëŸ¬ ë°œìƒ
    

    NotImplementedError: Could not run 'aten::as_strided' with arguments from the 'SparseCPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::as_strided' is only available for these backends: [CPU, CUDA, Meta, QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].
    
    CPU: registered at aten/src/ATen/RegisterCPU.cpp:31034 [kernel]
    CUDA: registered at aten/src/ATen/RegisterCUDA.cpp:43986 [kernel]
    Meta: registered at aten/src/ATen/RegisterMeta.cpp:26824 [kernel]
    QuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:929 [kernel]
    QuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]
    BackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]
    Python: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]
    FuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:491 [backend fallback]
    Functionalize: registered at aten/src/ATen/RegisterFunctionalization_0.cpp:20475 [kernel]
    Named: fallthrough registered at ../aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]
    Conjugate: fallthrough registered at ../aten/src/ATen/ConjugateFallback.cpp:21 [kernel]
    Negative: fallthrough registered at ../aten/src/ATen/native/NegateFallback.cpp:23 [kernel]
    ZeroTensor: registered at aten/src/ATen/RegisterZeroTensor.cpp:161 [kernel]
    ADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:4733 [kernel]
    AutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    Tracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:16728 [kernel]
    AutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:487 [backend fallback]
    AutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:354 [backend fallback]
    FuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:819 [kernel]
    FuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]
    Batched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1077 [kernel]
    VmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]
    FuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:210 [backend fallback]
    PythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:152 [backend fallback]
    FuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:487 [backend fallback]
    PythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]
    



```python
# Dense ì™€ Sparse Tensor ê°„ì˜ ì—°ì‚° (2ì°¨ì›)
a = torch.tensor([[0,1],[0,2]],dtype=torch.float)
b = torch.tensor([[1,0],[0,0]],dtype=torch.float).to_sparse()

sparse_b = b.to_sparse()

print('ë§ì…ˆ')
print(torch.add(a, b).to_dense() == torch.add(a, sparse_b).to_dense())

print('\n')
print('ê³±ì…ˆ')
print(torch.mul(a, b).to_dense() == torch.mul(a, sparse_b).to_dense())

print('\n')

print('í–‰ë ¬ê³±')
print(torch.matmul(a, b).to_dense() == torch.matmul(a, sparse_b).to_dense())
```

    ë§ì…ˆ
    tensor([[True, True],
            [True, True]])
    
    
    ê³±ì…ˆ
    tensor([[True, True],
            [True, True]])
    
    
    í–‰ë ¬ê³±
    tensor([[True, True],
            [True, True]])
    


```python
a = torch.tensor([[[0, 1], [0, 2]], [[0, 1], [0, 2]]],dtype=torch.float)
b = torch.tensor([[[1, 0], [0, 0]], [[1, 0], [0, 0]]],dtype=torch.float)

sparse_b = b.to_sparse()

print('ë§ì…ˆ')
print(torch.add(a, b).to_dense() == torch.add(a, sparse_b).to_dense())

print('\n')
print('ê³±ì…ˆ')
print(torch.mul(a, b).to_dense() == torch.mul(a, sparse_b).to_dense())

print('\n')

print('í–‰ë ¬ê³±')
print(torch.matmul(a, b).to_dense() == torch.matmul(a, sparse_b).to_dense()) # ì—ëŸ¬ ë°œìƒ
```

    ë§ì…ˆ
    tensor([[[True, True],
             [True, True]],
    
            [[True, True],
             [True, True]]])
    
    
    ê³±ì…ˆ
    tensor([[[True, True],
             [True, True]],
    
            [[True, True],
             [True, True]]])
    
    
    í–‰ë ¬ê³±
    


    ---------------------------------------------------------------------------

    NotImplementedError                       Traceback (most recent call last)

    <ipython-input-6-89ab68b0fa2a> in <cell line: 16>()
         14 
         15 print('í–‰ë ¬ê³±')
    ---> 16 print(torch.matmul(a, b).to_dense() == torch.matmul(a, sparse_b).to_dense())
    

    NotImplementedError: Could not run 'aten::as_strided' with arguments from the 'SparseCPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::as_strided' is only available for these backends: [CPU, CUDA, Meta, QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].
    
    CPU: registered at aten/src/ATen/RegisterCPU.cpp:31034 [kernel]
    CUDA: registered at aten/src/ATen/RegisterCUDA.cpp:43986 [kernel]
    Meta: registered at aten/src/ATen/RegisterMeta.cpp:26824 [kernel]
    QuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:929 [kernel]
    QuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]
    BackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]
    Python: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]
    FuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:491 [backend fallback]
    Functionalize: registered at aten/src/ATen/RegisterFunctionalization_0.cpp:20475 [kernel]
    Named: fallthrough registered at ../aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]
    Conjugate: fallthrough registered at ../aten/src/ATen/ConjugateFallback.cpp:21 [kernel]
    Negative: fallthrough registered at ../aten/src/ATen/native/NegateFallback.cpp:23 [kernel]
    ZeroTensor: registered at aten/src/ATen/RegisterZeroTensor.cpp:161 [kernel]
    ADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:4733 [kernel]
    AutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    Tracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:16728 [kernel]
    AutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:487 [backend fallback]
    AutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:354 [backend fallback]
    FuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:819 [kernel]
    FuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]
    Batched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1077 [kernel]
    VmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]
    FuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:210 [backend fallback]
    PythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:152 [backend fallback]
    FuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:487 [backend fallback]
    PythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]
    


#### ğŸ“ ì„¤ëª… : Sparse Tensor ì˜ Indexing
* ì¼ë°˜ í…ì„œì™€ ë™ì¼í•˜ê²Œ indexing ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.
  * slicing (":" ì„ ì‚¬ìš©)ì€ ë¶ˆê°€ëŠ¥ í•©ë‹ˆë‹¤.
  
ğŸ“š ì°¸ê³ í• ë§Œí•œ ìë£Œ:
* [sparse] : https://pytorch.org/docs/stable/sparse.html


```python
a = torch.tensor([[0,1 ], [0, 2]], dtype=torch.float)
b = torch.tensor([[[1, 0], [0, 0]], [[1, 0], [0, 0]]], dtype=torch.float)

sparse_a = a.to_sparse()
sparse_b = b.to_sparse()

print('2ì°¨ì› Sparse Tensor ì¸ë±ì‹±')
print(a[0] == sparse_a[0].to_dense())

print('\n')

print('3ì°¨ì› Sprase Tensor ì¸ë±ì‹±')
print(b[0] == sparse_b[0].to_dense())
```

    2ì°¨ì› Sparse Tensor ì¸ë±ì‹±
    tensor([True, True])
    
    
    3ì°¨ì› Sprase Tensor ì¸ë±ì‹±
    tensor([[True, True],
            [True, True]])
    


```python
a = torch.tensor([[0, 1], [0, 2]], dtype=torch.float).to_sparse_csr() # 2dim Sparse Tensor (CSR)
a[0,:] # 0í–‰ì˜ ëª¨ë“  ì›ì†Œ ì¶”ì¶œ => ì—ëŸ¬ ë°œìƒ
```


    ---------------------------------------------------------------------------

    NotImplementedError                       Traceback (most recent call last)

    <ipython-input-9-d2d97aa5e6da> in <cell line: 2>()
          1 a = torch.tensor([[0, 1], [0, 2]], dtype=torch.float).to_sparse_csr() # 2dim Sparse Tensor (CSR)
    ----> 2 a[0,:] # 0í–‰ì˜ ëª¨ë“  ì›ì†Œ ì¶”ì¶œ => ì—ëŸ¬ ë°œìƒ
    

    NotImplementedError: Could not run 'aten::as_strided' with arguments from the 'SparseCPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::as_strided' is only available for these backends: [CPU, CUDA, Meta, QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].
    
    CPU: registered at aten/src/ATen/RegisterCPU.cpp:31034 [kernel]
    CUDA: registered at aten/src/ATen/RegisterCUDA.cpp:43986 [kernel]
    Meta: registered at aten/src/ATen/RegisterMeta.cpp:26824 [kernel]
    QuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:929 [kernel]
    QuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]
    BackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]
    Python: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]
    FuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:491 [backend fallback]
    Functionalize: registered at aten/src/ATen/RegisterFunctionalization_0.cpp:20475 [kernel]
    Named: fallthrough registered at ../aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]
    Conjugate: fallthrough registered at ../aten/src/ATen/ConjugateFallback.cpp:21 [kernel]
    Negative: fallthrough registered at ../aten/src/ATen/native/NegateFallback.cpp:23 [kernel]
    ZeroTensor: registered at aten/src/ATen/RegisterZeroTensor.cpp:161 [kernel]
    ADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:4733 [kernel]
    AutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    Tracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:16728 [kernel]
    AutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:487 [backend fallback]
    AutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:354 [backend fallback]
    FuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:819 [kernel]
    FuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]
    Batched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1077 [kernel]
    VmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]
    FuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:210 [backend fallback]
    PythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:152 [backend fallback]
    FuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:487 [backend fallback]
    PythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]
    


#Reference
> <b><font color = green>(ğŸ“’ê°€ì´ë“œ)
- <a href='https://pytorch.org/docs/stable/index.html'>PyTorch ê³µì‹ ë¬¸ì„œ</a>
- <a href='https://bkshin.tistory.com/entry/NLP-7-%ED%9D%AC%EC%86%8C-%ED%96%89%EB%A0%AC-Sparse-Matrix-COO-%ED%98%95%EC%8B%9D-CSR-%ED%98%95%EC%8B%9D'>COO ì™€ CSR/CSC</a>

## Required Package

> torch == 2.0.1


```python

```
