---
layout: post
title: 딥러닝의 발전 5단계 (2)
date: 2023-12-19 14:35 +0900
last_modified_at: 2023-12-19 14:58:00 +0900
tags: [DeepLearning]
toc:  true
---

## 1. 전체 한판

![Alt text](\..\img\DL1-1.png)

## 2. 사전 학습과 미세 조정

기존 문제점 : 분류 대상이 / task가 바뀔 때마다 다른 모델이 필요

    ex. 동물을 구분하는 task

    1. 개 / 고양이 구분
    2. 소 / 돼지 구분

    각각 각기 다른 model이 필요하다

---

하지만 사전학습(pre-training)과 미세 조정(fine-tuning)을 이용하면 이 문제를 해결 할 수 있다.

**image data의 관점**

    ex. 동물을 구분하는 task

    많은 동물을 구분하는 model 한개를 구축

    이후, 구분하고자하는 동물에 따라 feature를 고정하고, 맵핑 쪽에 해당하는 연산들만 새로 학습

여전히 task마다 다른 모델이 필요하지만, pre-trained model이 있으면, <mark>필요한 데이터 수가 적어지고, 개발 속도가 올라간다.</mark>

**text data의 관점**

1. Pre-training

text를 통해 언어의 일반적인 특징을 익히게 한다. > 고정한다

(GPT1) 입력 텍스트에서 정답을 만들어낸다. (*Un-supervised pretraining*)

    ex. text : '곰 세마리가 한 집에 있어'

    #1 IN : '곰'               / OUT : '세마리가'
    #2 IN : '곰 세마리가'       / OUT : '한'
    #3 IN : '곰 세마리가 한'    / OUT : '집에'
    ...

2. Fine-tuning / Transfer Learning

task에 대해 mapping하는 것을 조정

### in-context learning

기존에는 text의 feature를 pre-train해서 fine-tuning해서 task에 맞는 모델을 만들어야했다.

Big model , zero/few shot에서는

<mark>task 별로 별도 모델이 필요없음</mark>

<mark>task에 맞는 데이터를 모을 필요가 없음</mark>

**fine-tuning없이**, input에서 필요한 task를 명시한 경우, 알아서 문제를 해결한다는 점이다.

(few shot은 모델에게 K개의 정답 예시를 알려주는 경우를 말함, zero shot은 정답 예시를 주지 않음)

