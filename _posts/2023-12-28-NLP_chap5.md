---
layout: post
title: NLP 4. 워드 임베딩 만들기
date: 2023-12-28 11:00 +0900
last_modified_at: 2023-12-28 11:00:00 +0900
tags: [NLP]
toc:  true
---

# 1. 문장 임베딩 만들기

## 자연어 처리를 위한 모델 구조

### 문장임베딩

단어 수준의 임베딩 기법은 자연어의 특성인 <br>모호성 (동음이의어)를 구분하기 어렵다는 한계가 있었다

2017년 이후에는 ELMO나 Transformer와 같은 언어모델에서<br>문장 수준의 언어 모델링을 고려해서 한계점을 돌파

### 언어 모델

자연어 문장 혹은 단어에 확률을 할당하여 컴퓨터가 처리할 수 있도록 하는 모델<br>
주어진 입력에 대해 가장 자연스로운 단어 시퀀스를 찾을 수 있음

---
#### 언어 모델링

주어진 단어들로부터 아직 모르는 단어들을 예측하는 작업을 말함

총 n개의 단어로 구성된 문장에서, (n-1)개의 단어가 주어졌을 때,<br>
n번째 위치에서 출현할 단어의 예측을 확률적으로 표현하면 다음과 같음

$P(w_n \| w_1, w_2, w_3, \dots , w_{n-1})$

문장은 어순을 고려하여 여러 단어로 이루어진 단어 시퀀스라고도 부르며,<br>
n개의 단어로 구성된 단어 시퀀스($W$)를 확률적으로 표현하면 다음과 같음

$\begin{aligned} P(W) &= P(w_1, w_2, w_3, \dots, w_n) \newline
&= P(w_1) \times P(w_2\|w_1) \times P(w_3\|w_1, w_2) \times \dots \times P(w_n\|w_1, w_2, \dots , w_{n-1})\end{aligned}$

특정 문장을 확률로 표현할 수 있다는 특징 $\Rightarrow$ 대부분의 자연어 처리 작업들에 활용 가능
<br>ex. 기계번역, 챗봇 ...

단어 시퀀스에 확률을 할당하는 주요 방법은
1. 문장 내 masking된 단어를 예측하거나 <br>(Masked Language Modeling)
2. 이전 단어들이 주어졌을 때, 다음 단어를 예측하도록 하는 것 <br>(Next Token Prediction)

## Seq2Seq

### 등장 배경

DNN이 음성 인식, 사물 인식에서 꾸준한 성과<br>
but 모델의 입/출력의 크기가 고정된다는 한계점이 존재<br>
자연어처리는 가변적인 입/출력을 가지기 때문에 어려웠음

RNN은 시퀀스 단위의 처리를 가질 수 있었지만,<br>
반복되는 셀의 개수만큼 처리해야해서 입출력이 달라지면 사용 불가

### Seq2Seq

LSTM 또는 GRU 기반의 구조를 가지고 고정된 길이의 단어 시퀀스를 입력으로 받아, 입력 시퀀스에 알맞은 길이의 시퀀스를 출력해주는 언어모델

### 모델 구조

![Alt text](\..\img\DL4-16.png)

sequence to sequence는 한 문장을 다른 문장으로 변환하는 모델이란 의미

Seq2Seq는 가변 길이의 입/출력을 처리하기 위해서 Encoder/Decoder 구조를 채택

인코더가 input을 context vector로 만들고 (인코딩)<br>
인코더가 인코딩한 context vector를 입력으로 받는 디코더가 출력 문장을 생성(디코딩)

이런 방식으로 서로 다른 입출력 시퀀스에 대응가능

---

LSTM이나 GRU를 사용하는 이유는, RNN의 문제인 Long-term dependency를 해결하는 모델이기 때문

---

Seq2Seq 모델을 학습할 때에는 Teacher forcing이라는 기법을 사용한다.

*Teacher forcing* : 이전 셀에서 예측한 단어를 다음 셀의 입력으로 하지 않고<br>
실제 정답 단어를 다음 셀의 입력으로 넣음

---

### 한계점

1. 고정된 크기의 벡터에 모든 정보를 압축하다 보니 정보 손실 발생
2. RNN 구조의 고질적인 문제인 Vanishing Gradient Problem