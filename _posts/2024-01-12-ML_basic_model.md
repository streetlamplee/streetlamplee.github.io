---
title: ML. 기본 ML 모델
layout: post
date: 2024-01-12 15:00 +0900
last_modified_at: 2024-01-12 15:00:00 +0900
tag: [ML, model]
toc: true
---

# 기본 ML 모델

## 1. Overview

### 1.1 머신러닝 모델의 목적

* 분류(Classification)
* 회귀(Regression)

### 1.2 머신러닝 모델의 분류

* 선형(Linear)
* 비선형(non-Linear)

## 2. Linear Regression

### 2.1 알고리즘 설명

데이터를 가장 잘 대변하는 **최적의 선**을 찾는 과정

독립변수(X)와 연속형 종속변수(Y) 사이의 **선형 관계**를 학습

$y = \beta_0 + \beta_1 x$

---

*최소자승법* (Ordinary Least Square, OLS)

X와 Y의 관계를 가장 잘 나타내는 Best line을 찾는 방법

SSE를 최소화하는 회귀계수 $\beta$를 구하는 방법

---

장점1 : 학습/예측 속도가 빠르다.<br>
장점2 : 모델의 해석이 명확하다. (회귀 계수를 해석할 수 있다.)

단점1 : X와 Y의 선형관계를 가정하지만, 현실에서는 이러한 가정이 잘 적용되지 않을 수 있다.<br>
단점2 : **이상치**에 민감

---

*선형 회귀의 가정*

1. 선형성<br>
독립변수 X와 종속변수 Y 사이에는 선형 관계가 성립한다.<br>
주로 시각화를 통해 확인한다.
2. 잔차들의 평균이 정규분포를 구성한다.<br>
회귀 분석의 신뢰성을 높이는 요소
3. 잔차들의 분산은 일정하다.<br>
회귀 분석의 신뢰성을 높이는 요소<br>
2, 3 가정이 위배되면 신뢰구간 및 가설 검정 결과가 부정확해지고, 모델의 예측 능력이 저하된다.<br>
즉 모델 자체의 신뢰성이 낮아진다.<br>
2, 3 가정을 확인하기 위해서는 Q-Q plot을 찍어, 빨간선에 가까울수록 정규성이 있다고 판단할 수 있다.
4. 독립변수 X 들 간 상관관계가 존재하지 않아야한다.<br>
다중공선성 문제가 나타날 수 있다.<br>
VIF(Variance Inflation Factor)를 이용해서 10이 넘으면 다중공선성에 문제가 있다고 여길 수 있다.

---

*주의점*

* 이상치 처리<br>
이상치 유무에 민감하게 결과가 나오므로 적절한 **이상치 제거** 필요<br>
보통 IQR을 이용해서 이상치를 검증한다.<br>
* R-square(결정계수)<br>
회귀 모형 내에서 독립변수 X가 설명할 수 있는 종속변수 Y의 변동성<br>
0에 가까운 경우, 선형 모델로는 Y를 설명할 수 없다고 해석할 수 있다.

---

*모델 결과 해석*

Coefficient : 회귀계수로서, 해당 독립변수의 1단위 상승에 대한, 종속변수 Y의 단위 상승량

p-value : 유의수준보다 높은 경우, 신뢰성이 떨어져서 사용하기 어렵다고 해석할 수 있음

## 3. KNN

### 3.1 알고리즘 설명

K-Nearest Neighbor

가까운 이웃에 위치한 K개의 데이터를 보고 데이터가 속할 그룹을 판단하는 과정

거리기반 모델이며, 사례 기반 학습(Instance-Based Learning)

거리를 L1으로 둘지, L2로 둘지에 따라 경계면의 모양이 조금 달라질 수 있다.

분류, 회귀 뿐만 아니라, **유사한 데이터를 구하기 위한 방법론**으로도 사용 가능하다

인접한 이웃을 구한다는 특징으로 인해 **결측치 보간**에도 사용 가능하다.

---

장점1 : 단순하고, 특별한 훈련을 거치치 않아 빠름<br>
장점2 : 특별한 가정이 존재하지 않다.

단점1 : 적절한 K 선택 필요<br>
단점2 : 데이터가 많아지면 분류가 느려짐<br>
단점3 : **데이터 스케일에 민감해서 스케일링이 필수적이다.**

---

*주의점*

K값 결정할 때에는, CV를 이용해서 K를 찾아야한다.

## 4. Decision Tree

### 4.1 알고리즘 설명

데이터 내 **규칙**을 찾아 Tree 구조로 데이터를 분류/회귀하는 과정

각 분기점을 node라고 부르며, 맨 마지막 node는 leaf node라고 부른다.

---

*불순도*

의사결정나무의 목표 : leaf node 의 불순도(impurity)를 최소화하는 것

정보획득량(imformation gain) : 분기 이전 불순도와 이후 불순도의 차이

정보획득량을 노드 분기의 기준으로 이용한다.

* Gini index(지니 계수)<br>
$I(A)=1-\sum^m_{k=1} p^2_k$<br>
이때, $p$는 k번째 클래스의 비율
* Entropy(엔트로피)<br>
$E=-\displaystyle \sum^k_{i=1} p_i \log_2 (p_i)$

위의 값이 높을수록 불순도가 높다.

따라서 위의 값을 최소화하도록 분기

---

*tree 분기 과정*

분할 종료 조건 : 순도가 100%가 되는 지점에서 분할 종료

분할 기준 선택 : 모든 변수에 대해 불순도를 계산하고, 해당 불순도가 가장 낮은 값을 기준으로 진행

---

장점1 : 직관적이라 이해가 쉽고, 스케일에 민감하지 않음<br>
장점2 : 범주형과 연속형 데이터 모두 처리 가능

단점1 : 트리가 깊어지면 overfitting 위험 존재 (pruning 필요)<br>
단점2 : 새로운 sample에 취약하다

---

*가지치기*

Pruning

트리의 깊이가 깊어지거나, leaf가 많아지면 overfitting 위험이 커지는 단점을 보완한다.

일부 leaf를 제거해서, 일반화 성능을 높임

* 트리의 depth 조절하기<br>
Max_depth, max_leaf_nodes, min_sample_split 과 같은 하이퍼파라미터로 설정

---

*트리 시각화*

결정경계와 트리 분할 과정을 시각화를 할 수 있다.

또한 feature importances를 이용해서, split에 기준이 된 횟수를 이용해서 feature를 볼 수 있다<br>하지만 절대적인 지표가 아님

## 5. Random Forest

### 5.1 알고리즘 설명

*Ensemble*<br>
주어진 데이터로 여러 개의 서로 다른 예측 모형을 생성해서, 이들의 결과를 종합하는 것<br>
대표적인 tree 모델 앙상블은 bagging, boosting, voting

*Bootstrap*<br>
**복원추출**을 사용해서 표본을 추출해 **모집단의 통계량을 추론**하는 통계적 방법

*Bagging*<br>
Bootstrap을 ML 앙상블에 사용하는 것<br>
표본을 여러 번 뽑아 모델을 학습시키고, 그 결과를 집계한다.<br>
과적합을 방지하면서, 일반화 성능을 높일 수 있는 방법

**Random Forest**<br>
Bagging의 일종이지만, 데이터 뿐만 아니라, 독립변수 X (feature)도 무작위로 생성해서 트리를 생성한다.<br>
N개의 tree에서 추출된 output을 voting, average해서 최종 output 생성<br>
분산의 감소에도 도움을 줄 수 있다.

---

장점1 : bootstrap을 이용해서, 기존 의사결정나무의 높은 분산을 줄일 수 있어 예측 성능이 향상<br>
장점2 : 여러 트리를 합쳐서 과적합 방지<br>
장점3 : 이외의 의사결정나무의 장점을 모두 흡수

단점1 : 대용량 데이터 학습에 많은 시간 소요<br>
단점2 : 생성하는 모든 tree를 해석할 수 없어, 모델의 해석력이 의사결정나무보다 떨어진다.
